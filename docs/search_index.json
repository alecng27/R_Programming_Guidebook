[["index.html", "R Programming Guidebook Project About", " R Programming Guidebook Project Alec Nguyen About This is my personal guidebook that I created with notes and examples on the workings of R, SQL and Web Scraping. This online book is created and developed using RStudio and the Bookdown package. https://alecng27.github.io/R_Programming_Guidebook/ Author Alec Nguyen Economics &amp; Data Science B.A, Lawrence University alec.nguyen27@gmail.com LinkedIn Project Portfolio "],["introduction-to-r.html", "1 Introduction to R 1.1 Intro to basics 1.2 Vectors 1.3 Matrices 1.4 Factors 1.5 Data frames 1.6 Lists", " 1 Introduction to R https://learn.datacamp.com/courses/free-introduction-to-r 1.1 Intro to basics Arithmetic Basic codes to perform calculation: ## Name Code Output ## 1 An addition 5+5 10 ## 2 A subtraction 5-5 0 ## 3 A multiplication 3*5 15 ## 4 A division (5+5)/2 5 ## 5 Exonentiation 2^5 32 ## 6 Modulo 28%%6 4 Variable Assignments A variable can store a value (e.g. 4) or an object (e.g. a function description) in R: my_socks &lt;- 2 Basic Data Types Here are some of the basic data types in R: my_numeric &lt;- 5 my_character &lt;- &quot;abcdefg&quot; my_logical &lt;- FALSE To check the data type of a variable, use the class() function as follow: class(my_numeric) ## [1] &quot;numeric&quot; class(my_character) ## [1] &quot;character&quot; class(my_logical) ## [1] &quot;logical&quot; 1.2 Vectors Create A Vector Create a vector with the function c(): poker_vector &lt;- c(140, -50, 20, -120, 240) roulette_vector &lt;- c(-24, -50, 100, -350, 10) Naming a Vector To assign names to the elements of a vector, use the names() function: names(poker_vector) &lt;- c(&quot;Monday&quot;, &quot;Tuesday&quot;, &quot;Wednesday&quot;, &quot;Thursday&quot;, &quot;Friday&quot;) names(roulette_vector) &lt;- c(&quot;Monday&quot;, &quot;Tuesday&quot;, &quot;Wednesday&quot;, &quot;Thursday&quot;, &quot;Friday&quot;) Typing the days multiple times is too much work, so to save time, assign the days of the week vector to a variable: days_vector, and then assign that to the other vectors with the class() function like below: days_vector &lt;- c(&quot;Monday&quot;, &quot;Tuesday&quot;, &quot;Wednesday&quot;, &quot;Thursday&quot;, &quot;Friday&quot;) names(poker_vector) &lt;- c(days_vector) names(roulette_vector) &lt;- c(days_vector) Calculating the winnings This code below calculates the total profit from poker and roulette each day: total_daily &lt;- poker_vector + roulette_vector total_daily ## Monday Tuesday Wednesday Thursday Friday ## 116 -100 120 -470 250 To calculate the sum of all elements of a vector, use the function sum(). total_poker &lt;- sum(poker_vector) total_roulette &lt;- sum(roulette_vector) total_week &lt;- total_poker + total_roulette total_week ## [1] -84 Vector Selection To select elements of a vector, use square brackets []: poker_wednesday &lt;- poker_vector[3] poker_wednesday ## Wednesday ## 20 Two ways to select multiple elements from a vector: poker_midweek &lt;- poker_vector[c(2,3,4)] poker_midweek &lt;- poker_vector[2:4] Another way to select is by using the assigned names of the vector elements above (Monday, Tuesday, …) instead of their numeric positions: poker_vector[&quot;Monday&quot;] ## Monday ## 140 poker_vector[c(&quot;Monday&quot;,&quot;Tuesday&quot;)] ## Monday Tuesday ## 140 -50 Selection By Comparison The (logical) comparison operators known to R are: &quot;&lt;&quot; for less than &quot;&gt;&quot; for greater than &quot;&lt;=&quot; for less than or equal to &quot;&gt;=&quot; for greater than or equal to &quot;==&quot; for equal to each other &quot;!=&quot; not equal to each other Application of these comparison operators on vectors: c(4, 5, 6) &gt; 5 ## [1] FALSE FALSE TRUE To find the days that had a positive poker return: poker_vector &gt; 0 ## Monday Tuesday Wednesday Thursday Friday ## TRUE FALSE TRUE FALSE TRUE To know not only the winning days, but also how much was made on those days: selection_vector &lt;- poker_vector &gt; 0 poker_winning_days &lt;- poker_vector[selection_vector] poker_winning_days ## Monday Wednesday Friday ## 140 20 240 1.3 Matrices A Matrix To construct a matrix in R, use the matrix() function: matrix(1:9, byrow = TRUE, nrow = 3) ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 ## [3,] 7 8 9 The 1:9 is the data that’s written into the matrix. Here,1:9 is a shortcut for c(1, 2, 3, 4, 5, 6, 7, 8, 9). The part byrow tells the matrix that the 1:9 is filled by the rows (left to right of each row from the top). To fill by the columns, write byrow = FALSE. The third part nrow tells the matrix to create 3 rows. Example of a data-set used to create a matrix: #Box office Star Wars (in millions) new_hope &lt;- c(460.998, 314.4) empire_strikes &lt;- c(290.475, 247.900) return_jedi &lt;- c(309.306, 165.8) #Create box_office (a vector) box_office &lt;- c(new_hope, empire_strikes, return_jedi) #Construct star_wars_matrix star_wars_matrix &lt;- matrix(box_office, byrow = TRUE, nrow = 3) star_wars_matrix ## [,1] [,2] ## [1,] 460.998 314.4 ## [2,] 290.475 247.9 ## [3,] 309.306 165.8 Naming A Matrix To add names ro the rows and columns of a matrix: region &lt;- c(&quot;US&quot;, &quot;non-US&quot;) titles &lt;- c(&quot;A New Hope&quot;, &quot;The Empire Strikes Back&quot;, &quot;Return of the Jedi&quot;) colnames(star_wars_matrix) &lt;- region rownames(star_wars_matrix) &lt;- titles Calculating Rows’ or Columns’ Sum Function rowSums() calculates the sum of each row of a matrix. This would would give the total revenue of each movie from US + Non-US: rowSums(star_wars_matrix) ## A New Hope The Empire Strikes Back Return of the Jedi ## 775.398 538.375 475.106 To calculate the total revenue of each region (US and Non-US) from all movies: colSums(star_wars_matrix) ## US non-US ## 1060.779 728.100 Adding Row or Column To The Matrix Add a column(s) to a matrix with the cbind() function, which merge matrices and/or vectors together by column. For example: worldwide_vector &lt;- rowSums(star_wars_matrix) all_wars_matrix &lt;- cbind(star_wars_matrix, worldwide_vector) all_wars_matrix ## US non-US worldwide_vector ## A New Hope 460.998 314.4 775.398 ## The Empire Strikes Back 290.475 247.9 538.375 ## Return of the Jedi 309.306 165.8 475.106 Selection of matrix elements Use the square brackets [ ] to select element(s) from a matrix. Vectors contain one type of data (like only x-axis) and matrices contain two types (like x and y-axis). So, use a comma to separate the rows, columns to be selected. For example: my_matrix[1,2] selects the element at the first row and second column. my_matrix[1:3,2:4] results in a matrix with the data on the rows 1, 2, 3 and columns 2, 3, 4. To select all elements of a row or a column, no number is needed before or after the comma: my_matrix[,1] selects all elements of the first column. my_matrix[1,] selects all elements of the first row. A little arithmetic with matrices 2 * matrix would multiply each element of my_matrix by two. matrix1 * matrix2 would create a new matrix where each element is the product of the corresponding elements in matrix1 and matrix2. -Estimated number of visitors visitors &lt;- all_wars_matrix/ticket_prices_matrix 1.4 Factors There are two types of categorical variables: nominal categorical variable and ordinal categorical variable. A nominal variable is a categorical variable without an implied order. For example, the categorical variable items_vector containing \"Car\", \"Key\", \"Phone\" and \"Shoe\". It’s hard to compare these elements using a standard. Ordinal variables have natural ordering. For example, the categorical variable temperature_vector with the categories: \"Low\", \"Medium\" and \"High\". The temps are comparable using a standard. The function factor() point out the different element groups in the data-set: survey_vector &lt;- c(&quot;M&quot;, &quot;F&quot;, &quot;F&quot;, &quot;M&quot;, &quot;M&quot;) factor_survey_vector &lt;- factor(survey_vector) factor_survey_vector ## [1] M F F M M ## Levels: F M Factor Levels(Naming Elements) To change the factor levels “M” and “F” to “Male” and “Female”: levels(factor_survey_vector) &lt;- c(&quot;Female&quot;, &quot;Male&quot;) factor_survey_vector ## [1] Male Female Female Male Male ## Levels: Female Male Summarizing a factor To know how many \"Male\" responses are in the study, and how many \"Female\" responses. The summary() function gives the answer to this question: summary(factor_survey_vector) ## Female Male ## 2 3 Ordered factors (Comparing elements) The function factor() convert speed_vector into unordered factor. To create an ordered factor, add two additional arguments: ordered and levels: speed_vector &lt;- c(&quot;medium&quot;, &quot;slow&quot;, &quot;slow&quot;, &quot;medium&quot;, &quot;fast&quot;) factor_speed_vector &lt;- factor(speed_vector, ordered = TRUE, levels = c(&quot;slow&quot;, &quot;medium&quot;, &quot;fast&quot;)) factor_speed_vector ## [1] medium slow slow medium fast ## Levels: slow &lt; medium &lt; fast The ordered = TRUE is saying that the elements can be compared with a standard. The levels state the elements of the factor in ranks. Comparing ordered factors The ordered factor_speed_vector enables the comparison of different elements: data_analyst2 &lt;- factor_speed_vector[2] data_analyst5 &lt;- factor_speed_vector[5] data_analyst2 &gt; data_analyst5 ## [1] FALSE 1.5 Data frames Use function data.frame() to create a data frame. Example from Datacamp below: #Definition of vectors name &lt;- c(&quot;Mercury&quot;, &quot;Venus&quot;, &quot;Earth&quot;, &quot;Mars&quot;, &quot;Jupiter&quot;, &quot;Saturn&quot;, &quot;Uranus&quot;, &quot;Neptune&quot;) type &lt;- c(&quot;Terrestrial planet&quot;, &quot;Terrestrial planet&quot;, &quot;Terrestrial planet&quot;, &quot;Terrestrial planet&quot;, &quot;Gas giant&quot;, &quot;Gas giant&quot;, &quot;Gas giant&quot;, &quot;Gas giant&quot;) diameter &lt;- c(0.382, 0.949, 1, 0.532, 11.209, 9.449, 4.007, 3.883) rotation &lt;- c(58.64, -243.02, 1, 1.03, 0.41, 0.43, -0.72, 0.67) rings &lt;- c(FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE) #Create a data frame from the vectors planets_df &lt;- data.frame(name, type, diameter, rotation, rings) planets_df ## name type diameter rotation rings ## 1 Mercury Terrestrial planet 0.382 58.64 FALSE ## 2 Venus Terrestrial planet 0.949 -243.02 FALSE ## 3 Earth Terrestrial planet 1.000 1.00 FALSE ## 4 Mars Terrestrial planet 0.532 1.03 FALSE ## 5 Jupiter Gas giant 11.209 0.41 TRUE ## 6 Saturn Gas giant 9.449 0.43 TRUE ## 7 Uranus Gas giant 4.007 -0.72 TRUE ## 8 Neptune Gas giant 3.883 0.67 TRUE Analyzation Tools Head and Tail The function head() and tail() show show the first few and last few observations of a data frame. Structure The function str() helps investigate the structure of the planets_df data frame: str(planets_df) ## &#39;data.frame&#39;: 8 obs. of 5 variables: ## $ name : chr &quot;Mercury&quot; &quot;Venus&quot; &quot;Earth&quot; &quot;Mars&quot; ... ## $ type : chr &quot;Terrestrial planet&quot; &quot;Terrestrial planet&quot; &quot;Terrestrial planet&quot; &quot;Terrestrial planet&quot; ... ## $ diameter: num 0.382 0.949 1 0.532 11.209 ... ## $ rotation: num 58.64 -243.02 1 1.03 0.41 ... ## $ rings : logi FALSE FALSE FALSE FALSE TRUE TRUE ... Selection of data frame elements Use [] to select specific elements from a data frame. Examples from Datacamp below: #Print out data for Mars (entire fourth row) planets_df[4,] ## name type diameter rotation rings ## 4 Mars Terrestrial planet 0.532 1.03 FALSE #Select first 5 values of diameter column planets_df[1:5,&quot;diameter&quot;] ## [1] 0.382 0.949 1.000 0.532 11.209 Shortcut $ to select dataframe elements The sign $ is a shortcut to select the wanted column that has a name: #Select the rings variable from planets_df rings_vector &lt;- planets_df$rings rings_vector ## [1] FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE To show the variable names of the data frame instead of only the result above: #Select all columns for planets with rings planets_df[rings_vector,] ## name type diameter rotation rings ## 5 Jupiter Gas giant 11.209 0.41 TRUE ## 6 Saturn Gas giant 9.449 0.43 TRUE ## 7 Uranus Gas giant 4.007 -0.72 TRUE ## 8 Neptune Gas giant 3.883 0.67 TRUE Shortcut subset() to select dataframe elements The subset(planets_df, subset = rings) function give out the exact result like above. Another example from Datacamp for variety: #Select planets with diameter &lt; 1 subset(planets_df, diameter &lt; 1) ## name type diameter rotation rings ## 1 Mercury Terrestrial planet 0.382 58.64 FALSE ## 2 Venus Terrestrial planet 0.949 -243.02 FALSE ## 4 Mars Terrestrial planet 0.532 1.03 FALSE Sorting Use function sort() to organize elements into ranks. To find out the order of planets according to diameter: order(planets_df$diameter) ## [1] 1 4 2 3 8 7 6 5 The results indicate that planet #1 is the smallest, #4 is the second-smallest, and planet #5 is the largest. Now, to reorganize the data frame according to the ranking of diameter: positions &lt;- order(planets_df$diameter) 1.6 Lists A list could contain matrices, vectors, other lists, … To create a list, use the function list(): my_list &lt;- list(comp1, comp2 ...) To give names to components of a list, there are two ways: my_list &lt;- list(your_comp1, your_comp2) names(my_list) &lt;- c(&quot;name1&quot;, &quot;name2&quot;) or my_list &lt;- list(name1 = your_comp1, name2 = your_comp2) To select elements from a list: -Select the vector representing the actors shining_list[[&quot;actors&quot;]] shining_list$actors Select the second element of the vector representing the actors shining_list[[&quot;actors&quot;]][2] "],["intermediate-r.html", "2 Intermediate R 2.1 Conditionals And Control Flow 2.2 Loops 2.3 Functions 2.4 The apply family 2.5 Utilities", " 2 Intermediate R https://learn.datacamp.com/courses/intermediate-r 2.1 Conditionals And Control Flow Reminder: == is for comparison and = is for assignment. TRUE is treated as 1 for arithmetic, and FALSE is treated as 0. Compare Vectors and Matrices Vectors Number of views on each site: linkedin &lt;- c(16, 9, 13, 5, 2, 17, 14) facebook &lt;- c(17, 7, 5, 16, 8, 13, 14) To find out which had more views: linkedin &gt;= facebook ## [1] FALSE TRUE TRUE FALSE FALSE TRUE TRUE Matrices Compare data in matrices: views &lt;- matrix(c(linkedin, facebook), nrow = 2, byrow = TRUE) # When is views less than or equal to 14? views &lt;= 14 ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] FALSE TRUE TRUE TRUE TRUE FALSE TRUE ## [2,] FALSE TRUE TRUE FALSE TRUE TRUE TRUE Logical Operators &amp; and | With “Or” function: |', only one condition (the right one or the left one) needs to be satisfied to spit outTRUE`: socks &lt;- 13 # Is last under 5 or above 10? socks &lt; 5 | socks &gt;10 ## [1] TRUE With “And” function: “&amp;,” both conditions (the right and left ones) need to be satisfied to spit out TRUE. In this case, the “left condition” is not satisfied, hence: # Is last between 15 (exclusive) and 20 (inclusive)? socks &gt; 15 &amp; socks &lt;= 20 ## [1] FALSE “!” Operator The ! operator negates a logical value: x &lt;- 5 y &lt;- 7 !((x &lt; 4) &amp; !(y &gt; 12)) ## [1] TRUE This can be a brain-twister. Both x and y element is FALSE. However, there’s a ! in front of y, therefore, (y-12) is TRUE. So, within the entire (), a FALSE and TRUE would spit out a FALSE because one component is FALSE. Then, the ! operator would flip the result, which becomes TRUE in the end. The “IF” Statement IFfunction allows the logic of “if this happens then do A.” For example: num_views &lt;- 14 if (num_views &gt; 15) { print(&quot;You&#39;re popular!&quot;) } This is the expanded of the above logic thought: “if this happens then do A, if doesn’t, then do B, and if both doesn’t happen, then do the C”: if (num_views &gt; 15) { print(&quot;You&#39;re popular!&quot;) } else if (num_views &lt;= 15 &amp; num_views &gt; 10) { print(&quot;Your number of views is average&quot;) } else { print(&quot;Try to be more visible!&quot;) } ## [1] &quot;Your number of views is average&quot; Logic are lawless inside “if-else” constructs. This is a good example from Datacamp: if (number &lt; 10) { if (number &lt; 5) { result &lt;- &quot;extra small&quot; } else { result &lt;- &quot;small&quot; } } else if (number &lt; 100) { result &lt;- &quot;medium&quot; } else { result &lt;- &quot;large&quot; } print(result) 2.2 Loops “While” Loop While function’s logic is “while this is true, keep doing the task.” Example from datacamp to understand: speed &lt;- 64 # Extend/adapt the while loop while (speed &gt; 30) { print(paste(&quot;Your speed is&quot;,speed)) if (speed &gt; 48) { print(&quot;Slow down big time!&quot;) speed &lt;- speed - 11 } else { print(&quot;Slow down!&quot;) speed &lt;- speed - 6 } } ## [1] &quot;Your speed is 64&quot; ## [1] &quot;Slow down big time!&quot; ## [1] &quot;Your speed is 53&quot; ## [1] &quot;Slow down big time!&quot; ## [1] &quot;Your speed is 42&quot; ## [1] &quot;Slow down!&quot; ## [1] &quot;Your speed is 36&quot; ## [1] &quot;Slow down!&quot; “For” Loop Loop A Vector, List This loop will print out all the views listed in the linkedin vector orderly from left to right: linkedin &lt;- c(16, 9, 13, 5, 2, 17, 14) # Loop version 1 for (views in linkedin) { print(views) } ## [1] 16 ## [1] 9 ## [1] 13 ## [1] 5 ## [1] 2 ## [1] 17 ## [1] 14 This loop will also print out all the views from the vector, but it does so by reffering to the location of the specific element within the vector to print out: # Loop version 2 for (i in 1:length(linkedin)) { print(linkedin[i]) } ## [1] 16 ## [1] 9 ## [1] 13 ## [1] 5 ## [1] 2 ## [1] 17 ## [1] 14 #Note: use &quot;[[ ]]&quot; to select the elements in loop version 2 when looping a list. Loop A Matrix A for loop inside a for loop is called a nested loop, example from Datacamp: ttt &lt;- matrix(c( &quot;O&quot;, NA, &quot;X&quot;, NA, &quot;O&quot;, &quot;O&quot;, &quot;X&quot;, NA, &quot;X&quot;), byrow = TRUE, nrow =3) for (i in 1:nrow(ttt)) { for (j in 1:ncol(ttt)) { print(paste(&quot;On row&quot;, i, &quot;and column&quot;, j, &quot;the board contains&quot;, ttt[i,j])) } } ## [1] &quot;On row 1 and column 1 the board contains O&quot; ## [1] &quot;On row 1 and column 2 the board contains NA&quot; ## [1] &quot;On row 1 and column 3 the board contains X&quot; ## [1] &quot;On row 2 and column 1 the board contains NA&quot; ## [1] &quot;On row 2 and column 2 the board contains O&quot; ## [1] &quot;On row 2 and column 3 the board contains O&quot; ## [1] &quot;On row 3 and column 1 the board contains X&quot; ## [1] &quot;On row 3 and column 2 the board contains NA&quot; ## [1] &quot;On row 3 and column 3 the board contains X&quot; “Break” And “Next” The break terminate the running code if the condition is FALSE. The next allow the code to the run after break. It skip over the element that made the code FALSE then continue. likes &lt;- c(16, 9, 13, 5, 2, 17, 14) for (heart in likes) { if (heart &gt; 10) { print(&quot;You&#39;re popular!&quot;) } else {print(&quot;Be more visible!&quot;) } if (heart &gt; 16) {print(&quot;This is ridiculous, I&#39;m outta here!&quot;) break } if(heart &lt; 5) {print(&quot;This is too embarrassing!&quot;) next } print(heart) } ## [1] &quot;You&#39;re popular!&quot; ## [1] 16 ## [1] &quot;Be more visible!&quot; ## [1] 9 ## [1] &quot;You&#39;re popular!&quot; ## [1] 13 ## [1] &quot;Be more visible!&quot; ## [1] 5 ## [1] &quot;Be more visible!&quot; ## [1] &quot;This is too embarrassing!&quot; ## [1] &quot;You&#39;re popular!&quot; ## [1] &quot;This is ridiculous, I&#39;m outta here!&quot; 2.3 Functions A way to see the components of a function is the args() function: args(sum) ## function (..., na.rm = FALSE) ## NULL Exclude “NA” From A Calculation linkedin &lt;- c(16, 9, 13, 5, NA, 17, 14) facebook &lt;- c(17, NA, 5, 16, 8, 13, 14) # Basic average of linkedin mean(linkedin) ## [1] NA # Advanced average of linkedin mean(linkedin, na.rm = TRUE) ## [1] 12.33333 The default setting in the mean() function is na.rm = FALSE, which means it doesn’t exclude the NA variables. However, when switched to TRUE, the function excludes the NA varaibles. When Is It Required? mean(x, trim = 0, na.rm = FALSE, ...) x is required; if you do not specify it, R will throw an error. trim and na.rm are optional arguments: they have a default value which is used if the arguments are not explicitly specified. Create A Function To create a function, assign a variable the function function(condition){body}: pow_two &lt;- function(x) { y &lt;- x ^ 2 print(paste(x, &quot;to the power two equals&quot;, y)) return(y) } pow_two(6) ## [1] &quot;6 to the power two equals 36&quot; ## [1] 36 #NOTE: &quot;y&quot; was defined inside the &quot;pow_two()&quot; function and therefore it is not accessible outside of that function. This is also true for the function&#39;s arguments of course - &quot;x&quot; in this case. Internal Variables of “Function()” are FIXED An external varaible can’t be entered in to change the internal make-up of a created function: triple &lt;- function(x) { x &lt;- 3*x x } #Testing whether R updates the variable &quot;a&quot;: a &lt;- 5 triple(a) ## [1] 15 a ## [1] 5 Even though the function triple(a) outputted 15, R didn’t print the new a variable as 15 but as 5. Example from Datacamp: likes &lt;- c(10, 18, 4) interpret &lt;- function(num_views) { if (num_views &gt; 15) { print(&quot;You&#39;re popular!&quot;) return(num_views) } else { print(&quot;Try to be more visible!&quot;) return(0) } } interpret(likes[2]) ## [1] &quot;You&#39;re popular!&quot; ## [1] 18 interpret(likes[3]) ## [1] &quot;Try to be more visible!&quot; ## [1] 0 Load an R Package There are basically two important functions when it comes to R packages: install.packages() installs a given package. library() which loads packages, i.e. attaches them to the search list on your R workspace. Anonymous functions An anonymous function is a function that’s NOT aasigned a variable(name): # Named function triple &lt;- function(x) { 3 * x } # Anonymous function with same implementation function(x) { 3 * x } ## function(x) { 3 * x } 2.4 The apply family “lapply()”* lappy() applies the function inputted inside the () over a vector or list, and spit out a list. lapply(X, FUN, ...) For example: numbers_list &lt;- list(c(17, 28, -2, 9, 22), c(2, -19, 54, 27, 11), c(91, 76, -34, 8, 10)) extremes_avg &lt;- function(x) { ( min(x) + max(x) ) / 2 } # Apply extremes_avg() over numbers_list using lapply() lapply(numbers_list, extremes_avg) ## [[1]] ## [1] 13 ## ## [[2]] ## [1] 17.5 ## ## [[3]] ## [1] 28.5 “sapply()”* sapply() applies the function inputted inside the () over a vector or list, and try to arrange the resulting list into an organized array. If not possible, sapply() will return the same list as lapply() spit out. sapply(X, FUN, ...) Continuing the example above: # Apply extremes_avg() over numbers_list using sapply() sapply(numbers_list, extremes_avg) ## [1] 13.0 17.5 28.5 The outputted result looks more compact than lapply(). “vapply()”* vapply() applies the function inputted inside the () over a vector or list like lapply() or sapply(). However, with vapply(), it requires a specified output format, meaning tell it what result type it should spit out: vapply(X, FUN, FUN.VALUE, ..., USE.NAMES = TRUE) The FUN.VALUE argument expects a template for the return argument of this function FUN. USE.NAMES is TRUE by default; in this case vapply() tries to generate a named array, if possible. Example: # Definition of the basics() function basics &lt;- function(x) { c(min = min(x), mean = mean(x), median = median(x), max = max(x)) } # Fix the error: vapply(numbers_list, basics, numeric(4)) ## [,1] [,2] [,3] ## min -2.0 -19 -34.0 ## mean 14.8 15 30.2 ## median 17.0 11 10.0 ## max 28.0 54 91.0 In this example, if numerics specified was 3 instead of 4, the code would NOT run and give an error because vapply() function requires a specific output format. In this case, the basics function has 4 elements: min, mean, median, and max, therefore, vapply() need to specified as numeric(4). 2.5 Utilities Mathematical Utilities abs() : Calculate the absolute value. sum() : Calculate the sum of all the values in a data structure. mean() : Calculate the arithmetic mean. round(): Round the values to 0 decimal places by default. Example: digits &lt;- c(1.9, -2.6, 4.0, -9.5, -3.4, 7.3) # Sum of absolute rounded values of errors sum(abs(round(digits))) ## [1] 29 Data Utilities seq(): Generate sequences, by specifying the from, to, and by arguments. rep(): Replicate elements of vectors and lists. rep(seq(1, 7, by = 2), times = 7) ## [1] 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 sort(): Sort a vector in ascending order by default. Works on numerics, but also on character strings and logicals. rev(): Reverse the elements in a data structures for which reversal is defined. str(): Display the structure of any R object. append(): Merge vectors or lists. is.*(): Check for the class of an R object. as.*(): Convert an R object from one class to another. unlist(): Flatten (possibly embedded) lists to produce a vector. Example: linkedin &lt;- list(16, 9, 13, 5, 2, 17, 14) facebook &lt;- list(17, 7, 5, 16, 8, 13, 14) # Convert linkedin and facebook to a vector: li_vec and fb_vec li_vec &lt;- unlist(linkedin) fb_vec &lt;- unlist(facebook) # Append fb_vec to li_vec: social_vec social_vec &lt;- append(li_vec, fb_vec) # Sort social_vec sort(social_vec, decreasing = TRUE) ## [1] 17 17 16 16 14 14 13 13 9 8 7 5 5 2 Regular Expressions Regular expressions can be used to see whether a pattern exists inside a character string or a vector of character strings. “grep()” and “grepl()” grepl(): the l in grepl() stands for logical, which indicates that this function returns TRUE when a pattern is found in the corresponding character string. grep(): returns a vector contains the location of the character strings(by order) that contains the pattern searched for. The caret: `^`, to match the content located in the start of a string. The dollar-sign: `$`, to match the content located in the end of a string. emails &lt;- c(&quot;john.doe@ivyleague.edu&quot;, &quot;education@world.gov&quot;, &quot;dalai.lama@peace.org&quot;, &quot;invalid.edu&quot;, &quot;quant@bigdatacollege.edu&quot;, &quot;cookie.monster@sesame.tv&quot;) #Search for the email addresses in the vector above that contains &quot;@&quot;, anything in between, and &quot;edu&quot;: grepl(pattern = &quot;@.*\\\\.edu$&quot;, emails) ## [1] TRUE FALSE FALSE FALSE TRUE FALSE grep(pattern = &quot;@.*\\\\.edu$&quot;, emails) ## [1] 1 5 # Subset emails using hits emails[grep(pattern = &quot;@.*\\\\.edu$&quot;, emails)] ## [1] &quot;john.doe@ivyleague.edu&quot; &quot;quant@bigdatacollege.edu&quot; .*: can be read as “any character that is matched zero or more times.”Both the dot and the asterisk are metacharacters. \\\\: is like a cut-off. It put a separation wall between the .* and .edu. “sub()” and “gsub()” sub() and gsub()can specify a replacement argument. If inside the character vector x, the regular expression pattern is found, the matching element(s) will be replaced with replacement.sub() only replaces the first match, whereas gsub() replaces all matches. emails &lt;- c(&quot;john.doe@ivyleague.edu&quot;, &quot;education@world.gov&quot;, &quot;dalai.lama@peace.org&quot;, &quot;invalid.edu&quot;, &quot;quant@bigdatacollege.edu&quot;, &quot;cookie.monster@sesame.tv&quot;) sub(pattern = &quot;@.*\\\\.edu$&quot;, replacement = &quot;@datacamp.edu&quot;, emails) ## [1] &quot;john.doe@datacamp.edu&quot; &quot;education@world.gov&quot; ## [3] &quot;dalai.lama@peace.org&quot; &quot;invalid.edu&quot; ## [5] &quot;quant@datacamp.edu&quot; &quot;cookie.monster@sesame.tv&quot; #The [1] and [5] elements has been changed. \\\\s: Match a space. The “s” is normally a character, escaping it \\\\ makes it a metacharacter. [0-9]+: Match the numbers 0 to 9, at least once (+). ([0-9]+): The parentheses are used to NOT confuse the pattern matching criterias. The \\\\1: is to input the regular expression [0-9]+ matched into the replacement argument. awards &lt;- c(&quot;Won 1 Oscar.&quot;, &quot;Another 9 wins &amp; 24 nominations.&quot;, &quot;2 wins &amp; 3 nominations.&quot;, &quot;Nominated for 2 Golden Globes. 1 more win &amp; 2 nominations.&quot;) sub(&quot;.*\\\\s([0-9]+)\\\\snomination.*$&quot;, &quot;\\\\1&quot;, awards) ## [1] &quot;Won 1 Oscar.&quot; &quot;24&quot; &quot;3&quot; &quot;2&quot; The logic behind the pattern criteria is as follow: skip any character and then a space between the number and the word “nomination.” Then, skip again any character after the word “nomination.” Date And Time Dates are represented by Date objects. Times are represented by POSIXct objects. However, dates and times are simple numerical values. Date objects store the number of days since the 1st of January in 1970. POSIXct store the number of seconds since the 1st of January in 1970. # Get the current date: today today &lt;- Sys.Date() today ## [1] &quot;2022-05-14&quot; # See what today looks like under the hood unclass(today) ## [1] 19126 # Get the current time: now now &lt;- Sys.time() now ## [1] &quot;2022-05-14 17:48:51 CDT&quot; # See what now looks like under the hood unclass(now) ## [1] 1652568531 Date Formats Use the as.Date() function to create a Date object from a simple character string. %Y: 4-digit year (1982) %y: 2-digit year (82) %m: 2-digit month (01) %d: 2-digit day of the month (13) %A: weekday (Wednesday) %a: abbreviated weekday (Wed) %B: month (January) %b: abbreviated month (Jan) # Definition of character strings representing dates str1 &lt;- &quot;May 23, &#39;96&quot; str2 &lt;- &quot;2012-03-15&quot; str3 &lt;- &quot;30/January/2006&quot; # Convert the strings to dates: date1, date2, date3 date1 &lt;- as.Date(str1, format = &quot;%b %d, &#39;%y&quot;) date2 &lt;- as.Date(str2, format = &quot;%Y-%m-%d&quot;) date3 &lt;- as.Date(str3, format = &quot;%d/%B/%Y&quot;) # Convert dates to formatted strings format(date1, &quot;%A&quot;) ## [1] &quot;Thursday&quot; format(date2, &quot;%d&quot;) ## [1] &quot;15&quot; format(date3, &quot;%b %Y&quot;) ## [1] &quot;Jan 2006&quot; Date Calculations Both Date and POSIXct objects are represented by simple numerical values under the hood. today &lt;- Sys.Date() today + 1 ## [1] &quot;2022-05-15&quot; today - 1 ## [1] &quot;2022-05-13&quot; as.Date(&quot;2015-03-12&quot;) - as.Date(&quot;2015-02-27&quot;) ## Time difference of 13 days Time Formats Use as.POSIXct() to convert a character string to a POSIXct object. %H: hours as a decimal number (00-23) %I: hours as a decimal number (01-12) %M: minutes as a decimal number %S: seconds as a decimal number %T: shorthand notation for the typical format %H:%M:%S %p: AM/PM indicator For a full list of conversion symbols, consult the strptime documentation in the console: ?strptime # Definition of character strings representing times str1 &lt;- &quot;May 23, &#39;96 hours:23 minutes:01 seconds:45&quot; str2 &lt;- &quot;2012-3-12 14:23:08&quot; # Convert the strings to POSIXct objects: time1, time2 time1 &lt;- as.POSIXct(str1, format = &quot;%B %d, &#39;%y hours:%H minutes:%M seconds:%S&quot;) time2 &lt;- as.POSIXct(str2, format = &quot;%Y-%m-%d %H:%M:%S&quot;) # Convert times to formatted strings format(time1, &quot;%M&quot;) ## [1] &quot;01&quot; format(time2, &quot;%I:%M %p&quot;) ## [1] &quot;02:23 PM&quot; Time Calculations Examples of doing calculations with POSIXct objects: now &lt;- Sys.time() now + 3600 # add an hour ## [1] &quot;2022-05-14 18:48:51 CDT&quot; now - 3600 * 24 # subtract a day ## [1] &quot;2022-05-13 17:48:51 CDT&quot; birth &lt;- as.POSIXct(&quot;1879-03-14 14:37:23&quot;) death &lt;- as.POSIXct(&quot;1955-04-18 03:47:12&quot;) einstein &lt;- death - birth einstein ## Time difference of 27792.56 days "],["intro-to-the-tidyverse.html", "3 Intro to the Tidyverse 3.1 Data wrangling 3.2 Data visualization 3.3 Grouping and summarizing 3.4 Types of visualizations", " 3 Intro to the Tidyverse https://learn.datacamp.com/courses/introduction-to-the-tidyverse 3.1 Data wrangling The following info goes into how to select (filter), arrange, add, change (mutate) specific variables and observations. Install these two packages so codes below work properly: install.packages(&quot;gapminder&quot;) install.packages(&quot;dplyr&quot;) Loading the gapminder and dplyr packages # Load the gapminder package library(gapminder) # Load the dplyr package library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union # Look at the gapminder dataset gapminder ## # A tibble: 1,704 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. ## 7 Afghanistan Asia 1982 39.9 12881816 978. ## 8 Afghanistan Asia 1987 40.8 13867957 852. ## 9 Afghanistan Asia 1992 41.7 16317921 649. ## 10 Afghanistan Asia 1997 41.8 22227415 635. ## # … with 1,694 more rows Filter, Arrange, And Mutate Verbs Filter Verb The filter verb extracts particular observations based on a condition. The ==is to compare two values. # Filter for China in 2002 gapminder %&gt;% filter(country == &quot;China&quot;,year == 2002) ## # A tibble: 1 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 China Asia 2002 72.0 1280400000 3119. Arrange Verb use arrange() to sort observations in ascending or descending order of a particular variable: # Sort in ascending order of lifeExp gapminder %&gt;% arrange(lifeExp) ## # A tibble: 1,704 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Rwanda Africa 1992 23.6 7290203 737. ## 2 Afghanistan Asia 1952 28.8 8425333 779. ## 3 Gambia Africa 1952 30 284320 485. ## 4 Angola Africa 1952 30.0 4232095 3521. ## 5 Sierra Leone Africa 1952 30.3 2143249 880. ## 6 Afghanistan Asia 1957 30.3 9240934 821. ## 7 Cambodia Asia 1977 31.2 6978607 525. ## 8 Mozambique Africa 1952 31.3 6446316 469. ## 9 Sierra Leone Africa 1957 31.6 2295678 1004. ## 10 Burkina Faso Africa 1952 32.0 4469979 543. ## # … with 1,694 more rows # Sort in descending order of lifeExp gapminder %&gt;% arrange(desc(lifeExp)) ## # A tibble: 1,704 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Japan Asia 2007 82.6 127467972 31656. ## 2 Hong Kong, China Asia 2007 82.2 6980412 39725. ## 3 Japan Asia 2002 82 127065841 28605. ## 4 Iceland Europe 2007 81.8 301931 36181. ## 5 Switzerland Europe 2007 81.7 7554661 37506. ## 6 Hong Kong, China Asia 2002 81.5 6762476 30209. ## 7 Australia Oceania 2007 81.2 20434176 34435. ## 8 Spain Europe 2007 80.9 40448191 28821. ## 9 Sweden Europe 2007 80.9 9031088 33860. ## 10 Israel Asia 2007 80.7 6426679 25523. ## # … with 1,694 more rows Mutate Verb mutate() changes or adds variables: # Use mutate to change lifeExp to be in months gapminder %&gt;% mutate(lifeExp = 12 * lifeExp) ## # A tibble: 1,704 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 346. 8425333 779. ## 2 Afghanistan Asia 1957 364. 9240934 821. ## 3 Afghanistan Asia 1962 384. 10267083 853. ## 4 Afghanistan Asia 1967 408. 11537966 836. ## 5 Afghanistan Asia 1972 433. 13079460 740. ## 6 Afghanistan Asia 1977 461. 14880372 786. ## 7 Afghanistan Asia 1982 478. 12881816 978. ## 8 Afghanistan Asia 1987 490. 13867957 852. ## 9 Afghanistan Asia 1992 500. 16317921 649. ## 10 Afghanistan Asia 1997 501. 22227415 635. ## # … with 1,694 more rows # Use mutate to create a new column called lifeExpMonths gapminder %&gt;% mutate(lifeExpMonths = 12 * lifeExp) ## # A tibble: 1,704 × 7 ## country continent year lifeExp pop gdpPercap lifeExpMonths ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. 346. ## 2 Afghanistan Asia 1957 30.3 9240934 821. 364. ## 3 Afghanistan Asia 1962 32.0 10267083 853. 384. ## 4 Afghanistan Asia 1967 34.0 11537966 836. 408. ## 5 Afghanistan Asia 1972 36.1 13079460 740. 433. ## 6 Afghanistan Asia 1977 38.4 14880372 786. 461. ## 7 Afghanistan Asia 1982 39.9 12881816 978. 478. ## 8 Afghanistan Asia 1987 40.8 13867957 852. 490. ## 9 Afghanistan Asia 1992 41.7 16317921 649. 500. ## 10 Afghanistan Asia 1997 41.8 22227415 635. 501. ## # … with 1,694 more rows Combining Filter, Mutate, And Arrange # Filter, mutate, and arrange the gapminder dataset gapminder %&gt;% filter(year == 2007) %&gt;% mutate(lifeExpMonths = 12 * lifeExp) %&gt;% arrange(desc(lifeExpMonths)) ## # A tibble: 142 × 7 ## country continent year lifeExp pop gdpPercap lifeExpMonths ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Japan Asia 2007 82.6 127467972 31656. 991. ## 2 Hong Kong, China Asia 2007 82.2 6980412 39725. 986. ## 3 Iceland Europe 2007 81.8 301931 36181. 981. ## 4 Switzerland Europe 2007 81.7 7554661 37506. 980. ## 5 Australia Oceania 2007 81.2 20434176 34435. 975. ## 6 Spain Europe 2007 80.9 40448191 28821. 971. ## 7 Sweden Europe 2007 80.9 9031088 33860. 971. ## 8 Israel Asia 2007 80.7 6426679 25523. 969. ## 9 France Europe 2007 80.7 61083916 30470. 968. ## 10 Canada Americas 2007 80.7 33390141 36319. 968. ## # … with 132 more rows 3.2 Data visualization Make sure to load the ggplot2 package. ggplot2 will allow the visualization of data into graphs. library(ggplot2) gapminder_1952 &lt;- gapminder %&gt;% filter(year == 1952) #Create a plot to compare population and life expectancy ggplot(gapminder_1952, aes(x = pop, y = lifeExp)) + geom_point() geom_point() stands for “geometric,” and the “point” tells R that it’s a scatter plot. aes() stands for aesthetic. The data is too cramped up to the left of the plot so use “log scales” to distribute the data better. *Log Scale # Scatter plot comparing pop and gdpPercap, with both axes on a log scale ggplot(gapminder_1952, aes(x = pop, y = gdpPercap)) + geom_point() +scale_x_log10() + scale_y_log10() the newly added “log_scales” has created a better looking graph by adjusting the scale of the x-axis. Additional Aesthetic Functions Color And Size Adding color to a scatter plot can be used to show which continent each point in a scatter plot represents. Then, adding size can be used to show the magnitude of each observations: # Add the color, and size aesthetics to represent the continents and the country&#39;s gdpPercap ggplot(gapminder_1952, aes(x = pop, y = lifeExp, color = continent, size = gdpPercap)) + geom_point() + scale_x_log10() Faceting/Subgraphs Use facet_wrap(~ ) function to divide a graph into subplots based on one of its variables: # Scatter plot comparing pop and lifeExp, faceted by continent ggplot(gapminder_1952, aes(x = pop, y = lifeExp)) + geom_point() + scale_x_log10() + facet_wrap(~ continent) 3.3 Grouping and summarizing Summarize Verb The function summarize() combines many observations from the same variable into one: # Filter for 1957 then summarize the median life expectancy and the maximum GDP per capita gapminder %&gt;% filter(year == 1957) %&gt;% summarize(medianLifeExp = median(lifeExp), maxGdpPercap = max(gdpPercap)) ## # A tibble: 1 × 2 ## medianLifeExp maxGdpPercap ## &lt;dbl&gt; &lt;dbl&gt; ## 1 48.4 113523. In this case, the code summarized the median life expectancy and maximum GDP/capita of all the countries and conitents from 1957. Group_by Verb This function group_by() is basically an advanced version of the filter() function. If the filter() along with the summarize() functions can summarize observations of variable(s) one of each filtered type at a time, then group_by() function can run the same calculations for all the observation units: # Find median life expectancy and maximum GDP per capita in each year gapminder %&gt;% group_by(year) %&gt;% summarize(medianLifeExp = median(lifeExp), maxGdpPercap = max(gdpPercap)) ## # A tibble: 12 × 3 ## year medianLifeExp maxGdpPercap ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1952 45.1 108382. ## 2 1957 48.4 113523. ## 3 1962 50.9 95458. ## 4 1967 53.8 80895. ## 5 1972 56.5 109348. ## 6 1977 59.7 59265. ## 7 1982 62.4 33693. ## 8 1987 65.8 31541. ## 9 1992 67.7 34933. ## 10 1997 69.4 41283. ## 11 2002 70.8 44684. ## 12 2007 71.9 49357. Visualizing the dataset Assigning the dataset to a variable then plug it in to the ggplot2 to make a graph: # Summarize medianGdpPercap within each continent within each year: by_year_continent by_year_continent &lt;- gapminder %&gt;% group_by(continent, year) %&gt;% summarize(medianGdpPercap = median(gdpPercap)) ## `summarise()` has grouped output by &#39;continent&#39;. You can override using the ## `.groups` argument. # Plot the change in medianGdpPercap in each continent over time ggplot(by_year_continent, aes(x = year, y = medianGdpPercap, color = continent)) + geom_point() + expand_limits(y = 0) 3.4 Types of visualizations Line Plot A line plot is useful for visualizing trends over time. Use geom_line() to create a line plot instead of geom_point() to create a scatter plot. The function expand_limits(y = 0) tells the y-axis to start from 0: # Create a line plot showing the change in medianGdpPercap by continent over time ggplot(by_year_continent, aes(x = year, y = medianGdpPercap, color = continent)) + geom_line() + expand_limits(y = 0) Bar Plot A bar plot is useful for visualizing summary statistics. use geom_col() to create a bar plot: # Filter for observations in the Oceania continent in 1952 oceania_1952 &lt;- gapminder %&gt;% filter(continent == &quot;Oceania&quot;, year == 1952) # Create a bar plot of gdpPercap by country ggplot(oceania_1952, aes(x = country, y = gdpPercap)) + geom_col() Histogram A histogram is useful for examining the distribution of a numeric variable. Every bar represents a bin of a variable, and the height of the bar represents how many observations fall into that bin. Use geom_histogram() to create a histogram: gapminder_1952 &lt;- gapminder %&gt;% filter(year == 1952) %&gt;% mutate(pop_by_mil = pop / 1000000) # Create a histogram of population (pop_by_mil) ggplot(gapminder_1952, aes(x = pop_by_mil)) + geom_histogram(bins = 50) Boxplot A boxplot is useful for comparing a distribution of values across several groups. # Create a boxplot comparing gdpPercap among continents ggplot(gapminder_1952, aes(x = continent, y = gdpPercap)) + geom_boxplot() + scale_y_log10() How To Add A Title To A Graph Use ggtitle() function to add a title to a graph: # Add a title to this graph: &quot;Comparing GDP per capita across continents&quot; ggplot(gapminder_1952, aes(x = continent, y = gdpPercap)) + geom_boxplot() + scale_y_log10() + ggtitle(&quot;Comparing GDP per capita across continents&quot;) "],["intro-to-data-visualization-with-ggplot2.html", "4 Intro to Data Visualization with ggplot2 4.1 Introduction 4.2 Aesthetics 4.3 Geometries 4.4 Themes", " 4 Intro to Data Visualization with ggplot2 https://learn.datacamp.com/courses/introduction-to-data-visualization-with-ggplot2 4.1 Introduction Changing One Geom Or Every Geom If you have multiple geoms, then mapping an aesthetic to data variable inside the call to ggplot() will change all the geoms. It is also possible to make changes to individual geoms by passing arguments to the geom_*() functions. geom_point() has an alpha argument that controls the opacity of the points. A value of 1 (the default) means that the points are totally opaque; a value of 0 means the points are totally transparent (and therefore invisible). Values in between specify transparency. geom_smooth() adds a smooth trend curve. ggplot(diamonds, aes(carat, price, color = clarity)) + geom_point(alpha = 0.4) + geom_smooth() ## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; Saving Plots As Variables Plots can be saved as variables, which can be added two later on using the + operator. This is really useful to make multiple related plots from a common base. # Assign the previous plot to a variable to make a base: plt_price_vs_carat &lt;- ggplot(diamonds, aes(carat, price)) # Assign the base plot to make a new plot: plt_price_vs_carat_by_clarity &lt;- plt_price_vs_carat + geom_point(aes(color = clarity)) # See the plot plt_price_vs_carat_by_clarity 4.2 Aesthetics Typical Visible Aesthetics ## Aesthetic Description ## 1 x X-axis position ## 2 y Y-axis position ## 3 fill Fill color ## 4 color Color of points, outlines of other geoms ## 5 size Area or radius of points, thickness of lines ## 6 alpha Transparency ## 7 linetype Line dash pattern ## 8 labels Text on a plot or axes ## 9 shape Shape All About Aesthetics: Color, Shape And Size These are the aesthetics to can consider within aes(): x, y, color, fill, size, alpha, labels and shape. diamonds ## # A tibble: 53,940 × 10 ## carat cut color clarity depth table price x y z ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 ## 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 ## 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 ## 4 0.29 Premium I VS2 62.4 58 334 4.2 4.23 2.63 ## 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 ## 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 ## 7 0.24 Very Good I VVS1 62.3 57 336 3.95 3.98 2.47 ## 8 0.26 Very Good H SI1 61.9 55 337 4.07 4.11 2.53 ## 9 0.22 Fair E VS2 65.1 61 337 3.87 3.78 2.49 ## 10 0.23 Very Good H VS1 59.4 61 338 4 4.05 2.39 ## # … with 53,930 more rows ggplot(diamonds, aes(carat, price, color = color)) + # Set the shape and size of the points geom_point(shape = 1, size = 1.5) All About Aesthetics: Color VS. Fill Typically, the color aesthetic changes the circumference line of a geom and the fill aesthetic changes the inside. geom_point() is an exception: you use color (not fill) for the point color. The default geom_point() uses shape = 19: a solid circle. An alternative is shape = 21: a circle that allow you to use both fill for the inside and color for the outline. This is lets you to map two aesthetics to each point: ggplot(diamonds, aes(carat, price, fill = clarity, color = color)) + geom_point(shape = 21, size = 4, alpha = 0.6) All About Aesthetics: Alpha, Shape And Label Be careful: the attributes can overwrite the aesthetics of a plot! Alpha # Base layer diamonds_E &lt;- diamonds %&gt;% filter(color == &quot;E&quot;) plt_diam_E &lt;- ggplot(diamonds_E, aes(carat, price)) # Map clarity to alpha: plt_diam_E + geom_point(aes(alpha = clarity)) Shape Use shape() function to substitute the observation symbols displayed in the plot into another variable: # Base layer diamonds_E &lt;- diamonds %&gt;% filter(color == &quot;E&quot;) plt_diam_E &lt;- ggplot(diamonds_E, aes(carat, price)) # Map cut to alpha: plt_diam_E + geom_point(aes(shape = cut)) ## Warning: Using shapes for an ordinal variable is not advised Label Use label() function to change the observation symbols displayed in the plot: diamonds_Fair &lt;- diamonds %&gt;% filter(cut == &quot;Fair&quot;) plt_diam_Fair &lt;- ggplot(diamonds_Fair, aes(carat, price)) # Use text layer and map color to label plt_diam_Fair + geom_text(aes(label = color)) All About Attributes: Color, Shape, And Size Use color(), shape(), and size() functions as attributes (means to not use inside ggplot()): # A hexadecimal color my_blue &lt;- &quot;#4ABEFF&quot; # Change the color mapping to a fill mapping ggplot(diamonds_Fair, aes(carat, price, fill = &quot;E&quot;)) + # Set point size and shape geom_point(color = my_blue, size = 7, shape = 5, alpha = 0.4) Aesthetic Label Functions Make use some of these functions for improving the appearance of the plot: labs() to set the x- and y-axis labels. It takes strings for each argument. scale_color_manual() defines properties of the color scale (i.e. axis). The first argument sets the legend title. values is a named vector of colors to use. palette &lt;- c(E = &quot;#377EB8&quot;) ggplot(diamonds_Fair, aes(carat, price, fill = &quot;E&quot;)) + geom_point(color = my_blue, size = 7, shape = 5, alpha = 0.4) + labs(x = &quot;This is a new x-axis name&quot;, y = &quot;New for Y&quot;) + scale_fill_manual(&quot;Color of the Diamonds&quot;, values = palette) Setting Axis Limits Specify the limits as separate arguments, or as a single numeric vector. That is, ylim(lo, hi) or ylim(c(lo, hi)). ggplot(mtcars, aes(mpg, 0)) + geom_jitter() + # Set the y-axis limits ylim(-2, 2) To spread out clustered variables use geom_jitter() or geom_point(position = \"jitter\"). 4.3 Geometries Scatter plots (using geom_point()) are intuitive, easily understood, and very common, but we must always consider overplotting, particularly in the following four situations: 1. Large datasets 2. Aligned values on a single axis 3. Low-precision data 4. Integer data How to Deal with Overplotting In Scatter Plots Adjust the numeric value in shape() function into \".\"to deal with overflowed data: # Plot price vs. carat, colored by clarity plt_price_vs_carat_by_clarity &lt;- ggplot(diamonds, aes(carat, price, color = clarity)) # Add a point layer with tiny points plt_price_vs_carat_by_clarity + geom_point(alpha = 0.5, shape = &quot;.&quot;) How to Deal with “Aligned Values” And “Low-precision Data” In Scatter Plots Aligning values on a single axis occurs when one axis is continuous and the other is categorical, which can be overcome with some form of jittering: ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + # Set the position to jitter geom_point(position = &quot;jitter&quot;, alpha = 0.5) Another way to jitter which gives the same result is: ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + # Use a jitter position function with width 0.1 geom_point(position = position_jitter(width = 0.1), alpha = 0.5) Positions In Histograms Histograms are specialized versions of bar plots. There’re various ways of applying positions to histograms. geom_histogram(), a special case of geom_bar(), has a position argument that can take on the following values: stack (the default): Bars for different groups are stacked on top of each other. dodge: Bars for different groups are placed side by side. geom_histogram(binwidth = 1, position = “dodge”) fill: Bars for different groups are shown as proportions. geom_histogram(binwidth = 1, position = “fill”) identity: Plot the values as they appear in the dataset. geom_histogram(binwidth = 1, position = “identity,” alpha = 0.4) Bar Plots geom_bar() counts the number of cases at each x position. geom_col() plot actual values. NOTE:the function geom_col() is just geom_bar() where both the position and stat arguments are set to “identity.” It is used when we want the heights of the bars to represent the exact values in the data. geom_bar() have three position options: stack: The default dodge: Preferred geom_bar(position = “dodge”) fill: To show proportions geom_bar(position = “fill”) Overlapping Bar Plots Instead of using position = \"dodge\", use position_dodge(), the same with position_jitter() to specify how much dodging (or jittering) wanted. geom_bar(position = position_dodge(width = 0.2)) 4.4 Themes The themes layer is the visual elements that aren’t part of the data. There’re 3 types: ## types modified_using ## 1 text element_text() ## 2 line element_line() ## 3 rectangle element_rect() To change stylistic elements of a plot, call theme() and set plot properties to a new value. Moving The Legend The legend is that little box on the right side of the plot that shows up when functions like color or shape is plotted: p + theme(legend.position = new_value) The new value can be: &quot;top&quot;, &quot;bottom&quot;, &quot;left&quot;, or &quot;right&#39;&quot;: place it at that side of the plot. &quot;none&quot;: don&#39;t draw it. ” c(x, y)“:”c(0, 0)” means the bottom-left and “c(1, 1)” means the top-right. ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point(position = position_jitter(width = 0.1), alpha = 0.5) + theme(legend.position = &quot;left&quot;) Modifying Theme Elements Many plot elements have multiple properties that can be set. Line elements in the plot such as axes and gridlines have a color, a thickness (size), and a line type (solid line, dashed, or dotted). To set the style of a line, you use element_line(). For example, to make the axis lines into red, dashed lines: p + theme(axis.line = element_line(color = &quot;red&quot;, linetype = &quot;dashed&quot;)) Similarly, element_rect() changes rectangles and element_text() changes text. To remove a plot element, use element_blank(). ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point(position = position_jitter(width = 0.1), alpha = 0.5) + theme(legend.position = &quot;left&quot;, rect = element_rect(fill = &quot;grey92&quot;), legend.key = element_rect(color = NA), # Turn off axis ticks axis.ticks = element_blank(), # Turn off the panel grid panel.grid = element_blank() ) Modifying Whitespace Whitespace means all the non-visible margins and spacing in the plot. To set a single whitespace value, use unit(x, unit), where x is the amount and unit is the unit of measure. Borders have 4 positions, use margin(top, right, bottom, left, unit). The margin order is in a clock-wise position starting from 12 o’clock. The default unit is \"pt\" (points), which scales well with text. Other options include \"cm\", \"in\" (inches) and \"lines\" (of text). ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point(position = position_jitter(width = 0.1), alpha = 0.5) + theme(legend.margin = margin(1, 1, 70, 10, &quot;mm&quot;)) Built-in Themes In addition to making your own themes, there are several out-of-the-box solutions that may save you lots of time. &quot;theme_gray()&quot; is the default. &quot;theme_bw()&quot; is useful when you use transparency. &quot;theme_classic()&quot; is more traditional. &quot;theme_void()&quot; removes everything but the data. Setting Themes Install the package ggthemes, which is a another source of built-in themes just like ggplot2. Reusing a theme across many plots helps to provide a consistent style. You have several options for this. 1. Assign the theme to a variable, and add it to each plot. 2. Set the theme as the default using &quot;theme_set()&quot;. library(ggthemes) new_theme &lt;- theme( rect = element_rect(fill = &quot;grey92&quot;), legend.key = element_rect(color = NA), axis.ticks = element_blank(), panel.grid = element_blank(), panel.grid.major.y = element_line(color = &quot;white&quot;, size = 0.5, linetype = &quot;dotted&quot;), axis.text = element_text(color = &quot;grey25&quot;), plot.title = element_text(face = &quot;italic&quot;, size = 16), legend.position = c(0.6, 0.1) ) # Combine the WSJ theme with new_theme new_theme_wsj &lt;- theme_wsj() + new_theme # Add the combined theme to the plot ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point(position = position_jitter(width = 0.1), alpha = 0.5) + new_theme_wsj NOTE: add another theme after an already existing theme to overide the settings of the previous theme. Using Geoms For Explanatory Plots This type of plot will be in an info-viz style, meaning that it would be similar to something you’d see in a magazine or website for a mostly lay audience. # Add a geom_segment() layer (Key component to create the plot structure below) ggplot(gm2007, aes(x = lifeExp, y = country, color = lifeExp)) + geom_point(size = 4) + geom_segment(aes(xend = 30, yend = country), size = 2) + # Add a geom_text() layer geom_text(aes(label = round(lifeExp,1)), color = &quot;white&quot;, size = 1.5) + # Modify the scales scale_x_continuous(&quot;&quot;, expand = c(0,0), limits = c(30,90), position = &quot;top&quot;) + # Add a title and caption labs(title = &quot;Highest and lowest life expectancies, 2007&quot;, caption = &quot;Source: gapminder&quot;) "],["working-with-data-in-the-tidyverse.html", "5 Working with Data in the Tidyverse 5.1 Explore your data 5.2 Tame your data 5.3 Tidy your data 5.4 Transform your data", " 5 Working with Data in the Tidyverse https://learn.datacamp.com/courses/working-with-data-in-the-tidyverse 5.1 Explore your data Load the readr package for every session so things work properly: library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union library(readr) election &lt;- read_csv(&quot;http://faculty.baruch.cuny.edu/geoportal/data/county_election/elpo12p010g.csv&quot;) ## Rows: 3153 Columns: 14 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (5): FIPS, STATE_FIPS, STATE, COUNTY, WINNER ## dbl (9): OBAMA, ROMNEY, OTHERS, TTL_VT, PCT_OBM, PCT_ROM, PCT_OTHR, PCT_WNR,... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. election ## # A tibble: 3,153 × 14 ## FIPS STATE_FIPS STATE COUNTY OBAMA ROMNEY OTHERS TTL_VT PCT_OBM PCT_ROM ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 01001 01 AL Autauga 6363 17379 231 23973 26.5 72.5 ## 2 01003 01 AL Baldwin 18329 65772 887 84988 21.6 77.4 ## 3 01005 01 AL Barbour 5912 5550 55 11517 51.3 48.2 ## 4 01007 01 AL Bibb 2202 6132 86 8420 26.2 72.8 ## 5 01009 01 AL Blount 2970 20757 333 24060 12.3 86.3 ## 6 01011 01 AL Bullock 4061 1251 10 5322 76.3 23.5 ## 7 01013 01 AL Butler 4374 5087 41 9502 46.0 53.5 ## 8 01015 01 AL Calhoun 15500 30272 468 46240 33.5 65.5 ## 9 01017 01 AL Chambers 6871 7626 132 14629 47.0 52.1 ## 10 01019 01 AL Cherokee 2132 7506 154 9792 21.8 76.7 ## # … with 3,143 more rows, and 4 more variables: PCT_OTHR &lt;dbl&gt;, WINNER &lt;chr&gt;, ## # PCT_WNR &lt;dbl&gt;, group &lt;dbl&gt; #To skip lines from the top, start from the 2nd line for example, use &quot;skip = 1&quot; after comma in read_csv(...,) readr is for rectangular data with extensions like: .csv, .tsv, .fwf, and .log read_csv() function to read a csv file. readxl() to read Microsoft Excel files. Assign Missing Values The read_csv() function also has an na argument, which allows you to specify value(s) that represent missing values in your data. The default values for the na argument are c(\"\", \"NA\"), so both are recorded as missing (NA) in R. When you read in data, you can add additional values like the string \"UNKNOWN\" to a vector of missing values using the c() function to combine multiple values into a single vector. The is.na() function is also helpful for identifying rows with missing values for a variable. read_csv(&quot;electiomn.csv&quot;, skip = 1, na = c(&quot;&quot;, &quot;NA&quot;, &quot;A new unspecified variable&quot;)) election %&gt;% filter(is.na(variable you want to filter to find na)) The election data didn’t have any observations classified as a na so the example above is for when assuming there’s a missing value you want to filter/find. How to list out all the variables of a dataset Use glimpse() to list out the hidden variables in the dataset: glimpse(election) ## Rows: 3,153 ## Columns: 14 ## $ FIPS &lt;chr&gt; &quot;01001&quot;, &quot;01003&quot;, &quot;01005&quot;, &quot;01007&quot;, &quot;01009&quot;, &quot;01011&quot;, &quot;0101… ## $ STATE_FIPS &lt;chr&gt; &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;,… ## $ STATE &lt;chr&gt; &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;,… ## $ COUNTY &lt;chr&gt; &quot;Autauga&quot;, &quot;Baldwin&quot;, &quot;Barbour&quot;, &quot;Bibb&quot;, &quot;Blount&quot;, &quot;Bullock… ## $ OBAMA &lt;dbl&gt; 6363, 18329, 5912, 2202, 2970, 4061, 4374, 15500, 6871, 213… ## $ ROMNEY &lt;dbl&gt; 17379, 65772, 5550, 6132, 20757, 1251, 5087, 30272, 7626, 7… ## $ OTHERS &lt;dbl&gt; 231, 887, 55, 86, 333, 10, 41, 468, 132, 154, 156, 38, 56, … ## $ TTL_VT &lt;dbl&gt; 23973, 84988, 11517, 8420, 24060, 5322, 9502, 46240, 14629,… ## $ PCT_OBM &lt;dbl&gt; 26.54236, 21.56657, 51.33281, 26.15202, 12.34414, 76.30590,… ## $ PCT_ROM &lt;dbl&gt; 72.49406, 77.38975, 48.18963, 72.82660, 86.27182, 23.50620,… ## $ PCT_OTHR &lt;dbl&gt; 0.963584, 1.043677, 0.477555, 1.021378, 1.384040, 0.187899,… ## $ WINNER &lt;chr&gt; &quot;Romney&quot;, &quot;Romney&quot;, &quot;Obama&quot;, &quot;Romney&quot;, &quot;Romney&quot;, &quot;Obama&quot;, &quot;… ## $ PCT_WNR &lt;dbl&gt; 72.49406, 77.38975, 51.33281, 72.82660, 86.27182, 76.30590,… ## $ group &lt;dbl&gt; 24, 24, 12, 24, 25, 14, 22, 23, 22, 24, 24, 22, 22, 24, 25,… Combine glimpse() with other functions in a sequence using the pipe (%&gt;%) operator. For example, function like arrange: election %&gt;% arrange(OBAMA) %&gt;% glimpse() # no argument needed here ## Rows: 3,153 ## Columns: 14 ## $ FIPS &lt;chr&gt; &quot;48269&quot;, &quot;48301&quot;, &quot;31009&quot;, &quot;31005&quot;, &quot;31075&quot;, &quot;48431&quot;, &quot;4803… ## $ STATE_FIPS &lt;chr&gt; &quot;48&quot;, &quot;48&quot;, &quot;31&quot;, &quot;31&quot;, &quot;31&quot;, &quot;48&quot;, &quot;48&quot;, &quot;48&quot;, &quot;51&quot;, &quot;31&quot;,… ## $ STATE &lt;chr&gt; &quot;TX&quot;, &quot;TX&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;NE&quot;, &quot;TX&quot;, &quot;TX&quot;, &quot;TX&quot;, &quot;VA&quot;, &quot;NE&quot;,… ## $ COUNTY &lt;chr&gt; &quot;King&quot;, &quot;Loving&quot;, &quot;Blaine&quot;, &quot;Arthur&quot;, &quot;Grant&quot;, &quot;Sterling&quot;, … ## $ OBAMA &lt;dbl&gt; 5, 9, 29, 30, 30, 31, 32, 33, 37, 41, 42, 44, 49, 51, 55, 5… ## $ ROMNEY &lt;dbl&gt; 139, 54, 268, 227, 322, 459, 324, 468, 6463, 237, 360, 526,… ## $ OTHERS &lt;dbl&gt; 1, 1, 6, 5, 11, 4, 7, 7, 222, 13, 6, 8, 9, 12, 10, 7, 6, 6,… ## $ TTL_VT &lt;dbl&gt; 145, 64, 303, 262, 363, 494, 363, 508, 6722, 291, 408, 578,… ## $ PCT_OBM &lt;dbl&gt; 3.448276, 14.062500, 9.570957, 11.450382, 8.264463, 6.27530… ## $ PCT_ROM &lt;dbl&gt; 95.86207, 84.37500, 88.44885, 86.64122, 88.70523, 92.91498,… ## $ PCT_OTHR &lt;dbl&gt; 0.689655, 1.562500, 1.980198, 1.908397, 3.030303, 0.809717,… ## $ WINNER &lt;chr&gt; &quot;Romney&quot;, &quot;Romney&quot;, &quot;Romney&quot;, &quot;Romney&quot;, &quot;Romney&quot;, &quot;Romney&quot;,… ## $ PCT_WNR &lt;dbl&gt; 95.86207, 84.37500, 88.44885, 86.64122, 88.70523, 92.91498,… ## $ group &lt;dbl&gt; 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,… Compare to the two code chunks, this arraged data shows the ascending order of OBAMA. How to get a list of summarized statistics of all the variables of a dataset Use skim() to list the standard statistics of the variables: library(skimr) skim(election) Table 5.1: Data summary Name election Number of rows 3153 Number of columns 14 _______________________ Column type frequency: character 5 numeric 9 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace FIPS 0 1 5 5 0 3153 0 STATE_FIPS 0 1 2 2 0 51 0 STATE 0 1 2 2 0 51 0 COUNTY 0 1 2 23 0 1862 0 WINNER 0 1 5 6 0 2 0 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist OBAMA 0 1 20897.02 73657.67 5.00 1572.00 3959.00 11259.00 2216903.00 ▇▁▁▁▁ ROMNEY 0 1 19321.72 44465.73 54.00 2926.00 6294.00 16032.00 885333.00 ▇▁▁▁▁ OTHERS 0 1 683.27 2262.16 0.00 72.00 178.00 485.00 78831.00 ▇▁▁▁▁ TTL_VT 0 1 40902.02 116118.03 64.00 4862.00 10565.00 28049.00 3181067.00 ▇▁▁▁▁ PCT_OBM 0 1 38.54 14.79 0.55 27.77 37.28 47.56 93.39 ▂▇▇▂▁ PCT_ROM 0 1 59.63 14.78 5.98 50.47 60.72 70.25 96.15 ▁▂▇▇▂ PCT_OTHR 0 1 1.83 1.00 0.00 1.17 1.69 2.35 14.87 ▇▁▁▁▁ PCT_WNR 0 1 64.29 9.97 47.87 56.12 63.06 71.31 96.15 ▇▇▆▂▁ group 0 1 20.70 4.52 11.00 22.00 23.00 24.00 25.00 ▃▁▁▃▇ You can combine skim() with other functions in a sequence using the pipe (%&gt;%) operator. For example, use function summary() to find how many variables of each type are in the dataset: election %&gt;% skim() %&gt;% # no argument needed here summary() # no argument needed here Table 5.2: Data summary Name Piped data Number of rows 3153 Number of columns 14 _______________________ Column type frequency: character 5 numeric 9 ________________________ Group variables None Count the data Distinct() Use distinct() to find out how many different type of observations are there in one variable. To find out how many distinct states are there in the STATE variable: election %&gt;% distinct(STATE) ## # A tibble: 51 × 1 ## STATE ## &lt;chr&gt; ## 1 AL ## 2 AZ ## 3 AR ## 4 CA ## 5 CO ## 6 CT ## 7 DE ## 8 DC ## 9 FL ## 10 GA ## # … with 41 more rows Count() count() adds a new column named n to store the counts. this count function basically does the group_by and summarize for you. count the number of counties in each state: election %&gt;% count(STATE) ## # A tibble: 51 × 2 ## STATE n ## &lt;chr&gt; &lt;int&gt; ## 1 AK 40 ## 2 AL 67 ## 3 AR 75 ## 4 AZ 15 ## 5 CA 58 ## 6 CO 64 ## 7 CT 8 ## 8 DC 1 ## 9 DE 3 ## 10 FL 67 ## # … with 41 more rows Adapt the code to count by a logical condition instead: election %&gt;% count(WINNER == &quot;Obama&quot;) ## # A tibble: 2 × 2 ## `WINNER == &quot;Obama&quot;` n ## &lt;lgl&gt; &lt;int&gt; ## 1 FALSE 2449 ## 2 TRUE 704 5.2 Tame your data Cast Column Types To assign or change the type of the columns in the dataset. Cast A Column To a Date Use parse_date(\"2012-14-08\", format = \"%Y-%d-%m\") then use col_date(format = \"%Y-%d-%m\") within cols() as the col_types argument of read_csv(). parse_date(&quot;2012-14-08&quot;, format = &quot;%Y-%d-%m&quot;) read_csv(...., col_types = cols( variale_name = col_date(format = &quot;%Y-%d-%m&quot;))) Cast A Column To a Number read_csv(...., variable_name = col_number()) Sometimes, there are na contained in the observations which would create errors while trying to cast. Diagnose parsing problems using a new readr function called problems(). Using problems() on a result of read_csv() will show you the rows and columns where parsing error occurred, what the parser expected to find (for example, a number), and the actual value that caused the parsing error. Cast A Column As A Factor Factors are categorical variables, where the possible values are a fixed and known set. Use parse_factor() to parse variables and col_factor() to cast columns as categorical. Both functions have a levels argument that is used to specify the possible values for the factors. When levels is set to NULL, the possible values will be inferred from the unique values in the dataset. Alternatively, you can pass a list of possible values. read_csv(...., variable_name = col_factor(levels = NULL)) For more details, go to the Cast A Factor And Examine Levels section of this chapter. Recode Values Use recode() function in the dplyr package. The recode function is to re-name the observations into something easier to understand. Recode A Character Variable election_CA &lt;- election %&gt;% filter(STATE == &quot;CA&quot;) election_CA %&gt;% mutate(STATE = recode(STATE, &quot;CA&quot; = &quot;California&quot;)) ## # A tibble: 58 × 14 ## FIPS STATE_FIPS STATE COUNTY OBAMA ROMNEY OTHERS TTL_VT PCT_OBM PCT_ROM ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 06001 06 Califo… Alameda 469684 108182 17776 595642 78.9 18.2 ## 2 06003 06 Califo… Alpine 389 236 28 653 59.6 36.1 ## 3 06005 06 Califo… Amador 6830 10281 538 17649 38.7 58.3 ## 4 06007 06 Califo… Butte 42669 44479 3604 90752 47.0 49.0 ## 5 06009 06 Califo… Calaver… 8670 12365 751 21786 39.8 56.8 ## 6 06011 06 Califo… Colusa 2314 3601 119 6034 38.3 59.7 ## 7 06013 06 Califo… Contra … 290824 136517 10885 438226 66.4 31.2 ## 8 06015 06 Califo… Del Nor… 3791 4614 365 8770 43.2 52.6 ## 9 06017 06 Califo… El Dora… 35166 50973 2635 88774 39.6 57.4 ## 10 06019 06 Califo… Fresno 129129 124490 5208 258827 49.9 48.1 ## # … with 48 more rows, and 4 more variables: PCT_OTHR &lt;dbl&gt;, WINNER &lt;chr&gt;, ## # PCT_WNR &lt;dbl&gt;, group &lt;dbl&gt; Recode A Numeric Variable Into Factor Dummy variables are often used in data analysis to bin a variable into one of two categories to indicate the absence or presence of something. Dummy variables take the value 0 or 1 to stand for, for example, V_engine or S_engine. Car_engine &lt;- mtcars[1:10,] %&gt;% mutate(Engine_Type = recode(vs, &quot;0&quot; = &quot;V_engine&quot;, .default = &quot;S_engine&quot;)) %&gt;% select(vs, Engine_Type, everything(), -&quot;carb&quot;) Car_engine ## vs Engine_Type mpg cyl disp hp drat wt qsec am gear ## Mazda RX4 0 V_engine 21.0 6 160.0 110 3.90 2.620 16.46 1 4 ## Mazda RX4 Wag 0 V_engine 21.0 6 160.0 110 3.90 2.875 17.02 1 4 ## Datsun 710 1 S_engine 22.8 4 108.0 93 3.85 2.320 18.61 1 4 ## Hornet 4 Drive 1 S_engine 21.4 6 258.0 110 3.08 3.215 19.44 0 3 ## Hornet Sportabout 0 V_engine 18.7 8 360.0 175 3.15 3.440 17.02 0 3 ## Valiant 1 S_engine 18.1 6 225.0 105 2.76 3.460 20.22 0 3 ## Duster 360 0 V_engine 14.3 8 360.0 245 3.21 3.570 15.84 0 3 ## Merc 240D 1 S_engine 24.4 4 146.7 62 3.69 3.190 20.00 0 4 ## Merc 230 1 S_engine 22.8 4 140.8 95 3.92 3.150 22.90 0 4 ## Merc 280 1 S_engine 19.2 6 167.6 123 3.92 3.440 18.30 0 4 #Since there&#39;re only 2 distinct levels, the &quot;1&quot; = &quot;S_engine&quot; can be coded as &quot;.default&quot;. Select And Reorder Variables Selecting a subset of columns to print can help check that a mutate() worked as expected, and rearranging columns next to each other can help spot obvious errors in data entry. The select() helpers are functions that allow selection of variables based on their names: ## Function Usage ## 1 starts_with() starts with a prefix ## 2 ends_with() ends with a prefix ## 3 contains() contains a literal string ## 4 matches() matches a regular expression ## 5 num_range() a numerical range like x01, x02, x03. ## 6 one_of() variables in character vector. ## 7 everything() all variables. ## 8 last_col() last variable, possibly with an offset. # Move vs, and Engine_type to front and show only from mpg to drat: Car_engine[1:10,] %&gt;% select(vs, Engine_Type, mpg:drat) ## vs Engine_Type mpg cyl disp hp drat ## Mazda RX4 0 V_engine 21.0 6 160.0 110 3.90 ## Mazda RX4 Wag 0 V_engine 21.0 6 160.0 110 3.90 ## Datsun 710 1 S_engine 22.8 4 108.0 93 3.85 ## Hornet 4 Drive 1 S_engine 21.4 6 258.0 110 3.08 ## Hornet Sportabout 0 V_engine 18.7 8 360.0 175 3.15 ## Valiant 1 S_engine 18.1 6 225.0 105 2.76 ## Duster 360 0 V_engine 14.3 8 360.0 245 3.21 ## Merc 240D 1 S_engine 24.4 4 146.7 62 3.69 ## Merc 230 1 S_engine 22.8 4 140.8 95 3.92 ## Merc 280 1 S_engine 19.2 6 167.6 123 3.92 Reformat Variable Names To change names WITHOUT changing the order of the variables, write everything() first in the select() function. The function clean_names() takes an argument case that can be used to convert variable names to other cases, like \"upper_camel\" or \"all_caps\". use clean_names() from the janitor package to convert all variable names to snake_case. install.packages(&quot;janitor&quot;) library(janitor) ## ## Attaching package: &#39;janitor&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## chisq.test, fisher.test election[1:10,] %&gt;% clean_names(&quot;snake&quot;) ## # A tibble: 10 × 14 ## fips state_fips state county obama romney others ttl_vt pct_obm pct_rom ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 01001 01 AL Autauga 6363 17379 231 23973 26.5 72.5 ## 2 01003 01 AL Baldwin 18329 65772 887 84988 21.6 77.4 ## 3 01005 01 AL Barbour 5912 5550 55 11517 51.3 48.2 ## 4 01007 01 AL Bibb 2202 6132 86 8420 26.2 72.8 ## 5 01009 01 AL Blount 2970 20757 333 24060 12.3 86.3 ## 6 01011 01 AL Bullock 4061 1251 10 5322 76.3 23.5 ## 7 01013 01 AL Butler 4374 5087 41 9502 46.0 53.5 ## 8 01015 01 AL Calhoun 15500 30272 468 46240 33.5 65.5 ## 9 01017 01 AL Chambers 6871 7626 132 14629 47.0 52.1 ## 10 01019 01 AL Cherokee 2132 7506 154 9792 21.8 76.7 ## # … with 4 more variables: pct_othr &lt;dbl&gt;, winner &lt;chr&gt;, pct_wnr &lt;dbl&gt;, ## # group &lt;dbl&gt; #Notice how all the variables are lower_case now. How To Rename, Subset, And Reorder Variables At Once To rename, then subset (choose to show only selected) , and finally reorder variables in one code line, use select(). dataset_name %&gt;% select( new_variable_name_ = starts_with(&quot;old_name&quot;) ) This code will find all variables in dataset_name whose names start with old_name, then rename each variable as new_name_&lt;N&gt;, where N is a number. If dataset_name has variables oldname, oldname_v1, oldname3, then the code will replace these names with new_name_1, new_name_2, new_name_3. The arguments inputted into select() determines what R will show. And, the order of the arguments inputted will determine how the resulting order of the variables will be. election[1:10,] %&gt;% select(Precint_ = contains(&quot;PCT&quot;), everything(), -&quot;group&quot;) ## # A tibble: 10 × 13 ## Precint_1 Precint_2 Precint_3 Precint_4 FIPS STATE_FIPS STATE COUNTY OBAMA ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 26.5 72.5 0.964 72.5 01001 01 AL Autauga 6363 ## 2 21.6 77.4 1.04 77.4 01003 01 AL Baldwin 18329 ## 3 51.3 48.2 0.478 51.3 01005 01 AL Barbour 5912 ## 4 26.2 72.8 1.02 72.8 01007 01 AL Bibb 2202 ## 5 12.3 86.3 1.38 86.3 01009 01 AL Blount 2970 ## 6 76.3 23.5 0.188 76.3 01011 01 AL Bullock 4061 ## 7 46.0 53.5 0.431 53.5 01013 01 AL Butler 4374 ## 8 33.5 65.5 1.01 65.5 01015 01 AL Calhoun 15500 ## 9 47.0 52.1 0.902 52.1 01017 01 AL Chambers 6871 ## 10 21.8 76.7 1.57 76.7 01019 01 AL Cherokee 2132 ## # … with 4 more variables: ROMNEY &lt;dbl&gt;, OTHERS &lt;dbl&gt;, TTL_VT &lt;dbl&gt;, ## # WINNER &lt;chr&gt; This example, the PCT_name has been replaced into Precint_&lt;N&gt;. The Precint_ is first in the select() function so it will be shown first, then next is everything() so everything else will be shown the way it is, then exclude the group variable. 5.3 Tidy your data gather() ?gather The gather() function collapses multiple columns into two columns. It reshapes the dataset from wide to long, it reduces the number of columns and increases the number of rows. library(tidyr) Car_engine[1:2,] %&gt;% gather(key = &quot;measurements&quot;, value = &quot;specs&quot;, mpg:hp) %&gt;% select(measurements, specs) ## measurements specs ## 1 mpg 21 ## 2 mpg 21 ## 3 cyl 6 ## 4 cyl 6 ## 5 disp 160 ## 6 disp 160 ## 7 hp 110 ## 8 hp 110 seperate() Within a tidy dataset, a column should only represent only one variable, but if observations from that variable contains two distinct type of info, we can seperate it with seperate(). This is an example for the code before/after using the seperate funtion: week_ratings series episode viewers_7day &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; 1 1 e1_7day 2.24 2 2 e1_7day 3.1 3 3 e1_7day 3.85 4 4 e1_7day 6.6 5 5 e1_7day 8.51 6 6 e1_7day 11.6 7 7 e1_7day 13.6 8 8 e1_7day 9.46 9 1 e2_7day 3 10 2 e2_7day 3.53 week_ratings &lt;- ratings2 %&gt;% select(series, ends_with(&quot;7day&quot;)) %&gt;% gather(episode, viewers_7day, ends_with(&quot;7day&quot;), na.rm = TRUE) %&gt;% # Edit to separate key column and drop extra separate(episode, into = c(&quot;episode&quot;,&quot;day&quot;)) week_ratings series episode day viewers_7day &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 1 e1 7day 2.24 2 2 e1 7day 3.1 3 3 e1 7day 3.85 4 4 e1 7day 6.6 5 5 e1 7day 8.51 6 6 e1 7day 11.6 7 7 e1 7day 13.6 8 8 e1 7day 9.46 9 1 e2 7day 3 10 2 e2 7day 3.53 Unite Columns In the tidyr package, the opposite of separate() isunite(). Sometimes you need to paste values from two or more columns together to tidy. Here is an example usage for unite(): data %&gt;% unite(new_var, old_var1, old_var2) To apply the function into the election data, merge the COUNTY into the STATE: election_CA[1:5,] %&gt;% unite(LOCATION, STATE, COUNTY, sep = &quot;, &quot;) ## # A tibble: 5 × 13 ## FIPS STATE_FIPS LOCATION OBAMA ROMNEY OTHERS TTL_VT PCT_OBM PCT_ROM PCT_OTHR ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 06001 06 CA, Ala… 469684 108182 17776 595642 78.9 18.2 2.98 ## 2 06003 06 CA, Alp… 389 236 28 653 59.6 36.1 4.29 ## 3 06005 06 CA, Ama… 6830 10281 538 17649 38.7 58.3 3.05 ## 4 06007 06 CA, But… 42669 44479 3604 90752 47.0 49.0 3.97 ## 5 06009 06 CA, Cal… 8670 12365 751 21786 39.8 56.8 3.45 ## # … with 3 more variables: WINNER &lt;chr&gt;, PCT_WNR &lt;dbl&gt;, group &lt;dbl&gt; spread() Spreading reshapes the data from long to wide, adds columns and shrinks the rows. tidy_ratings_all[1:10,] ## series episode days viewers ## 1 1 1 7 2.24 ## 2 2 1 7 3.10 ## 3 3 1 7 3.85 ## 4 4 1 7 6.60 ## 5 5 1 7 8.51 ## 6 6 1 7 11.62 ## 7 7 1 7 13.58 ## 8 8 1 7 9.46 ## 9 6 1 28 11.73 ## 10 7 1 28 13.86 tidy_ratings_all %&gt;% # Count viewers by series and days count(series, days, wt = viewers) %&gt;% # Adapt to spread counted values spread(days, n, sep = &quot;_&quot;) ## series days_7 days_28 ## 1 1 16.620 NA ## 2 2 31.610 NA ## 3 3 50.010 NA ## 4 4 73.540 NA ## 5 5 100.393 NA ## 6 6 123.110 113.00 ## 7 7 135.630 138.45 ## 8 8 90.170 92.87 5.4 Transform your data How To Create A Range-filtered Column Use case_when() to create a new column that represents a given range: bakers_sample &lt;- bakers[10:17,] %&gt;% select(baker, star_baker, technical_winner) bakers_sample ## baker star_baker technical_winner ## 10 Ruth 0 0 ## 11 Ben 0 1 ## 12 Holly 2 2 ## 13 Ian 0 0 ## 14 Janet 1 1 ## 15 Jason 2 1 ## 16 Joanne 1 3 ## 17 Keith 0 0 # Create skills variable with 4 levels bakers_skill &lt;- bakers_sample %&gt;% mutate(skill = case_when( star_baker &gt; technical_winner ~ &quot;super_star&quot;, star_baker &lt; technical_winner ~ &quot;high_tech&quot;, star_baker == 0 &amp; technical_winner == 0 ~ NA_character_, star_baker == technical_winner ~ &quot;well_rounded&quot; )) %&gt;% drop_na(skill) bakers_skill ## baker star_baker technical_winner skill ## 1 Ben 0 1 high_tech ## 2 Holly 2 2 well_rounded ## 3 Janet 1 1 well_rounded ## 4 Jason 2 1 super_star ## 5 Joanne 1 3 high_tech Cast A Factor And Examine Levels Cast skill as a factor: bakers_fct_skill &lt;- bakers_skill %&gt;% mutate(skill = as.factor(skill)) # Examine levels bakers_fct_skill %&gt;% pull(skill) %&gt;% levels() ## [1] &quot;high_tech&quot; &quot;super_star&quot; &quot;well_rounded&quot; For more details, go to the Cast A Column As A Factor section of this chapter. Cast Characters As Dates Use lubridate to parse and cast a date variable within a mutate(). ?lubridate library(lubridate) ## ## Attaching package: &#39;lubridate&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## date, intersect, setdiff, union bakers %&gt;% mutate(last_date_appeared = as.character(last_date_appeared)) %&gt;% mutate(last_date = ymd(last_date_appeared), last_month = month(last_date_appeared, label = TRUE)) %&gt;% select(baker, last_date_appeared, last_date, last_month) ## baker last_date_appeared last_date last_month ## 1 Annetha 2010-08-24 2010-08-24 Aug ## 2 David 2010-09-07 2010-09-07 Sep ## 3 Edd 2010-09-21 2010-09-21 Sep ## 4 Jasminder 2010-09-14 2010-09-14 Sep ## 5 Jonathan 2010-08-31 2010-08-31 Aug ## 6 Lea 2010-08-17 2010-08-17 Aug ## 7 Louise 2010-08-24 2010-08-24 Aug ## 8 Mark 2010-08-17 2010-08-17 Aug ## 9 Miranda 2010-09-21 2010-09-21 Sep ## 10 Ruth 2010-09-21 2010-09-21 Sep ## 11 Ben 2011-09-06 2011-09-06 Sep ## 12 Holly 2011-10-04 2011-10-04 Oct ## 13 Ian 2011-08-30 2011-08-30 Aug ## 14 Janet 2011-09-27 2011-09-27 Sep ## 15 Jason 2011-09-13 2011-09-13 Sep ## 16 Joanne 2011-10-04 2011-10-04 Oct ## 17 Keith 2011-08-16 2011-08-16 Aug ## 18 Mary-Anne 2011-10-04 2011-10-04 Oct ## 19 Robert 2011-09-13 2011-09-13 Sep ## 20 Simon 2011-08-23 2011-08-23 Aug ## 21 Urvashi 2011-08-30 2011-08-30 Aug ## 22 Yasmin 2011-09-20 2011-09-20 Sep ## 23 Brendan 2012-10-16 2012-10-16 Oct ## 24 Cathryn 2012-10-02 2012-10-02 Oct ## 25 Danny 2012-10-09 2012-10-09 Oct ## 26 James 2012-10-16 2012-10-16 Oct ## 27 John 2012-10-16 2012-10-16 Oct ## 28 Manisha 2012-09-11 2012-09-11 Sep ## 29 Natasha 2012-08-14 2012-08-14 Aug ## 30 Peter 2012-08-21 2012-08-21 Aug ## 31 Ryan 2012-09-25 2012-09-25 Sep ## 32 Sarah-Jane 2012-09-25 2012-09-25 Sep ## 33 Victoria 2012-08-28 2012-08-28 Aug ## 34 Stuart 2012-09-04 2012-09-04 Sep ## 35 Ali 2013-09-10 2013-09-10 Sep ## 36 Beca 2013-10-15 2013-10-15 Oct ## 37 Christine 2013-10-08 2013-10-08 Oct ## 38 Deborah 2013-09-03 2013-09-03 Sep ## 39 Frances 2013-10-22 2013-10-22 Oct ## 40 Glenn 2013-10-01 2013-10-01 Oct ## 41 Howard 2013-09-24 2013-09-24 Sep ## 42 Kimberley 2013-10-22 2013-10-22 Oct ## 43 Lucy 2013-08-27 2013-08-27 Aug ## 44 Mark 2013-09-03 2013-09-03 Sep ## 45 Robert 2013-09-17 2013-09-17 Sep ## 46 Ruby 2013-10-22 2013-10-22 Oct ## 47 Toby 2013-08-20 2013-08-20 Aug ## 48 Chetna 2014-10-01 2014-10-01 Oct ## 49 Claire 2014-08-06 2014-08-06 Aug ## 50 Diana 2014-09-03 2014-09-03 Sep ## 51 Enwezor 2014-08-13 2014-08-13 Aug ## 52 Iain 2014-08-27 2014-08-27 Aug ## 53 Jordan 2014-08-20 2014-08-20 Aug ## 54 Kate 2014-09-17 2014-09-17 Sep ## 55 Luis 2014-10-08 2014-10-08 Oct ## 56 Martha 2014-09-24 2014-09-24 Sep ## 57 Nancy 2014-10-08 2014-10-08 Oct ## 58 Norman 2014-09-03 2014-09-03 Sep ## 59 Richard 2014-10-08 2014-10-08 Oct ## 60 Alvin 2015-09-09 2015-09-09 Sep ## 61 Dorret 2015-08-19 2015-08-19 Aug ## 62 Flora 2015-09-30 2015-09-30 Sep ## 63 Ian 2015-10-07 2015-10-07 Oct ## 64 Marie 2015-08-12 2015-08-12 Aug ## 65 Mat 2015-09-16 2015-09-16 Sep ## 66 Nadiya 2015-10-07 2015-10-07 Oct ## 67 Paul 2015-09-23 2015-09-23 Sep ## 68 Sandy 2015-08-26 2015-08-26 Aug ## 69 Stu 2015-08-05 2015-08-05 Aug ## 70 Tamal 2015-10-07 2015-10-07 Oct ## 71 Ugnė 2015-09-02 2015-09-02 Sep ## 72 Andrew 2016-10-26 2016-10-26 Oct ## 73 Benjamina 2016-10-12 2016-10-12 Oct ## 74 Candice 2016-10-26 2016-10-26 Oct ## 75 Jane 2016-10-26 2016-10-26 Oct ## 76 Kate 2016-09-14 2016-09-14 Sep ## 77 Lee 2016-08-24 2016-08-24 Aug ## 78 Louise 2016-08-31 2016-08-31 Aug ## 79 Michael 2016-09-07 2016-09-07 Sep ## 80 Rav 2016-09-28 2016-09-28 Sep ## 81 Selasi 2016-10-19 2016-10-19 Oct ## 82 Tom 2016-10-05 2016-10-05 Oct ## 83 Val 2016-09-21 2016-09-21 Sep ## 84 Chris 2017-09-05 2017-09-05 Sep ## 85 Flo 2017-09-12 2017-09-12 Sep ## 86 James 2017-09-26 2017-09-26 Sep ## 87 Julia 2017-10-03 2017-10-03 Oct ## 88 Kate 2017-10-31 2017-10-31 Oct ## 89 Liam 2017-10-17 2017-10-17 Oct ## 90 Peter 2017-08-29 2017-08-29 Aug ## 91 Sophie 2017-10-31 2017-10-31 Oct ## 92 Stacey 2017-10-24 2017-10-24 Oct ## 93 Steven 2017-10-31 2017-10-31 Oct ## 94 Tom 2017-09-19 2017-09-19 Sep ## 95 Yan 2017-10-10 2017-10-10 Oct In the example above, I turned last_date_appeared into a character type variable first since it was a date already. But the point of this is proving the function of lubridate in converting character into date variable. Calulate Timespans The first step to calculating a timespan in lubridate is to make an interval, duration, period, then use division to convert the units to what wanted (like weeks(x) or months(x)). The x refers to the number of time units to be included in the period: bakers_time &lt;- bakers %&gt;% select(baker, first_date_appeared, last_date_appeared) bakers_time[1:5,] %&gt;% mutate(time_on_air = interval(first_date_appeared, last_date_appeared), weeks_on_air = time_on_air / weeks(1)) ## baker first_date_appeared last_date_appeared ## 1 Annetha 2010-08-17 2010-08-24 ## 2 David 2010-08-17 2010-09-07 ## 3 Edd 2010-08-17 2010-09-21 ## 4 Jasminder 2010-08-17 2010-09-14 ## 5 Jonathan 2010-08-17 2010-08-31 ## time_on_air weeks_on_air ## 1 2010-08-17 UTC--2010-08-24 UTC 1 ## 2 2010-08-17 UTC--2010-09-07 UTC 3 ## 3 2010-08-17 UTC--2010-09-21 UTC 5 ## 4 2010-08-17 UTC--2010-09-14 UTC 4 ## 5 2010-08-17 UTC--2010-08-31 UTC 2 String Wrangling Use stringr package to transform, detect, replace, and remove observations within a data frame: library(stringr) election[1:7,] %&gt;% mutate(WINNER = str_to_upper(WINNER), WINNER = str_replace(WINNER, &quot;MA&quot;, &quot;mshazam&quot;), WINNER = str_replace(WINNER, &quot;RO&quot;, &quot;Bolosho&quot;), GROUP_24 = str_detect(group, &quot;24&quot;)) %&gt;% select(COUNTY, WINNER, GROUP_24) ## # A tibble: 7 × 3 ## COUNTY WINNER GROUP_24 ## &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; ## 1 Autauga BoloshoMNEY TRUE ## 2 Baldwin BoloshoMNEY TRUE ## 3 Barbour OBAmshazam FALSE ## 4 Bibb BoloshoMNEY TRUE ## 5 Blount BoloshoMNEY FALSE ## 6 Bullock OBAmshazam FALSE ## 7 Butler BoloshoMNEY FALSE "],["categorical-data-in-the-tidyverse.html", "6 Categorical Data in the Tidyverse 6.1 Introduction to Factor Variables 6.2 Manipulating Factor Variables 6.3 Creating Factor Variables 6.4 Case Study on Flight Etiquette", " 6 Categorical Data in the Tidyverse https://learn.datacamp.com/courses/categorical-data-in-the-tidyverse Required packages to load: library(ggplot2) library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union library(tidyr) library(readr) library(forcats) library(stringr) library(scales) ## ## Attaching package: &#39;scales&#39; ## The following object is masked from &#39;package:readr&#39;: ## ## col_factor 6.1 Introduction to Factor Variables Getting Number Of Levels mutate() and summarise() in dplyr both have variants to add the suffix if or all to change the operation. mutate_if() and summarise_if apply their “second argument”-a function, to all columns where the “first argument” is true, and mutate_all() and summarise_all take one argument, a function, and apply it to all columns. # Change all the character columns to factors responses_as_factors &lt;- multiple_choice_responses %&gt;% mutate_if(is.character, as.factor) number_of_levels &lt;- responses_as_factors %&gt;% # apply the function nlevels to each column summarise_all(nlevels) %&gt;% # change the dataset from wide to long gather(variable, num_levels) number_of_levels[1:8,] ## # A tibble: 8 × 2 ## variable num_levels ## &lt;chr&gt; &lt;int&gt; ## 1 LearningPlatformUsefulnessArxiv 2 ## 2 LearningPlatformUsefulnessBlogs 2 ## 3 LearningPlatformUsefulnessCollege 2 ## 4 LearningPlatformUsefulnessCompany 1 ## 5 LearningPlatformUsefulnessConferences 2 ## 6 LearningPlatformUsefulnessFriends 1 ## 7 LearningPlatformUsefulnessKaggle 2 ## 8 LearningPlatformUsefulnessNewsletters 1 Examining Number Of Levels dplyr has two other functions to explore a dataset. The first is top_n(x, var), which give the first x rows of a dataset based on the value of var. mtcars %&gt;% top_n(2, mpg) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 ## Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 The other ispull(), which allows extraction of a column and take out the name, leaving only the value(s) from the column. pull can be used interchangeably with$as a way to extract a column as a vector from a table. For example, mean(mtcars$mpg) is the same as mean(mtcars %&gt;% pull(mpg)) or mtcars %&gt;% pull(mpg) %&gt;% mean(). mtcars %&gt;% top_n(2, mpg) %&gt;% pull(mpg) ## [1] 32.4 33.9 Reordering A Variable By Its Frequency One of the primary purposes of the forcats package is to make it easy to quickly change visualizations when working with qualitative variables. Flip the coordinates using coord_flip() to make it more readable. Use fct_infreq() to make the bar plot ordered by frequency. Use fct_rev() to reverse the order of a factor. Use fct_reorder() from forcats to take one variable and order it by another. # Make a bar plot ggplot(multiple_choice_responses, aes(x = fct_rev(fct_infreq(EmployerIndustry)))) + geom_bar() + # flip the coordinates coord_flip() 6.2 Manipulating Factor Variables How To Change The Order Of Levels Use fct_relevel() to change the order of factor levels. # Get the levels of WorkInternalVsExternalTools levels(responses_data$WorkInternalVsExternalTools) ## [1] &quot;Approximately half internal and half external&quot; ## [2] &quot;Do not know&quot; ## [3] &quot;Entirely external&quot; ## [4] &quot;Entirely internal&quot; ## [5] &quot;More external than internal&quot; ## [6] &quot;More internal than external&quot; # Reorder the levels from internal to external reordered_respsonses &lt;- responses_data[1:8,] %&gt;% mutate(WorkInternalVsExternalTools = fct_relevel(WorkInternalVsExternalTools, &quot;Entirely internal&quot;, &quot;More internal than external&quot;, &quot;Approximately half internal and half external&quot;, &quot;More external than internal&quot;, &quot;Entirely external&quot;, &quot;Do not know&quot;)) Sometimes, a data have many levels and only need to move one. Instead of writing out all the levels, use some arguments in fct_relevel() to save time. In the code below, using after = Inf will move the specified level to the last, if after = n, in which n is a number, then that specified level will the reordered to after that nth place. # Reorder the levels from internal to external somwhat_reordered &lt;- responses_data %&gt;% # Move &quot;Do not know&quot; to be the last level. mutate(WorkInternalVsExternalTools = fct_relevel(WorkInternalVsExternalTools, &quot;Do not know&quot;, after = Inf)) %&gt;% # Move &quot;Entirely internal&quot; and &quot;&quot;Approximately half internal and half external&quot; to the front mutate(WorkInternalVsExternalTools = fct_relevel(WorkInternalVsExternalTools, &quot;Entirely internal&quot;, &quot;Approximately half internal and half external&quot;)) # Get the levels of WorkInternalVsExternalTools levels(somwhat_reordered$WorkInternalVsExternalTools) ## [1] &quot;Entirely internal&quot; ## [2] &quot;Approximately half internal and half external&quot; ## [3] &quot;Entirely external&quot; ## [4] &quot;More external than internal&quot; ## [5] &quot;More internal than external&quot; ## [6] &quot;Do not know&quot; Renaming Levels Use fct_recode() to rename levels, with the new level name on the left, and the old on the right: # Reorder the levels from internal to external renamed_respsonses &lt;- responses_data[1:8,] %&gt;% mutate(WorkInternalVsExternalTools = fct_recode(WorkInternalVsExternalTools, &quot;Entirely In&quot; = &quot;Entirely internal&quot;, &quot;More In than Out&quot; = &quot;More internal than external&quot;, &quot;Hald In and Out&quot; = &quot;Approximately half internal and half external&quot;, &quot;More Out than In&quot; = &quot;More external than internal&quot;, &quot;Entirely Out&quot; = &quot;Entirely external&quot;, &quot;IDK&quot; = &quot;Do not know&quot;)) # Get the levels of WorkInternalVsExternalTools levels(renamed_respsonses$WorkInternalVsExternalTools) ## [1] &quot;Hald In and Out&quot; &quot;IDK&quot; &quot;Entirely Out&quot; &quot;Entirely In&quot; ## [5] &quot;More Out than In&quot; &quot;More In than Out&quot; How To Collapse/Merge Levels Sometimes, a dataset would have too many levels that ties to a general category, so merging those levels into one umbrella level would be easier to the eyes. Use fct_collapse() to collapse the levels into one. After collapsing the desired levels into one, there would non-essential levels, classifying those unnecessary levels into other level would be ideal. Use fct_other() to turn other levels into other(use keep = to retain the umbrella created titles) responses_data %&gt;% # Create new variable, grouped_titles, by collapsing levels in CurrentJobTitleSelect mutate(grouped_titles = fct_collapse(CurrentJobTitleSelect, &quot;Computer Scientist&quot; = &quot;Software Developer/Software Engineer&quot;, &quot;Data Analyst/Scientist/Engineer&quot; = c(&quot;DBA/Database Engineer&quot;, &quot;Data Scientist&quot;, &quot;Business Analyst&quot;))) %&gt;% # Keep all the new titles and turn every other title into &quot;Other&quot; mutate(grouped_titles = fct_other(grouped_titles, keep = c(&quot;Computer Scientist&quot;, &quot;Data Analyst/Scientist/Engineer&quot;))) %&gt;% # Get a count of the grouped titles count(grouped_titles) ## # A tibble: 4 × 2 ## grouped_titles n ## &lt;fct&gt; &lt;int&gt; ## 1 Data Analyst/Scientist/Engineer 5 ## 2 Computer Scientist 5 ## 3 Other 2 ## 4 &lt;NA&gt; 8 Showing Levels By Proportion Use fct_lump() to show only the selected levels that exceed the specified proportion(percentage of that level in the dataset), otherwise, collapse all the non-qualified levels into other). responses_data %&gt;% # remove NAs of MLMethodNextYearSelect filter(!is.na(MLMethodNextYearSelect)) %&gt;% # create ml_method, which lumps all those with less than 5% of people into &quot;Other&quot; mutate(ml_method = fct_lump(MLMethodNextYearSelect, prop = .11)) %&gt;% # count the frequency of your new variable, sorted in descending order count(ml_method, sort = TRUE) ## # A tibble: 5 × 2 ## ml_method n ## &lt;fct&gt; &lt;int&gt; ## 1 Deep learning 6 ## 2 Other 4 ## 3 Neural Nets 2 ## 4 Random Forests 2 ## 5 Text Mining 2 Showing The “N” Number Most Common Levels Instead of showing levels by proportion but to show the most common n levels, use: fct_lump(dataset_name, n = __, other_level = &quot;title_of_non-qualified_levels&quot;) For example: responses_data %&gt;% # remove NAs filter(!is.na(MLMethodNextYearSelect)) %&gt;% # create ml_method, retaining the first most common method and renaming others &quot;other method&quot; mutate(ml_method = fct_lump(MLMethodNextYearSelect, n = 1, other_level = &quot;other method&quot;)) %&gt;% # count the frequency of your new variable, sorted in descending order count(ml_method, sort = TRUE) ## # A tibble: 2 × 2 ## ml_method n ## &lt;fct&gt; &lt;int&gt; ## 1 other method 10 ## 2 Deep learning 6 6.3 Creating Factor Variables Use str_remove() from the stringr package to remove a certain phrase from observations in a column: platform_usefulness_long &lt;- responses_data %&gt;% # select columns with LearningPlatformUsefulness in title select(contains(&quot;LearningPlatformUsefulness&quot;)) %&gt;% # change data from wide to long gather(learning_platform, usefulness) %&gt;% # remove rows where usefulness is NA filter(!is.na(usefulness)) platform_usefulness_long[1:7,] ## # A tibble: 7 × 2 ## learning_platform usefulness ## &lt;chr&gt; &lt;chr&gt; ## 1 LearningPlatformUsefulnessArxiv Very useful ## 2 LearningPlatformUsefulnessArxiv Very useful ## 3 LearningPlatformUsefulnessArxiv Somewhat useful ## 4 LearningPlatformUsefulnessArxiv Somewhat useful ## 5 LearningPlatformUsefulnessBlogs Very useful ## 6 LearningPlatformUsefulnessBlogs Somewhat useful ## 7 LearningPlatformUsefulnessBlogs Very useful # remove &quot;LearningPlatformUsefulness&quot; from each string in learning_platform platform_usefulness_short &lt;- platform_usefulness_long %&gt;% mutate(learning_platform = str_remove(learning_platform, &quot;LearningPlatformUsefulness&quot;)) platform_usefulness_short[1:7,] ## # A tibble: 7 × 2 ## learning_platform usefulness ## &lt;chr&gt; &lt;chr&gt; ## 1 Arxiv Very useful ## 2 Arxiv Very useful ## 3 Arxiv Somewhat useful ## 4 Arxiv Somewhat useful ## 5 Blogs Very useful ## 6 Blogs Somewhat useful ## 7 Blogs Very useful How To Add A Counted Column With “add+count()” add_count() adds a column to the dataset, n, keeping the same number of rows as the original dataset. Just like count(), n defaults to be the number of rows for each group. Set wt equal to another column to make n equal to the sum of that column for each group: perc_useful_platform &lt;- platform_usefulness_short %&gt;% # change dataset to one row per learning_platform usefulness pair with number of entries for each count(learning_platform, usefulness) %&gt;% # use add_count to create column with total number of answers for that learning_platform add_count(learning_platform, wt = n, name=&#39;nn&#39;) %&gt;% # create a new column, perc, that is the percentage of people giving that response for that learning_platform mutate(perc = n / nn) perc_useful_platform[1:5,] ## # A tibble: 5 × 5 ## learning_platform usefulness n nn perc ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Arxiv Somewhat useful 2 4 0.5 ## 2 Arxiv Very useful 2 4 0.5 ## 3 Blogs Somewhat useful 2 5 0.4 ## 4 Blogs Very useful 3 5 0.6 ## 5 College Somewhat useful 2 5 0.4 Changing And Creating Variables With “case_when()” Make a new categorical variable/column base on another column using case_when(): mtcars[1:7,] %&gt;% # Create the speed column/variable based on hp (horse power) mutate(speed = case_when( hp &lt; 99 ~ &quot;Too Slow&quot;, between(hp, 100, 199) ~ &quot;Fast&quot;, between(hp, 200, 299) ~ &quot;Very Fast&quot;, hp &gt; 300 ~ &quot;Super Sonic&quot; )) %&gt;% select(hp, speed) ## hp speed ## Mazda RX4 110 Fast ## Mazda RX4 Wag 110 Fast ## Datsun 710 93 Too Slow ## Hornet 4 Drive 110 Fast ## Hornet Sportabout 175 Fast ## Valiant 105 Fast ## Duster 360 245 Very Fast 6.4 Case Study on Flight Etiquette Utilize the tools above to practice on an actual dataset: flying_etiquette &lt;- read_csv(&quot;https://assets.datacamp.com/production/repositories/1834/datasets/bef2c6e1ef42a2f230383e080fa7379912860017/flying-etiquette.csv&quot;) ## Rows: 1040 Columns: 27 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (26): How often do you travel by plane?, Do you ever recline your seat w... ## dbl (1): RespondentID ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. Changing Characters To Factors There’re backticks around the column name because there are spaces in the column names! Normally, underscores are used. Use mutate_if() to change character type columns into factor type: flying_etiquette %&gt;% # Change characters to factors mutate_if(is.character, as.factor) %&gt;% # Filter out those who have never flown on a plane filter(`How often do you travel by plane?` != &quot;Never&quot;) Reformat/Filter Selected Data For Graphing Reformat the data to get it in shape for plotting. The graph only looks at flier’s opinions about whether different behaviors are rude, so select only those columns. gathered_data &lt;- flying_etiquette %&gt;% mutate_if(is.character, as.factor) %&gt;% filter(`How often do you travel by plane?` != &quot;Never&quot;) %&gt;% # Select columns containing &quot;rude&quot; select(contains(&quot;rude&quot;)) %&gt;% # Change format from wide to long gather(response_var, value) ## Warning: attributes are not identical across measure variables; ## they will be dropped Cleaning Up Strings Use regex or regular expressions to capture general patterns. For example, .*science would match data science in the string data science rocks!. These are two basic notations: ## Expression Does_This ## 1 . matches any character ## 2 * zero or more times gathered_data[1:4,] %&gt;% select(response_var) ## # A tibble: 4 × 1 ## response_var ## &lt;chr&gt; ## 1 Is itrude to move to an unsold seat on a plane? ## 2 Is itrude to move to an unsold seat on a plane? ## 3 Is itrude to move to an unsold seat on a plane? ## 4 Is itrude to move to an unsold seat on a plane? gathered_data[1:4,] %&gt;% # Remove everything before and including &quot;rude to &quot; (with that space at the end!) mutate(response_var = str_remove(response_var, &quot;.*rude to &quot;)) %&gt;% # Remove &quot;on a plane&quot; mutate(response_var = str_remove(response_var, &quot;on a plane&quot;)) ## # A tibble: 4 × 2 ## response_var value ## &lt;chr&gt; &lt;chr&gt; ## 1 move to an unsold seat ? &lt;NA&gt; ## 2 move to an unsold seat ? No, not rude at all ## 3 move to an unsold seat ? No, not rude at all ## 4 move to an unsold seat ? No, not rude at all Dichotomizing Variables/Simplifying Observations In the value column, there are a variety of observations that indicates a range from not rude, somwhat, to very rude. To make it easier to plot, to simplify/categorize the observations, split it into 2 groups, rude and not rude. gathered_data[1:10,] %&gt;% pull(value) ## [1] NA &quot;No, not rude at all&quot; &quot;No, not rude at all&quot; ## [4] &quot;No, not rude at all&quot; &quot;No, not rude at all&quot; &quot;Yes, somewhat rude&quot; ## [7] &quot;No, not rude at all&quot; &quot;No, not rude at all&quot; &quot;No, not rude at all&quot; ## [10] &quot;Yes, very rude&quot; Right now, our data is still in the format of one row per person per question. But for us to graph it, we’ll want to change that so each row is a question with the summary information about the response to that question. rude_behaviors &lt;- gathered_data %&gt;% mutate(response_var = str_replace(response_var, &#39;.*rude to &#39;, &#39;&#39;)) %&gt;% mutate(response_var = str_replace(response_var, &#39;on a plane&#39;, &#39;&#39;)) %&gt;% # Remove rows that are NA in the value column filter(!is.na(value)) %&gt;% mutate(rude = if_else(value %in% c(&quot;No, not rude at all&quot;, &quot;No, not at all rude&quot;), 0, 1)) %&gt;% # Group by response_var group_by(response_var) %&gt;% # Create perc_rude, the percent considering each behavior rude summarise(perc_rude = mean(rude)) rude_behaviors ## # A tibble: 9 × 2 ## response_var perc_rude ## &lt;chr&gt; &lt;dbl&gt; ## 1 ask someone to switch seats with you in order to be closer to famil… 0.171 ## 2 ask someone to switch seats with you in order to be closer to frien… 0.258 ## 3 bring a baby ? 0.303 ## 4 knowingly bring unruly children ? 0.827 ## 5 move to an unsold seat ? 0.193 ## 6 recline your seat ? 0.412 ## 7 say more than a few words tothe stranger sitting next to you ? 0.211 ## 8 wake a passenger up if you are trying to go to the bathroom? 0.371 ## 9 wake a passenger up if you are trying to walk around? 0.734 initial Plot The initial plot will have a few problems that are aesthetically unpleasing. For example, there are no labels, x-axis titles are jumbled up. We’ll fix it step by step: initial_plot &lt;- rude_behaviors %&gt;% # reorder response_var by perc_rude mutate(response_var = fct_reorder(response_var, perc_rude)) %&gt;% # make a bar plot of perc_rude by response_var ggplot(aes(x = response_var, y = perc_rude)) + geom_col() # View your plot initial_plot Fixing Labels Our initial plot displays the data, but there’s a lot of work to do. Let’s start with the text above and around the plot. We’ve seen how the ggplot2 layer labs() can take multiple arguments to modify the text: titled_plot &lt;- initial_plot + # Add the title, subtitle, and caption labs(title = &quot;Hell Is Other People In A Pressurized Metal Tube&quot;, subtitle = &quot;Percentage of 874 air-passenger respondents who said action is very or somewhat rude&quot;, caption = &quot;Source: SurveyMonkey Audience&quot;, # Remove the x- and y-axis labels x = &quot;&quot;, y = &quot;&quot;) titled_plot Flipping X And Y For Better Visualization When the labels for our x-axis run into each other, we have two main choices: change the labels to be vertical instead of horizontal or flip the axes. flipped_plot &lt;- titled_plot + # Flip the axes coord_flip() + # Remove the x-axis ticks and labels theme(axis.text.x = element_blank(), axis.ticks.x = element_blank()) flipped_plot Finalizing Steps The FiveThirtyEight chart displayed the bar’s height above each bar. We can do this with a geom_text() layer. Positioning the text just right above the bar takes a few extra argument. position = position_dodge(0.9), vjust = 1 part of the code adjusts the percentage output position on the bar so it looks better. flipped_plot + # Apply percent() to perc_rude to label above the bar with the perc value geom_text(aes(label = percent(perc_rude), y = perc_rude + .03), position = position_dodge(0.9), vjust = 1) "],["data-manipulation-with-dplyr.html", "7 Data Manipulation with dplyr 7.1 Transforming Data with dplyr 7.2 Aggregating Data 7.3 Selecting and Transforming Data 7.4 Case Study: The babynames Dataset", " 7 Data Manipulation with dplyr https://learn.datacamp.com/courses/data-manipulation-with-dplyr 7.1 Transforming Data with dplyr select(), mutate(), filter(), and arrange() For more details, go to the Data Wrangling section of Intro to the Tidyverse. 7.2 Aggregating Data count() A simple way to aggregate data is to count to find out the number of observations. Use count(): mtcars %&gt;% count(cyl, wt = hp, sort = TRUE) ## cyl n ## 1 8 2929 ## 2 4 909 ## 3 6 856 Here, count() sorted out the distinct observations from the cyl variable, the wt (weight) would count the total number of hp (horse power) for each distinct cyl instead of counting the total number of of observations that belongs to each distinct vyl. The wt argument basically plug in another column to count instea. group_by() And summarize() For more details, go to the Grouping and summarizing section of Intro to the Tidyverse. top_n() This function select the most extreme observations. The number 1 here select the highest hp in each distinct cyl observations: mtcars %&gt;% select(cyl, hp) %&gt;% mutate(cyl = as.factor(cyl)) %&gt;% group_by(cyl) %&gt;% top_n(1, hp) ## # A tibble: 3 × 2 ## # Groups: cyl [3] ## cyl hp ## &lt;fct&gt; &lt;dbl&gt; ## 1 4 113 ## 2 6 175 ## 3 8 335 For more details, go to the Introduction to Factor Variables section of Categorical Data in the Tidyverse. 7.3 Selecting and Transforming Data select() And rename()* For more details, go to the Tame your data section of Working with Data in the Tidyverse. Transmute() The function transmute() is like a combination of select and mutate. It only outputs the selected variables, and allow changing, creating new variables inside the same bracket: mtcars[1:5,] %&gt;% transmute(weight_lbs = wt, weight_kg = weight_lbs / 2.205) ## weight_lbs weight_kg ## Mazda RX4 2.620 1.188209 ## Mazda RX4 Wag 2.875 1.303855 ## Datsun 710 2.320 1.052154 ## Hornet 4 Drive 3.215 1.458050 ## Hornet Sportabout 3.440 1.560091 7.4 Case Study: The babynames Dataset babynames &lt;- readRDS(gzcon(url(&quot;https://assets.datacamp.com/production/repositories/4984/datasets/a924ac5d86adba2e934d489cb9db446236f62b2c/babynames.rds&quot;))) babynames ## # A tibble: 332,595 × 3 ## year name number ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; ## 1 1880 Aaron 102 ## 2 1880 Ab 5 ## 3 1880 Abbie 71 ## 4 1880 Abbott 5 ## 5 1880 Abby 6 ## 6 1880 Abe 50 ## 7 1880 Abel 9 ## 8 1880 Abigail 12 ## 9 1880 Abner 27 ## 10 1880 Abraham 81 ## # … with 332,585 more rows Filtering and arranging for one year Filter for only the year 1990 and sort the table in descending order of the number of babies born: babynames %&gt;% # Filter for the year 1990 filter(year == 1990) %&gt;% # Sort the number column in descending order arrange(desc(number)) ## # A tibble: 21,223 × 3 ## year name number ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; ## 1 1990 Michael 65560 ## 2 1990 Christopher 52520 ## 3 1990 Jessica 46615 ## 4 1990 Ashley 45797 ## 5 1990 Matthew 44925 ## 6 1990 Joshua 43382 ## 7 1990 Brittany 36650 ## 8 1990 Amanda 34504 ## 9 1990 Daniel 33963 ## 10 1990 David 33862 ## # … with 21,213 more rows Using top_n with babynames filter() and arrange() were used to find the most common names in one year. However, group_by() and top_n() can be used to find the most common name in every year: babynames %&gt;% group_by(year) %&gt;% top_n(1, number) ## # A tibble: 28 × 3 ## # Groups: year [28] ## year name number ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; ## 1 1880 John 9701 ## 2 1885 Mary 9166 ## 3 1890 Mary 12113 ## 4 1895 Mary 13493 ## 5 1900 Mary 16781 ## 6 1905 Mary 16135 ## 7 1910 Mary 22947 ## 8 1915 Mary 58346 ## 9 1920 Mary 71175 ## 10 1925 Mary 70857 ## # … with 18 more rows Visualizing names with ggplot2 Filter for only the names Steven, Thomas, and Matthew, and assign it to an object called selected_names. The %in% operator can be used within filter() by including c() and a vector of values. selected_names &lt;- babynames %&gt;% filter(name %in% c(&quot;Steven&quot;, &quot;Thomas&quot;, &quot;Matthew&quot;)) Visualize the three names as a line plot over time, with each name represented by a different color: # Filter for the names Steven, Thomas, and Matthew selected_names &lt;- babynames %&gt;% filter(name %in% c(&quot;Steven&quot;, &quot;Thomas&quot;, &quot;Matthew&quot;)) # Plot the names using a different color for each name ggplot(selected_names, aes(x = year, y = number, color = name)) + geom_line() Finding the year each name is most common Calculate the total number of people born in that year as year_total. Next, use year_total to calculate the fraction of people born in each year that have each name. Then, use the fraction column, in combination with top_n(), to identify the year each name is most common: babynames_fraction &lt;- babynames %&gt;% group_by(year) %&gt;% mutate(year_total = sum(number)) %&gt;% ungroup() %&gt;% mutate(fraction = number / year_total) # Find the year each name is most common babynames_fraction %&gt;% group_by(name) %&gt;% top_n(1, fraction) ## # A tibble: 48,040 × 5 ## # Groups: name [48,040] ## year name number year_total fraction ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1880 Abbott 5 201478 0.0000248 ## 2 1880 Abe 50 201478 0.000248 ## 3 1880 Abner 27 201478 0.000134 ## 4 1880 Adelbert 28 201478 0.000139 ## 5 1880 Adella 26 201478 0.000129 ## 6 1880 Adolf 6 201478 0.0000298 ## 7 1880 Adolph 93 201478 0.000462 ## 8 1880 Agustus 5 201478 0.0000248 ## 9 1880 Albert 1493 201478 0.00741 ## 10 1880 Albertina 7 201478 0.0000347 ## # … with 48,030 more rows lag() Function The lag() function can be used to find the differences between two vectors by shifting the vectors to the right: v &lt;- c(1, 3, 6, 14) v ## [1] 1 3 6 14 lag(v) ## [1] NA 1 3 6 Changes in popularity of a name To find the changes in popularity of the name \"Matthew\" over the years, we will minus the fraction by the lag of the fraction to calculate the difference between each year. The difference represents the up or down of the fraction, which means the popularity increasing or decreasing: babynames_fraction %&gt;% filter(name == &quot;Matthew&quot;) %&gt;% arrange(year) %&gt;% mutate(difference = fraction - lag(fraction)) %&gt;% arrange(desc(difference)) ## # A tibble: 28 × 6 ## year name number year_total fraction difference ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1975 Matthew 28665 3014943 0.00951 0.00389 ## 2 1970 Matthew 20265 3604252 0.00562 0.00286 ## 3 1985 Matthew 47367 3563364 0.0133 0.00223 ## 4 1980 Matthew 38054 3439117 0.0111 0.00156 ## 5 1965 Matthew 10015 3624610 0.00276 0.00109 ## 6 1960 Matthew 6942 4152075 0.00167 0.000853 ## 7 1955 Matthew 3287 4012691 0.000819 0.000447 ## 8 1915 Matthew 798 1830351 0.000436 0.000102 ## 9 1950 Matthew 1303 3502592 0.000372 0.0000967 ## 10 1910 Matthew 197 590607 0.000334 0.0000811 ## # … with 18 more rows "],["joining-data-with-dplyr.html", "8 Joining Data with dplyr 8.1 Joining Tables 8.2 Left and Right Joins 8.3 Full, Semi, and Anti Joins 8.4 Case Study: Joins on Stack Overflow Data", " 8 Joining Data with dplyr https://learn.datacamp.com/courses/joining-data-with-dplyr 8.1 Joining Tables inner_join() The inner_join() is the key to bring tables together.This function will allow two tables to be joined from the commonly-shared value column(s) on which they should be joined. In this example, parts and part_categories are merged by using part_cat_id and id columns from each dataset, then the suffix() added the names to help differentiate the variables: glimpse(parts) ## Rows: 17,501 ## Columns: 3 ## $ part_num &lt;chr&gt; &quot;0901&quot;, &quot;0902&quot;, &quot;0903&quot;, &quot;0904&quot;, &quot;1&quot;, &quot;10016414&quot;, &quot;10026stk… ## $ name &lt;chr&gt; &quot;Baseplate 16 x 30 with Set 080 Yellow House Print&quot;, &quot;Base… ## $ part_cat_id &lt;dbl&gt; 1, 1, 1, 1, 7, 58, 58, 44, 65, 27, 27, 27, 65, 65, 27, 27,… glimpse(part_categories) ## Rows: 64 ## Columns: 2 ## $ id &lt;dbl&gt; 1, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 2… ## $ name &lt;chr&gt; &quot;Baseplates&quot;, &quot;Bricks Sloped&quot;, &quot;Duplo, Quatro and Primo&quot;, &quot;Bricks… parts %&gt;% inner_join(part_categories, by = c(&quot;part_cat_id&quot; = &quot;id&quot;), suffix = c(&quot;_part&quot;, &quot;_category&quot;)) ## # A tibble: 17,501 × 4 ## part_num name_part part_cat_id name_category ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0901 Baseplate 16 x 30 with Set 080 Yello… 1 Baseplates ## 2 0902 Baseplate 16 x 24 with Set 080 Small… 1 Baseplates ## 3 0903 Baseplate 16 x 24 with Set 080 Red H… 1 Baseplates ## 4 0904 Baseplate 16 x 24 with Set 080 Large… 1 Baseplates ## 5 1 Homemaker Bookcase 2 x 4 x 4 7 Containers ## 6 10016414 Sticker Sheet #1 for 41055-1 58 Stickers ## 7 10026stk01 Sticker for Set 10026 - (44942/41841… 58 Stickers ## 8 10039 Pullback Motor 8 x 4 x 2/3 44 Mechanical ## 9 10048 Minifig Hair Tousled 65 Minifig Headwear ## 10 10049 Minifig Shield Broad with Spiked Bot… 27 Minifig Accesso… ## # … with 17,491 more rows Another situation is when wanting to merge two datasets that share the same column(s), using c() won’t be necessary. Here, the by = argument only needs the shared column name: parts %&gt;% inner_join(inventory_parts, by = &quot;part_num&quot;) ## # A tibble: 258,958 × 6 ## part_num name part_cat_id inventory_id color_id quantity ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0901 Baseplate 16 x 30 with S… 1 1973 2 1 ## 2 0902 Baseplate 16 x 24 with S… 1 1973 2 1 ## 3 0903 Baseplate 16 x 24 with S… 1 1973 2 1 ## 4 0904 Baseplate 16 x 24 with S… 1 1973 2 1 ## 5 1 Homemaker Bookcase 2 x 4… 7 508 15 1 ## 6 1 Homemaker Bookcase 2 x 4… 7 1158 15 2 ## 7 1 Homemaker Bookcase 2 x 4… 7 6590 15 2 ## 8 1 Homemaker Bookcase 2 x 4… 7 9679 15 2 ## 9 1 Homemaker Bookcase 2 x 4… 7 12256 1 2 ## 10 1 Homemaker Bookcase 2 x 4… 7 13356 15 1 ## # … with 258,948 more rows Joining Multiple Datasets Use inner_join() and the pipe (%&gt;%) to join together multiple datasets. In this example, sets, a table that tells us about each LEGO kit, will be connected with inventories, a table that tells us the specific version of a given set, and finally to inventory_parts, a table which tells us how many of each part is available in each LEGO kit: sets %&gt;% # Add inventories using an inner join inner_join(inventories, by = &quot;set_num&quot;) %&gt;% # Add inventory_parts using an inner join inner_join(inventory_parts, by = c(&quot;id&quot; = &quot;inventory_id&quot;)) ## # A tibble: 258,958 × 9 ## set_num name year theme_id id version part_num color_id quantity ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 700.3-1 Medium Gift … 1949 365 24197 1 bdoor01 2 2 ## 2 700.3-1 Medium Gift … 1949 365 24197 1 bdoor01 15 1 ## 3 700.3-1 Medium Gift … 1949 365 24197 1 bdoor01 4 1 ## 4 700.3-1 Medium Gift … 1949 365 24197 1 bslot02 15 6 ## 5 700.3-1 Medium Gift … 1949 365 24197 1 bslot02 2 6 ## 6 700.3-1 Medium Gift … 1949 365 24197 1 bslot02 4 6 ## 7 700.3-1 Medium Gift … 1949 365 24197 1 bslot02 1 6 ## 8 700.3-1 Medium Gift … 1949 365 24197 1 bslot02 14 6 ## 9 700.3-1 Medium Gift … 1949 365 24197 1 bslot02a 15 6 ## 10 700.3-1 Medium Gift … 1949 365 24197 1 bslot02a 2 6 ## # … with 258,948 more rows To utilize this, join an additional table, colors, which tell the color of each part in each set, to answer the question, “what is the most common color of a LEGO piece?” # Count the number of colors and sort sets %&gt;% inner_join(inventories, by = &quot;set_num&quot;) %&gt;% inner_join(inventory_parts, by = c(&quot;id&quot; = &quot;inventory_id&quot;)) %&gt;% inner_join(colors, by = c(&quot;color_id&quot; = &quot;id&quot;), suffix = c(&quot;_set&quot;, &quot;_color&quot;)) %&gt;% count(name_color, sort = TRUE) ## # A tibble: 134 × 2 ## name_color n ## &lt;chr&gt; &lt;int&gt; ## 1 Black 48068 ## 2 White 30105 ## 3 Light Bluish Gray 26024 ## 4 Red 21602 ## 5 Dark Bluish Gray 19948 ## 6 Yellow 17088 ## 7 Blue 12980 ## 8 Light Gray 8632 ## 9 Reddish Brown 6960 ## 10 Tan 6664 ## # … with 124 more rows From the output, Black seems to be the most common LEGO piece. inner_join() ONLY keeps the perfectly matched observations between two connected datasets. 8.2 Left and Right Joins left_join() left_join() keeps ALL the observations from the left dataset and keep ONLY the matched observations from the right-side dataset. Combine the star_destroyer and millennium_falcon tables with the suffixes _falcon and _star_destroyer: inventory_parts_joined &lt;- inventories %&gt;% inner_join(inventory_parts, by = c(&quot;id&quot; = &quot;inventory_id&quot;)) %&gt;% select(-id, -version) %&gt;% arrange(desc(quantity)) millennium_falcon &lt;- inventory_parts_joined %&gt;% filter(set_num == &quot;7965-1&quot;) star_destroyer &lt;- inventory_parts_joined %&gt;% filter(set_num == &quot;75190-1&quot;) millennium_falcon %&gt;% left_join(star_destroyer, by = c(&quot;part_num&quot;, &quot;color_id&quot;), suffix = c(&quot;_falcon&quot;, &quot;_star_destroyer&quot;)) ## # A tibble: 263 × 6 ## set_num_falcon part_num color_id quantity_falcon set_num_star_destroyer ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 7965-1 63868 71 62 &lt;NA&gt; ## 2 7965-1 3023 0 60 &lt;NA&gt; ## 3 7965-1 3021 72 46 75190-1 ## 4 7965-1 2780 0 37 75190-1 ## 5 7965-1 60478 72 36 &lt;NA&gt; ## 6 7965-1 6636 71 34 75190-1 ## 7 7965-1 3009 71 28 75190-1 ## 8 7965-1 3665 71 22 &lt;NA&gt; ## 9 7965-1 2412b 72 20 75190-1 ## 10 7965-1 3010 71 19 &lt;NA&gt; ## # … with 253 more rows, and 1 more variable: quantity_star_destroyer &lt;dbl&gt; From the output, the quantity_falcon kept all the observations, and the quantity_star_destroyer only kept the matched ones, the NAs are the unmatched ones. right_join() right_join() keeps ALL the observations from the right dataset and keep ONLY the matched observations from the left-side dataset. In this example, count the part_cat_id from parts, before using a right_join to join with part_categories. This is to know the count of part_cat_id in parts, and also to see if there are any part_cat_id observations not available in parts by using filter(): parts %&gt;% count(part_cat_id) %&gt;% right_join(part_categories, by = c(&quot;part_cat_id&quot; = &quot;id&quot;)) %&gt;% # Filter for NA filter(is.na(n)) ## # A tibble: 1 × 3 ## part_cat_id n name ## &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; ## 1 66 NA Modulex Here, the left-side dataset is the n column, which contains a NA observation, and the name column is the right-sde dataset which retains all the observations. Joining tables to themselves Tables can be joined to themselves! In the themes dataset, there are two columns id and parent_id, which has a connection to each other. On the parent_id side, Arctic Technic, Competition, Expert Builder, Model are assigned as 1 on parent_id column, which are sub-branches of the Technic, which has id of 1 on the left-side. Then, the Airport, Construction, Farm, Fire, Harbor are assigned as 5 on parent_id column, which are the sub-branches of Model, which has id of 5 of the left-side. This table has a hierarchical table. themes ## # A tibble: 665 × 3 ## id name parent_id ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Technic NA ## 2 2 Arctic Technic 1 ## 3 3 Competition 1 ## 4 4 Expert Builder 1 ## 5 5 Model 1 ## 6 6 Airport 5 ## 7 7 Construction 5 ## 8 8 Farm 5 ## 9 9 Fire 5 ## 10 10 Harbor 5 ## # … with 655 more rows themes %&gt;% # Inner join the themes table inner_join(themes, by = c(&quot;id&quot; = &quot;parent_id&quot;), suffix = c(&quot;_parent&quot;, &quot;_child&quot;)) %&gt;% # Filter for the &quot;Harry Potter&quot; parent name filter(name_parent == &quot;Harry Potter&quot;) ## # A tibble: 6 × 5 ## id name_parent parent_id id_child name_child ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 246 Harry Potter NA 247 Chamber of Secrets ## 2 246 Harry Potter NA 248 Goblet of Fire ## 3 246 Harry Potter NA 249 Order of the Phoenix ## 4 246 Harry Potter NA 250 Prisoner of Azkaban ## 5 246 Harry Potter NA 251 Sorcerer&#39;s Stone ## 6 246 Harry Potter NA 667 Fantastic Beasts The code chunk above outputs all the sub-branches that belong to the Harry Potter. Joining themes to their grandchildren inner_join() can merge themes to a filtered version of itself again to establish a connection between our last join’s children and their children. Use another inner join to combine themes again with itself: # Join themes to itself again to find the grandchild relationships themes %&gt;% inner_join(themes, by = c(&quot;id&quot; = &quot;parent_id&quot;), suffix = c(&quot;_parent&quot;, &quot;_child&quot;)) %&gt;% inner_join(themes, by = c(&quot;id_child&quot; = &quot;parent_id&quot;), suffix = c(&quot;_parent&quot;, &quot;_grandchild&quot;)) ## # A tibble: 158 × 7 ## id_parent name_parent parent_id id_child name_child id_grandchild name ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 Technic NA 5 Model 6 Airport ## 2 1 Technic NA 5 Model 7 Constructi… ## 3 1 Technic NA 5 Model 8 Farm ## 4 1 Technic NA 5 Model 9 Fire ## 5 1 Technic NA 5 Model 10 Harbor ## 6 1 Technic NA 5 Model 11 Off-Road ## 7 1 Technic NA 5 Model 12 Race ## 8 1 Technic NA 5 Model 13 Riding Cyc… ## 9 1 Technic NA 5 Model 14 Robot ## 10 1 Technic NA 5 Model 15 Traffic ## # … with 148 more rows Left-joining a table to itself Some themes might not have any sub-branches at all, which means they won’t be included in the inner_join(). Identify those with a left_join() and a filter(): themes %&gt;% # Left join the themes table to its own children left_join(themes, by = c(&quot;id&quot; = &quot;parent_id&quot;), suffix = c(&quot;_parent&quot;, &quot;_child&quot;)) %&gt;% # Filter for themes that have no child themes filter(is.na(name_child)) ## # A tibble: 586 × 5 ## id name_parent parent_id id_child name_child ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 2 Arctic Technic 1 NA &lt;NA&gt; ## 2 3 Competition 1 NA &lt;NA&gt; ## 3 4 Expert Builder 1 NA &lt;NA&gt; ## 4 6 Airport 5 NA &lt;NA&gt; ## 5 7 Construction 5 NA &lt;NA&gt; ## 6 8 Farm 5 NA &lt;NA&gt; ## 7 9 Fire 5 NA &lt;NA&gt; ## 8 10 Harbor 5 NA &lt;NA&gt; ## 9 11 Off-Road 5 NA &lt;NA&gt; ## 10 12 Race 5 NA &lt;NA&gt; ## # … with 576 more rows The output above shows all the theme groups that doesn’t have any sub-branches. 8.3 Full, Semi, and Anti Joins full_join() left_join and right_join keeps all the observations in one of the two sides. full_join() keeps all the observations on BOTH sides: inventory_sets_themes &lt;- inventory_parts_joined %&gt;% inner_join(sets, by = &quot;set_num&quot;) %&gt;% inner_join(themes, by = c(&quot;theme_id&quot; = &quot;id&quot;), suffix = c(&quot;_set&quot;, &quot;_theme&quot;)) batman &lt;- inventory_sets_themes %&gt;% filter(name_theme == &quot;Batman&quot;) batman_parts &lt;- batman %&gt;% count(part_num, color_id, wt = quantity) star_wars &lt;- inventory_sets_themes %&gt;% filter(name_theme == &quot;Star Wars&quot;) star_wars_parts &lt;- star_wars %&gt;% count(part_num, color_id, wt = quantity) The variables above set up the data to prepare for the full_join to find out the difference pieces used in batman and star_wars: batman_parts %&gt;% # Combine the star_wars_parts table full_join(star_wars_parts, by = c(&quot;part_num&quot;, &quot;color_id&quot;), suffix = c(&quot;_batman&quot;, &quot;_star_wars&quot;)) %&gt;% # Replace NAs with 0s in the n_batman and n_star_wars columns replace_na(list(n_batman = 0, n_star_wars = 0)) ## # A tibble: 3,628 × 4 ## part_num color_id n_batman n_star_wars ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 10113 0 11 0 ## 2 10113 272 1 0 ## 3 10113 320 1 0 ## 4 10183 57 1 0 ## 5 10190 0 2 0 ## 6 10201 0 1 21 ## 7 10201 4 3 0 ## 8 10201 14 1 0 ## 9 10201 15 6 0 ## 10 10201 71 4 5 ## # … with 3,618 more rows semi_join() semi_join() filter and output the observations that of the left-side dataset that matches with the right-side. (ONLY show the left-side observations that arematched, not showing both datasets). Let’s focus on the batwing dataset, determine which parts are in both the batwing and batmobile sets. batmobile &lt;- inventory_parts_joined %&gt;% filter(set_num == &quot;7784-1&quot;) %&gt;% select(-set_num) batwing &lt;- inventory_parts_joined %&gt;% filter(set_num == &quot;70916-1&quot;) %&gt;% select(-set_num) # Filter the batwing set for parts that are also in the batmobile set batwing %&gt;% semi_join(batmobile, by = &quot;part_num&quot;) ## # A tibble: 126 × 3 ## part_num color_id quantity ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3023 0 22 ## 2 3024 0 22 ## 3 3623 0 20 ## 4 2780 0 17 ## 5 3666 0 16 ## 6 3710 0 14 ## 7 6141 4 12 ## 8 2412b 71 10 ## 9 6141 72 10 ## 10 6558 1 9 ## # … with 116 more rows anti_join() The opposite of semi_join is anti_join(). It shows the observations that are in the left-side dataset but is not in the right-side dataset. In other words, what observations are in the first table but not in the second table? Let’s focus on the batwing dataset, determine which sets are in one, but not the other. # Filter the batwing set for parts that aren&#39;t in the batmobile set batwing %&gt;% anti_join(batmobile, by = &quot;part_num&quot;) ## # A tibble: 183 × 3 ## part_num color_id quantity ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 11477 0 18 ## 2 99207 71 18 ## 3 22385 0 14 ## 4 99563 0 13 ## 5 10247 72 12 ## 6 2877 72 12 ## 7 61409 72 12 ## 8 11153 0 10 ## 9 98138 46 10 ## 10 2419 72 9 ## # … with 173 more rows Aggregating sets to look at their differences To compare two individual sets, and the kinds of LEGO pieces that comprise them, aggregate the data into separate themes. Additionally, add a column to understand the percentages of specific pieces that are part of each set, rather than looking at the numbers of pieces alone. inventory_parts_themes &lt;- inventories %&gt;% inner_join(inventory_parts, by = c(&quot;id&quot; = &quot;inventory_id&quot;)) %&gt;% arrange(desc(quantity)) %&gt;% select(-id, -version) %&gt;% inner_join(sets, by = &quot;set_num&quot;) %&gt;% inner_join(themes, by = c(&quot;theme_id&quot; = &quot;id&quot;), suffix = c(&quot;_set&quot;, &quot;_theme&quot;)) batman_colors &lt;- inventory_parts_themes %&gt;% filter(name_theme == &quot;Batman&quot;) %&gt;% group_by(color_id) %&gt;% summarize(total = sum(quantity)) %&gt;% mutate(percent = total / sum(total)) star_wars_colors &lt;- inventory_parts_themes %&gt;% filter(name_theme == &quot;Star Wars&quot;) %&gt;% group_by(color_id) %&gt;% summarize(total = sum(quantity)) %&gt;% mutate(percent = total / sum(total)) Combining sets Prior to visualizing the data, combine these tables to be able to directly compare the themes’ colors. Join the batman_colors and star_wars_colors tables; be sure to include all observations from both tables. Replace the NAs in the total_batman and total_star_wars columns. Add a difference column that calculates the difference between percent_batman and percent_star_wars, and a total column, which is the sum of total_batman and total_star_wars. Add a filter to select observations where total is at least 200. colors_joined &lt;- batman_colors %&gt;% # Join the Batman and Star Wars colors full_join(star_wars_colors, by = &quot;color_id&quot;, suffix = c(&quot;_batman&quot;, &quot;_star_wars&quot;)) %&gt;% # Replace NAs in the total_batman and total_star_wars columns replace_na(list(total_batman = 0, total_star_wars = 0)) %&gt;% inner_join(colors, by = c(&quot;color_id&quot; = &quot;id&quot;)) %&gt;% # Create the difference and total columns mutate(difference = percent_batman - percent_star_wars, total = total_batman + total_star_wars) %&gt;% # Filter for totals greater than 200 filter(total &gt;= 200) %&gt;% mutate(name = fct_reorder(name, difference)) colors_joined ## # A tibble: 16 × 9 ## color_id total_batman percent_batman total_star_wars percent_star_wars name ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 0 2807 0.296 3258 0.207 Black ## 2 1 243 0.0256 410 0.0261 Blue ## 3 4 529 0.0558 434 0.0276 Red ## 4 14 426 0.0449 207 0.0132 Yellow ## 5 15 404 0.0426 1771 0.113 White ## 6 19 142 0.0150 1012 0.0644 Tan ## 7 28 98 0.0103 183 0.0116 Dark … ## 8 36 86 0.00907 246 0.0156 Trans… ## 9 46 200 0.0211 39 0.00248 Trans… ## 10 70 297 0.0313 373 0.0237 Reddi… ## 11 71 1148 0.121 3264 0.208 Light… ## 12 72 1453 0.153 2433 0.155 Dark … ## 13 84 278 0.0293 31 0.00197 Mediu… ## 14 179 154 0.0162 232 0.0148 Flat … ## 15 378 22 0.00232 430 0.0273 Sand … ## 16 7 0 NA 209 0.0133 Light… ## # … with 3 more variables: rgb &lt;chr&gt;, difference &lt;dbl&gt;, total &lt;dbl&gt; Visualizing the difference: Batman and Star Wars Create a bar plot using the colors_joined table to display the most prominent colors in the Batman and Star Wars themes, with the bars colored by their name. color_palette &lt;- structure(c(&quot;#05131D&quot;, &quot;#0055BF&quot;, &quot;#C91A09&quot;, &quot;#F2CD37&quot;, &quot;#FFFFFF&quot;, &quot;#E4CD9E&quot;, &quot;#958A73&quot;, &quot;#C91A09&quot;, &quot;#F5CD2F&quot;, &quot;#582A12&quot;, &quot;#A0A5A9&quot;, &quot;#6C6E68&quot;, &quot;#CC702A&quot;, &quot;#898788&quot;, &quot;#A0BCAC&quot;), .Names = c(&quot;Black&quot;, &quot;Blue&quot;, &quot;Red&quot;, &quot;Yellow&quot;, &quot;White&quot;, &quot;Tan&quot;, &quot;Dark Tan&quot;, &quot;Trans-Red&quot;, &quot;Trans-Yellow&quot;, &quot;Reddish Brown&quot;, &quot;Light Bluish Gray&quot;, &quot;Dark Bluish Gray&quot;, &quot;Medium Dark Flesh&quot;, &quot;Flat Silver&quot;, &quot;Sand Green&quot;)) # Create a bar plot using colors_joined and the name and difference columns ggplot(colors_joined, aes(name, difference, fill = name)) + geom_col() + coord_flip() + scale_fill_manual(values = color_palette, guide = FALSE) + labs(y = &quot;Difference: Batman - Star Wars&quot;) ## Warning: Removed 1 rows containing missing values (position_stack). ## Warning: It is deprecated to specify `guide = FALSE` to remove a guide. Please ## use `guide = &quot;none&quot;` instead. 8.4 Case Study: Joins on Stack Overflow Data questions &lt;- readRDS(gzcon(url(&quot;https://assets.datacamp.com/production/repositories/5284/datasets/89d5a716b4f41dbe4fcda1a7a1190f24f58f0e47/questions.rds&quot;))) tags &lt;- readRDS(gzcon(url(&quot;https://assets.datacamp.com/production/repositories/5284/datasets/207c31b235786e73496fd7e58e416779911a9d98/tags.rds&quot;))) question_tags &lt;- readRDS(gzcon(url(&quot;https://assets.datacamp.com/production/repositories/5284/datasets/966938d665c69bffd87393b345ea2837a94bab97/question_tags.rds&quot;))) answers &lt;- readRDS(gzcon(url(&quot;https://assets.datacamp.com/production/repositories/5284/datasets/6cb9c039aa8326d98de37afefa32e1c458764638/answers.rds&quot;))) Left-joining questions and tags Three of the Stack Overflow survey datasets are questions, question_tags, and tags: questions: an ID and the score, or how many times the question has been upvoted; the data only includes R-based questions question_tags: a tag ID for each question and the question’s id tags: a tag id and the tag’s name, which can be used to identify the subject of each question, such as ggplot2 or dplyr In this exercise, stitch together these datasets and replacing NAs in important fields: # Replace the NAs in the tag_name column questions_with_tags &lt;- questions %&gt;% left_join(question_tags, by = c(&quot;id&quot; = &quot;question_id&quot;)) %&gt;% left_join(tags, by = c(&quot;tag_id&quot; = &quot;id&quot;)) %&gt;% replace_na(list(tag_name = &quot;only-r&quot;)) questions_with_tags ## # A tibble: 545,694 × 5 ## id creation_date score tag_id tag_name ## &lt;int&gt; &lt;date&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 22557677 2014-03-21 1 18 regex ## 2 22557677 2014-03-21 1 139 string ## 3 22557677 2014-03-21 1 16088 time-complexity ## 4 22557677 2014-03-21 1 1672 backreference ## 5 22557707 2014-03-21 2 NA only-r ## 6 22558084 2014-03-21 2 6419 time-series ## 7 22558084 2014-03-21 2 92764 panel-data ## 8 22558395 2014-03-21 2 5569 function ## 9 22558395 2014-03-21 2 134 sorting ## 10 22558395 2014-03-21 2 9412 vectorization ## # … with 545,684 more rows Comparing scores across tags Let’s do a quick bit of analysis on it! You’ll use familiar dplyr verbs like group_by, summarize, arrange, and n to find out the average score of the most asked questions: questions_with_tags %&gt;% # Group by tag_name group_by(tag_name) %&gt;% # Get mean score and num_questions summarize(score = mean(score), num_questions = n()) %&gt;% # Sort num_questions in descending order arrange(desc(num_questions)) ## # A tibble: 7,841 × 3 ## tag_name score num_questions ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 only-r 1.26 48541 ## 2 ggplot2 2.61 28228 ## 3 dataframe 2.31 18874 ## 4 shiny 1.45 14219 ## 5 dplyr 1.95 14039 ## 6 plot 2.24 11315 ## 7 data.table 2.97 8809 ## 8 matrix 1.66 6205 ## 9 loops 0.743 5149 ## 10 regex 2 4912 ## # … with 7,831 more rows What tags never appear on R questions? The tags table includes all Stack Overflow tags, but some have nothing to do with R. Filter for just the tags that never appear on an R question with anti_join: # Using a join, filter for tags that are never on an R question tags %&gt;% anti_join(question_tags, by = c(&quot;id&quot; = &quot;tag_id&quot;)) ## # A tibble: 40,459 × 2 ## id tag_name ## &lt;dbl&gt; &lt;chr&gt; ## 1 124399 laravel-dusk ## 2 124402 spring-cloud-vault-config ## 3 124404 spring-vault ## 4 124405 apache-bahir ## 5 124407 astc ## 6 124408 simulacrum ## 7 124410 angulartics2 ## 8 124411 django-rest-viewsets ## 9 124414 react-native-lightbox ## 10 124417 java-module ## # … with 40,449 more rows Finding gaps between questions and answers Join together questions with answers to measure the time between questions and answers. Use an inner_join to combine the questions and answers tables using the suffixes \"_question\" and \"_answer\", respectively. Subtract creation_date_question from creation_date_answer within the as.integer() function to create the gap column. questions %&gt;% # Inner join questions and answers with proper suffixes inner_join(answers, by = c(&quot;id&quot; = &quot;question_id&quot;), suffix = c(&quot;_question&quot;, &quot;_answer&quot;)) %&gt;% # Subtract creation_date_question from creation_date_answer to create gap mutate(gap = as.integer(creation_date_answer - creation_date_question)) ## # A tibble: 380,643 × 7 ## id creation_date_question score_question id_answer creation_date_answer ## &lt;int&gt; &lt;date&gt; &lt;int&gt; &lt;int&gt; &lt;date&gt; ## 1 22557677 2014-03-21 1 22560670 2014-03-21 ## 2 22557707 2014-03-21 2 22558516 2014-03-21 ## 3 22557707 2014-03-21 2 22558726 2014-03-21 ## 4 22558084 2014-03-21 2 22558085 2014-03-21 ## 5 22558084 2014-03-21 2 22606545 2014-03-24 ## 6 22558084 2014-03-21 2 22610396 2014-03-24 ## 7 22558084 2014-03-21 2 34374729 2015-12-19 ## 8 22558395 2014-03-21 2 22559327 2014-03-21 ## 9 22558395 2014-03-21 2 22560102 2014-03-21 ## 10 22558395 2014-03-21 2 22560288 2014-03-21 ## # … with 380,633 more rows, and 2 more variables: score_answer &lt;int&gt;, gap &lt;int&gt; Joining question and answer counts Determine how many questions actually yield answers. Count the number of answers for each question, then join the answers counts with the questions table: answer_counts &lt;- answers %&gt;% count(question_id, sort = TRUE) question_answer_counts &lt;- questions %&gt;% left_join(answer_counts, by = c(&quot;id&quot; = &quot;question_id&quot;)) %&gt;% replace_na(list(n = 0)) Average answers by question Determine, on average, how many answers each questions gets. Some of the important variables from this table include: n, the number of answers for each question, and tag_name, the name of each tag associated with each question. tagged_answers &lt;- question_answer_counts %&gt;% inner_join(question_tags, by = c(&quot;id&quot; = &quot;question_id&quot;)) %&gt;% inner_join(tags, by = c(&quot;tag_id&quot; = &quot;id&quot;)) tagged_answers %&gt;% # Aggregate by tag_name group_by(tag_name) %&gt;% # Summarize questions and average_answers summarize(questions = n(), average_answers = mean(n)) %&gt;% # Sort the questions in descending order arrange(desc(questions)) ## # A tibble: 7,840 × 3 ## tag_name questions average_answers ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 ggplot2 28228 1.15 ## 2 dataframe 18874 1.67 ## 3 shiny 14219 0.921 ## 4 dplyr 14039 1.55 ## 5 plot 11315 1.23 ## 6 data.table 8809 1.47 ## 7 matrix 6205 1.45 ## 8 loops 5149 1.39 ## 9 regex 4912 1.91 ## 10 function 4892 1.30 ## # … with 7,830 more rows bind_rows() The bind_row function basically merges two tables together, or stack them into one big dataset instead of leaving them as two seperate tables. Binding and counting posts with tags Combine the questions_with_tags and answers_with_tags tables into posts_with_tags. Add a year column to the posts_with_tags table, then aggregate to count posts by type, year, and tag_name. questions_with_tags &lt;- questions %&gt;% inner_join(question_tags, by = c(&quot;id&quot; = &quot;question_id&quot;)) %&gt;% inner_join(tags, by = c(&quot;tag_id&quot; = &quot;id&quot;)) answers_with_tags &lt;- answers %&gt;% inner_join(question_tags, by = &quot;question_id&quot;) %&gt;% inner_join(tags, by = c(&quot;tag_id&quot; = &quot;id&quot;)) # Combine the two tables into posts_with_tags posts_with_tags &lt;- bind_rows(questions_with_tags %&gt;% mutate(type = &quot;question&quot;), answers_with_tags %&gt;% mutate(type = &quot;answer&quot;)) # Add a year column, then aggregate by type, year, and tag_name by_type_year_tag &lt;- posts_with_tags %&gt;% mutate(year = year(creation_date)) %&gt;% count(type, year, tag_name) by_type_year_tag ## # A tibble: 58,299 × 4 ## type year tag_name n ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; ## 1 answer 2008 bayesian 1 ## 2 answer 2008 dataframe 3 ## 3 answer 2008 dirichlet 1 ## 4 answer 2008 eof 1 ## 5 answer 2008 file 1 ## 6 answer 2008 file-io 1 ## 7 answer 2008 function 7 ## 8 answer 2008 global-variables 7 ## 9 answer 2008 math 2 ## 10 answer 2008 mathematical-optimization 1 ## # … with 58,289 more rows The code chunk outputs a dataset of posts (either questions or answers) that are tagged with their created dates, and a count for that post. Visualizing questions and answers in tags Create a plot to examine the information that the table contains about questions and answers for the dplyr and ggplot2 tags. Filter the by_type_year_tag table for the dplyr and ggplot2 tags. Create a line plot with that filtered table that plots the frequency (n) over time, colored by question/answer and faceted by tag. # Filter for the dplyr and ggplot2 tag names by_type_year_tag_filtered &lt;- by_type_year_tag %&gt;% filter(tag_name %in% c(&quot;dplyr&quot;, &quot;ggplot2&quot;)) # Create a line plot faceted by the tag name ggplot(by_type_year_tag_filtered, aes(year, n, color = type)) + geom_line() + facet_wrap(~ tag_name) "],["cleaning-data-in-r.html", "9 Cleaning Data in R 9.1 Common Data Problems 9.2 Categorical and Text Data 9.3 Advanced Data Problems 9.4 Record Linkage", " 9 Cleaning Data in R https://learn.datacamp.com/courses/cleaning-data-in-r 9.1 Common Data Problems Converting data types Before beginning to analyze any dataset, it’s important to take a look at the different types of columns from the. do that by using glimpse(): # Glimpse at bike_share_rides glimpse(bike_share_rides) ## Rows: 35,229 ## Columns: 10 ## $ ride_id &lt;int&gt; 52797, 54540, 87695, 45619, 70832, 96135, 29928, 83331… ## $ date &lt;chr&gt; &quot;2017-04-15&quot;, &quot;2017-04-19&quot;, &quot;2017-04-14&quot;, &quot;2017-04-03&quot;… ## $ duration &lt;chr&gt; &quot;1316.15 minutes&quot;, &quot;8.13 minutes&quot;, &quot;24.85 minutes&quot;, &quot;6… ## $ station_A_id &lt;dbl&gt; 67, 21, 16, 58, 16, 6, 5, 16, 5, 81, 30, 16, 16, 67, 2… ## $ station_A_name &lt;chr&gt; &quot;San Francisco Caltrain Station 2 (Townsend St at 4th… ## $ station_B_id &lt;dbl&gt; 89, 64, 355, 368, 81, 66, 350, 91, 62, 81, 109, 10, 80… ## $ station_B_name &lt;chr&gt; &quot;Division St at Potrero Ave&quot;, &quot;5th St at Brannan St&quot;, … ## $ bike_id &lt;dbl&gt; 1974, 860, 2263, 1417, 507, 75, 388, 239, 1449, 3289, … ## $ user_gender &lt;chr&gt; &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;… ## $ user_birth_year &lt;dbl&gt; 1972, 1986, 1993, 1981, 1981, 1988, 1993, 1996, 1993, … # Summary of user_birth_year summary(bike_share_rides$user_birth_year) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1900 1979 1986 1984 1991 2001 The summary statistics of user_birth_year don’t seem to offer much useful information about the different birth years because the user_birth_year column is a numeric type and should be converted to a factor. Use dplyr and assertive packages to convert a column into a factor and assert/confirm whether a column is the type wanted or not. Use as.___() functions to convert objects to a new data type. Use assert_is____() functions to confirm an object’s data type. # Convert user_birth_year to factor: user_birth_year_fct bike_share_rides &lt;- bike_share_rides %&gt;% mutate(user_birth_year_fct = as.factor(user_birth_year)) If the assert is TRUE, nothing will be outputted: # Assert user_birth_year_fct is a factor assert_is_factor(bike_share_rides$user_birth_year_fct) # Summary of user_birth_year_fct summary(bike_share_rides$user_birth_year_fct) ## 1900 1902 1923 1931 1938 1939 1941 1942 1943 1945 1946 1947 1948 1949 1950 1951 ## 1 7 2 23 2 1 3 10 4 16 5 24 9 30 37 25 ## 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 ## 70 49 65 66 112 62 156 99 196 161 256 237 245 349 225 363 ## 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 ## 365 331 370 548 529 527 563 601 481 541 775 876 825 1016 1056 1262 ## 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 ## 1157 1318 1606 1672 2135 1872 2062 1582 1703 1498 1476 1185 813 358 365 348 ## 2000 2001 ## 473 30 Trimming strings Another common dirty data problem is having extra bits like percent signs or periods in numbers, causing them to be read in as character. Use str_remove() to remove \"minutes\" from the duration column of bike_share_rides. Add this as a new column called duration_trimmed. Convert the duration_trimmed column to a numeric type and add this as a new column called duration_mins. Glimpse at bike_share_rides and assert that the duration_mins column is numeric. bike_share_rides &lt;- bike_share_rides %&gt;% # Remove &#39;minutes&#39; from duration: duration_trimmed mutate(duration_trimmed = str_remove(duration, &quot;minutes&quot;), # Convert duration_trimmed to numeric: duration_mins duration_mins = as.numeric(duration_trimmed)) # Glimpse at bike_share_rides glimpse(bike_share_rides) ## Rows: 35,229 ## Columns: 13 ## $ ride_id &lt;int&gt; 52797, 54540, 87695, 45619, 70832, 96135, 29928, 8… ## $ date &lt;chr&gt; &quot;2017-04-15&quot;, &quot;2017-04-19&quot;, &quot;2017-04-14&quot;, &quot;2017-04… ## $ duration &lt;chr&gt; &quot;1316.15 minutes&quot;, &quot;8.13 minutes&quot;, &quot;24.85 minutes&quot;… ## $ station_A_id &lt;dbl&gt; 67, 21, 16, 58, 16, 6, 5, 16, 5, 81, 30, 16, 16, 6… ## $ station_A_name &lt;chr&gt; &quot;San Francisco Caltrain Station 2 (Townsend St at… ## $ station_B_id &lt;dbl&gt; 89, 64, 355, 368, 81, 66, 350, 91, 62, 81, 109, 10… ## $ station_B_name &lt;chr&gt; &quot;Division St at Potrero Ave&quot;, &quot;5th St at Brannan S… ## $ bike_id &lt;dbl&gt; 1974, 860, 2263, 1417, 507, 75, 388, 239, 1449, 32… ## $ user_gender &lt;chr&gt; &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;M… ## $ user_birth_year &lt;dbl&gt; 1972, 1986, 1993, 1981, 1981, 1988, 1993, 1996, 19… ## $ user_birth_year_fct &lt;fct&gt; 1972, 1986, 1993, 1981, 1981, 1988, 1993, 1996, 19… ## $ duration_trimmed &lt;chr&gt; &quot;1316.15 &quot;, &quot;8.13 &quot;, &quot;24.85 &quot;, &quot;6.35 &quot;, &quot;9.8 &quot;, &quot;1… ## $ duration_mins &lt;dbl&gt; 1316.15, 8.13, 24.85, 6.35, 9.80, 17.47, 16.52, 14… # Assert duration_mins is numeric assert_is_numeric(bike_share_rides$duration_mins) For more details, go to the String Wrangling section at the bottom of Transform your data chapter of Working with Data in the Tidyverse. Range constraints Time range Values that are out of range can throw off an analysis, so it’s important to catch them early on. examine the duration_min column: Bikes are not allowed to be kept out more than 24 hours/1440 minutes at a time, but issues with some of the bikes caused inaccurate recording of the time they were returned. Create a three-bin histogram of the duration_min column of bike_share_rides using ggplot2 to identify if there is out-of-range data. Replace the values of duration_min that are greater than 1440 minutes (24 hours) with 1440. Add this to bike_share_rides as a new column called duration_min_const. Assert that all values of duration_min_const are between 0 and 1440: # Create breaks breaks &lt;- c(min(bike_share_rides$duration_mins), 0, 1440, max(bike_share_rides$duration_mins)) # Create a histogram of duration_min ggplot(bike_share_rides, aes(duration_mins)) + geom_histogram(breaks = breaks) # duration_min_const: replace vals of duration_min &gt; 1440 with 1440 bike_share_rides &lt;- bike_share_rides %&gt;% mutate(duration_min_const = replace(duration_mins, duration_mins &gt; 1440, 1440)) # Make sure all values of duration_min_const are between 0 and 1440 assert_all_are_in_closed_range(bike_share_rides$duration_min_const, lower = 0, upper = 1440) Date range Something has gone wrong and there are data with dates from the future, which is way outside of the date range to be working with. To fix this, remove any rides from the dataset that have a date in the future. Convert the date column of bike_share_rides from character to the Date data type. Assert that all values in the date column happened sometime in the past and not in the future. # Convert date to Date type bike_share_rides &lt;- bike_share_rides %&gt;% mutate(date = as.Date(date)) # Make sure all dates are in the past assert_all_are_in_past(bike_share_rides$date) ## Warning: Coercing bike_share_rides$date to class &#39;POSIXct&#39;. Filter bike_share_rides to get only the rides from the past or today, and save this as bike_share_rides_past. Assert that the dates in bike_share_rides_past occurred only in the past. # Filter for rides that occurred before or on today&#39;s date bike_share_rides_past &lt;- bike_share_rides %&gt;% filter(date &lt;= today()) # Make sure all dates from bike_share_rides_past are in the past assert_all_are_in_past(bike_share_rides_past$date) ## Warning: Coercing bike_share_rides_past$date to class &#39;POSIXct&#39;. Uniqueness constraints Full duplicates When multiple rows of a data frame share the same values for all columns, they’re full duplicates of each other. Removing duplicates like this is important, since having the same value repeated multiple times can alter summary statistics like the mean and median. Get the total number of full duplicates in bike_share_rides. Remove all full duplicates from bike_share_rides and save the new data frame as bike_share_rides_unique. Get the total number of full duplicates in the new bike_share_rides_unique data frame. # Count the number of full duplicates sum(duplicated(bike_share_rides)) ## [1] 0 # Remove duplicates bike_share_rides_unique &lt;- distinct(bike_share_rides) # Count the full duplicates in bike_share_rides_unique sum(duplicated(bike_share_rides_unique)) ## [1] 0 Partial duplicates Identify any partial duplicates and then practice the most common technique to deal with them, which involves dropping all partial duplicates, keeping only the first. Remove full and partial duplicates from bike_share_rides based on ride_id only, keeping all columns. Store this as bike_share_rides_unique. # Remove full and partial duplicates bike_share_rides_unique &lt;- bike_share_rides %&gt;% # Only based on ride_id instead of all cols distinct(ride_id, .keep_all = TRUE) # Find duplicated ride_ids in bike_share_rides_unique bike_share_rides_unique %&gt;% # Count the number of occurrences of each ride_id count(ride_id) %&gt;% # Filter for rows with a count &gt; 1 filter(n &gt; 1) ## # A tibble: 0 × 2 ## # … with 2 variables: ride_id &lt;int&gt;, n &lt;int&gt; Aggregating partial duplicates Another way of handling partial duplicates is to compute a summary statistic of the values that differ between partial duplicates, such as mean, median, maximum, or minimum. This can come in handy when you’re not sure how your data was collected and want an average, or if based on domain knowledge, you’d rather have too high of an estimate than too low of an estimate (or vice versa). bike_share_rides %&gt;% # Group by ride_id and date group_by(ride_id, date) %&gt;% # Add duration_min_avg column mutate(duration_min_avg = mean(duration_mins)) %&gt;% # Remove duplicates based on ride_id and date, keep all cols distinct(ride_id, date, .keep_all = TRUE) %&gt;% # Remove duration_min column select(-duration_mins) ## # A tibble: 35,229 × 14 ## # Groups: ride_id, date [35,229] ## ride_id date duration station_A_id station_A_name station_B_id ## &lt;int&gt; &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 52797 2017-04-15 1316.15 minutes 67 San Francisco C… 89 ## 2 54540 2017-04-19 8.13 minutes 21 Montgomery St B… 64 ## 3 87695 2017-04-14 24.85 minutes 16 Steuart St at M… 355 ## 4 45619 2017-04-03 6.35 minutes 58 Market St at 10… 368 ## 5 70832 2017-04-10 9.8 minutes 16 Steuart St at M… 81 ## 6 96135 2017-04-18 17.47 minutes 6 The Embarcadero… 66 ## 7 29928 2017-04-22 16.52 minutes 5 Powell St BART … 350 ## 8 83331 2017-04-11 14.72 minutes 16 Steuart St at M… 91 ## 9 72424 2017-04-05 4.12 minutes 5 Powell St BART … 62 ## 10 25910 2017-04-20 25.77 minutes 81 Berry St at 4th… 81 ## # … with 35,219 more rows, and 8 more variables: station_B_name &lt;chr&gt;, ## # bike_id &lt;dbl&gt;, user_gender &lt;chr&gt;, user_birth_year &lt;dbl&gt;, ## # user_birth_year_fct &lt;fct&gt;, duration_trimmed &lt;chr&gt;, ## # duration_min_const &lt;dbl&gt;, duration_min_avg &lt;dbl&gt; 9.2 Categorical and Text Data Membership data range A categorical data column would sometime have a limited range of observations that can be classified into membership list. Observations that doesn’t belong to this membership are outliers, and wouldn’t make sense. Count the number of occurrences of each dest_size in sfo_survey. \"huge\", \" Small \", \"Large \", and \" Hub\" appear to violate membership constraints. # Count the number of occurrences of dest_size sfo_survey %&gt;% count(dest_size) ## dest_size n ## 1 Small 1 ## 2 Hub 1 ## 3 Hub 1756 ## 4 Large 143 ## 5 Large 1 ## 6 Medium 682 ## 7 Small 225 Use the correct filtering join on sfo_survey and dest_sizes to get the rows of sfo_survey that have a valid dest_size: dest_sizes &lt;- structure(list(dest_size = c(&quot;Small&quot;, &quot;Medium&quot;, &quot;Large&quot;, &quot;Hub&quot; ), passengers_per_day = structure(c(1L, 3L, 4L, 2L), .Label = c(&quot;0-20K&quot;, &quot;100K+&quot;, &quot;20K-70K&quot;, &quot;70K-100K&quot;), class = &quot;factor&quot;)), .Names = c(&quot;dest_size&quot;, &quot;passengers_per_day&quot;), row.names = c(NA, -4L), class = &quot;data.frame&quot;) # Remove bad dest_size rows sfo_survey %&gt;% # Join with dest_sizes semi_join(dest_sizes, by = &quot;dest_size&quot;)%&gt;% # Count the number of each dest_size count(dest_size) ## dest_size n ## 1 Hub 1756 ## 2 Large 143 ## 3 Medium 682 ## 4 Small 225 Identifying inconsistency Sometimes, there are different kinds of inconsistencies that can occur within categories, making it look like a variable has more categories than it should. Examine the dest_size column again as well as the cleanliness column and determine what kind of issues, if any, these two categorical variables face. Count the number of occurrences of each category of the dest_size variable of sfo_survey. The categories in dest_size have inconsistent white space: # Count dest_size sfo_survey %&gt;% count(dest_size) ## dest_size n ## 1 Small 1 ## 2 Hub 1 ## 3 Hub 1756 ## 4 Large 143 ## 5 Large 1 ## 6 Medium 682 ## 7 Small 225 Count the number of occurrences of each category of the cleanliness variable of sfo_survey. The categories in cleanliness have inconsistent capitalization. # Count cleanliness sfo_survey %&gt;% count(cleanliness) ## cleanliness n ## 1 Average 433 ## 2 Clean 970 ## 3 Dirty 2 ## 4 Somewhat clean 1254 ## 5 Somewhat dirty 30 ## 6 &lt;NA&gt; 120 Correcting inconsistency dest_size has whitespace inconsistencies and cleanliness has capitalization inconsistencies, use the new tools to fix the inconsistent values in sfo_survey instead of removing the data points entirely. Add a column to sfo_survey called dest_size_trimmed that contains the values in the dest_size column with all leading and trailing whitespace removed. Add another column called cleanliness_lower that contains the values in the cleanliness column converted to all lowercase. # Add new columns to sfo_survey sfo_survey &lt;- sfo_survey %&gt;% # dest_size_trimmed: dest_size without whitespace mutate(dest_size_trimmed = str_trim(dest_size), # cleanliness_lower: cleanliness converted to lowercase cleanliness_lower = str_to_lower(cleanliness)) # Count values of dest_size_trimmed sfo_survey %&gt;% count(dest_size_trimmed) ## dest_size_trimmed n ## 1 Hub 1757 ## 2 Large 144 ## 3 Medium 682 ## 4 Small 226 # Count values of cleanliness_lower sfo_survey %&gt;% count(cleanliness_lower) ## cleanliness_lower n ## 1 average 433 ## 2 clean 970 ## 3 dirty 2 ## 4 somewhat clean 1254 ## 5 somewhat dirty 30 ## 6 &lt;NA&gt; 120 Collapsing categories Sometimes, there are observations that have input error that make it slightly different from the group it should belong to. Collapse(merge, or cover the error over with an umbrella group) to simply, fix the variable: # Count categories of dest_region sfo_survey %&gt;% count(dest_region) ## dest_region n ## 1 Asia 260 ## 2 Australia/New Zealand 66 ## 3 Canada/Mexico 220 ## 4 Central/South America 29 ## 5 East US 498 ## 6 Europe 401 ## 7 Middle East 79 ## 8 Midwest US 281 ## 9 West US 975 \"EU\", \"eur\", and \"Europ\" need to be collapsed to \"Europe\". Create a vector called europe_categories containing the three values of dest_region that need to be collapsed. Add a new column to sfo_survey called dest_region_collapsed that contains the values from the dest_region column, except the categories stored in europe_categories should be collapsed to Europe. # Count categories of dest_region sfo_survey %&gt;% count(dest_region) ## dest_region n ## 1 Asia 260 ## 2 Australia/New Zealand 66 ## 3 Canada/Mexico 220 ## 4 Central/South America 29 ## 5 East US 498 ## 6 Europe 401 ## 7 Middle East 79 ## 8 Midwest US 281 ## 9 West US 975 # Categories to map to Europe europe_categories &lt;- c(&quot;Europ&quot;, &quot;eur&quot;, &quot;EU&quot;) # Add a new col dest_region_collapsed sfo_survey %&gt;% # Map all categories in europe_categories to Europe mutate(dest_region_collapsed = fct_collapse(dest_region, Europe = europe_categories)) %&gt;% # Count categories of dest_region_collapsed count(dest_region_collapsed) ## Warning: Unknown levels in `f`: Europ, eur, EU ## dest_region_collapsed n ## 1 Asia 260 ## 2 Australia/New Zealand 66 ## 3 Canada/Mexico 220 ## 4 Central/South America 29 ## 5 East US 498 ## 6 Europe 401 ## 7 Middle East 79 ## 8 Midwest US 281 ## 9 West US 975 For more details, go to the (How To Collapse/Merge Levels) section of Manipulating Factor Variables chapter of Categorical Data in the Tidyverse. Detecting inconsistent text data Sometimes, in a column, there are inconsistent observations in different formats. Filter for rows with phone numbers that contain \"(\", or \")\". Remember to use fixed() when searching for parentheses. sfo_survey[1:10,] %&gt;% filter(str_detect(safety, &quot;safe&quot;) | str_detect(safety, &quot;danger&quot;)) ## id day airline destination dest_region dest_size ## 1 1844 Monday TURKISH AIRLINES ISTANBUL Middle East Hub ## 2 1840 Monday TURKISH AIRLINES ISTANBUL Middle East Hub ## 3 1837 Monday TURKISH AIRLINES ISTANBUL Middle East Hub ## 4 3010 Wednesday AMERICAN MIAMI East US Hub ## 5 1838 Monday TURKISH AIRLINES ISTANBUL Middle East Hub ## 6 1845 Monday TURKISH AIRLINES ISTANBUL Middle East Hub ## 7 2097 Monday UNITED INTL MEXICO CITY Canada/Mexico Hub ## 8 1846 Monday TURKISH AIRLINES ISTANBUL Middle East Hub ## boarding_area dept_time wait_min cleanliness safety ## 1 Gates 91-102 2018-12-31 315 Somewhat clean Somewhat safe ## 2 Gates 91-102 2018-12-31 165 Average Somewhat safe ## 3 Gates 91-102 2018-12-31 225 Somewhat clean Somewhat safe ## 4 Gates 50-59 2018-12-31 88 Somewhat clean Very safe ## 5 Gates 91-102 2018-12-31 195 Somewhat clean Very safe ## 6 Gates 91-102 2018-12-31 135 Average Somewhat safe ## 7 Gates 91-102 2018-12-31 145 Somewhat clean Somewhat safe ## 8 Gates 91-102 2018-12-31 145 Clean Somewhat safe ## satisfaction dest_size_trimmed cleanliness_lower ## 1 Somewhat satsified Hub somewhat clean ## 2 Somewhat satsified Hub average ## 3 Somewhat satsified Hub somewhat clean ## 4 Somewhat satsified Hub somewhat clean ## 5 Somewhat satsified Hub somewhat clean ## 6 Somewhat satsified Hub average ## 7 Somewhat satsified Hub somewhat clean ## 8 Somewhat satsified Hub clean For more details, go to the String Wrangling section at the bottom of Transform your data chapter of Working with Data in the Tidyverse. Replacing and removing The str_remove_all() function will remove all instances of the string passed to it. sfo_survey[1:10,] %&gt;% mutate(safe_or_not = str_remove_all(safety, &quot;Somewhat&quot;)) %&gt;% select(airline, safe_or_not) ## airline safe_or_not ## 1 TURKISH AIRLINES Neutral ## 2 TURKISH AIRLINES safe ## 3 TURKISH AIRLINES safe ## 4 TURKISH AIRLINES safe ## 5 TURKISH AIRLINES Neutral ## 6 AMERICAN Very safe ## 7 TURKISH AIRLINES Very safe ## 8 TURKISH AIRLINES safe ## 9 UNITED INTL safe ## 10 TURKISH AIRLINES safe Again, go to the String Wrangling section at the bottom of Transform your data Filter/select observations with certain length The str_length() function takes in a character vector, returns a number for each element that indicates the length of each element. clean_only &lt;- sfo_survey %&gt;% filter(str_length(cleanliness_lower) == 5) clean_only[1:10,] %&gt;% select(airline, cleanliness_lower) ## airline cleanliness_lower ## 1 TURKISH AIRLINES clean ## 2 TURKISH AIRLINES clean ## 3 TURKISH AIRLINES clean ## 4 TURKISH AIRLINES clean ## 5 TURKISH AIRLINES clean ## 6 TURKISH AIRLINES clean ## 7 CATHAY PACIFIC clean ## 8 UNITED clean ## 9 UNITED clean ## 10 FRONTIER clean 9.3 Advanced Data Problems Date uniformity Make sure that the accounts dataset doesn’t contain any uniformity problems. In this exercise, investigate the date_opened column and clean it up so that all the dates are in the same format. By default, as.Date() can’t convert \"Month DD, YYYY\" formats: as.Date(accounts$date_opened) ## [1] &quot;2003-10-19&quot; NA &quot;2008-07-29&quot; &quot;2005-06-09&quot; &quot;2012-03-31&quot; ## [6] &quot;2007-06-20&quot; NA &quot;2019-06-03&quot; &quot;2011-05-07&quot; &quot;2018-04-07&quot; ## [11] &quot;2018-11-16&quot; &quot;2001-04-16&quot; &quot;2005-04-21&quot; &quot;2006-06-13&quot; &quot;2009-01-07&quot; ## [16] &quot;2012-07-07&quot; NA NA &quot;2004-05-21&quot; &quot;2001-09-06&quot; ## [21] &quot;2005-04-09&quot; &quot;2009-10-20&quot; &quot;2003-05-16&quot; &quot;2015-10-25&quot; NA ## [26] NA NA &quot;2008-12-27&quot; &quot;2015-11-11&quot; &quot;2009-02-26&quot; ## [31] &quot;2008-12-26&quot; NA NA &quot;2005-12-13&quot; NA ## [36] &quot;2004-12-03&quot; &quot;2016-10-19&quot; NA &quot;2009-10-05&quot; &quot;2013-07-11&quot; ## [41] &quot;2002-03-24&quot; &quot;2015-10-17&quot; NA NA &quot;2019-11-12&quot; ## [46] NA NA &quot;2019-10-01&quot; &quot;2000-08-17&quot; &quot;2001-04-11&quot; ## [51] NA &quot;2016-06-30&quot; NA NA &quot;2013-05-23&quot; ## [56] &quot;2017-02-24&quot; NA &quot;2004-11-02&quot; &quot;2019-03-06&quot; &quot;2018-09-01&quot; ## [61] NA &quot;2002-12-31&quot; &quot;2013-07-27&quot; &quot;2014-01-10&quot; &quot;2011-12-14&quot; ## [66] NA &quot;2008-03-01&quot; &quot;2018-05-07&quot; &quot;2017-11-23&quot; NA ## [71] &quot;2008-09-27&quot; NA &quot;2008-01-07&quot; NA &quot;2005-05-11&quot; ## [76] &quot;2003-08-12&quot; NA NA NA &quot;2014-11-25&quot; ## [81] NA NA NA &quot;2008-04-01&quot; NA ## [86] &quot;2002-10-01&quot; &quot;2011-03-25&quot; &quot;2000-07-11&quot; &quot;2014-10-19&quot; NA ## [91] &quot;2013-06-20&quot; &quot;2008-01-16&quot; &quot;2016-06-24&quot; NA NA ## [96] &quot;2007-04-29&quot; NA NA For more details, go to the Date Formats section of Utilities chapter of Intermediate R. Convert the dates in the date_opened column to the same format using the formats vector and store this as a new column called date_opened_clean: # Define the date formats formats &lt;- c(&quot;%Y-%m-%d&quot;, &quot;%B %d, %Y&quot;) # Convert dates to the same format accounts[1:10,] %&gt;% mutate(date_opened_clean = parse_date_time(date_opened, formats)) ## id date_opened total date_opened_clean ## 1 A880C79F 2003-10-19 169305 2003-10-19 ## 2 BE8222DF October 05, 2018 107460 2018-10-05 ## 3 19F9E113 2008-07-29 15297152 2008-07-29 ## 4 A2FE52A3 2005-06-09 14897272 2005-06-09 ## 5 F6DC2C08 2012-03-31 124568 2012-03-31 ## 6 D2E55799 2007-06-20 13635752 2007-06-20 ## 7 53AE87EF December 01, 2017 15375984 2017-12-01 ## 8 3E97F253 2019-06-03 14515800 2019-06-03 ## 9 4AE79EA1 2011-05-07 23338536 2011-05-07 ## 10 2322DFB4 2018-04-07 189524 2018-04-07 Currency uniformity Now that dates are in order, correct any unit differences. First, plot the data, there’s a group of very high values, and a group of relatively lower values. The bank has two different offices - one in New York, and one in Tokyo, so the accounts managed by the Tokyo office are in Japanese yen instead of U.S. Create a scatter plot with date_opened on the x-axis and total on the y-axis: # Scatter plot of opening date and total amount accounts %&gt;% ggplot(aes(x = date_opened, y = total)) + geom_point() Left join accounts and account_offices by their id columns. Convert the totals from the Tokyo office from yen to dollars, and keep the total from the New York office in dollars. Store this as a new column called total_usd: # Left join accounts to account_offices by id accounts[1:10,] %&gt;% left_join(account_offices, by = &quot;id&quot;) %&gt;% # Convert totals from the Tokyo office to USD mutate(total_usd = ifelse(office == &quot;Tokyo&quot;, total / 104, total)) ## id date_opened total office total_usd ## 1 A880C79F 2003-10-19 169305 New York 169305 ## 2 BE8222DF October 05, 2018 107460 New York 107460 ## 3 19F9E113 2008-07-29 15297152 Tokyo 147088 ## 4 A2FE52A3 2005-06-09 14897272 Tokyo 143243 ## 5 F6DC2C08 2012-03-31 124568 New York 124568 ## 6 D2E55799 2007-06-20 13635752 Tokyo 131113 ## 7 53AE87EF December 01, 2017 15375984 Tokyo 147846 ## 8 3E97F253 2019-06-03 14515800 Tokyo 139575 ## 9 4AE79EA1 2011-05-07 23338536 Tokyo 224409 ## 10 2322DFB4 2018-04-07 189524 New York 189524 Cross field validation Cross field validation basically means cross-checking/comparing with other columns to make sure the compared column values make sense. Validating totals There are three different funds that account holders can store their money in. In this exercise, validate whether the total amount in each account is equal to the sum of the amount in fund_A, fund_B, and fund_C. Create a new column called theoretical_total that contains the sum of the amounts in each fund. Find the accounts where the total doesn’t match the theoretical_total. # Find invalid totals accounts_funds %&gt;% # theoretical_total: sum of the three funds mutate(theoretical_total = fund_A + fund_B + fund_C) %&gt;% # Find accounts where total doesn&#39;t match theoretical_total filter(theoretical_total != total) ## id date_opened total fund_A fund_B fund_C acct_age theoretical_total ## 1 D5EB0F00 2001-04-16 130920 69487 48681 56408 19 174576 ## 2 92C237C6 2005-12-13 85362 72556 21739 19537 15 113832 ## 3 0E5B69F5 2018-05-07 134488 88475 44383 46475 2 179333 Validating age Now that some inconsistencies in the total amounts been found, there may also be inconsistencies in the acct_age column, maybe these inconsistencies are related. Validate the age of each account and see if rows with inconsistent acct_ages are the same ones that had inconsistent totals. Create a new column called theoretical_age that contains the age of each account based on the date_opened. Find the accounts where the acct_age doesn’t match the theoretical_age. # Find invalid acct_age accounts_funds %&gt;% # theoretical_age: age of acct based on date_opened mutate(theoretical_age = floor(as.numeric(date_opened %--% today(), &quot;years&quot;))) %&gt;% # Filter for rows where acct_age is different from theoretical_age filter(acct_age != theoretical_age) ## id date_opened total fund_A fund_B fund_C acct_age theoretical_age ## 1 A880C79F 2003-10-19 169305 85018 75580 8707 17 18 ## 2 BE8222DF 2018-10-05 107460 64784 35194 7482 2 3 ## 3 19F9E113 2008-07-29 147088 64029 15300 67759 12 13 ## 4 A2FE52A3 2005-06-09 143243 63466 54053 25724 15 16 ## 5 F6DC2C08 2012-03-31 124568 21156 47935 55477 8 10 ## 6 D2E55799 2007-06-20 131113 79241 26800 25072 13 14 ## 7 53AE87EF 2017-12-01 147846 38450 29185 80211 3 4 ## 8 3E97F253 2019-06-03 139575 11045 65907 62623 1 2 ## 9 4AE79EA1 2011-05-07 224409 68394 80418 75597 9 11 ## 10 2322DFB4 2018-04-07 189524 66964 52238 70322 2 4 ## 11 645335B2 2018-11-16 154001 68691 56400 28910 2 3 ## 12 D5EB0F00 2001-04-16 130920 69487 48681 56408 19 21 ## 13 1EB593F7 2005-04-21 191989 75388 84199 32402 15 17 ## 14 DDBA03D9 2006-06-13 92473 32931 22162 37380 14 15 ## 15 40E4A2F4 2009-01-07 180547 82564 68210 29773 12 13 ## 16 39132EEA 2012-07-07 150115 26358 74286 49471 8 9 ## 17 387F8E4D 2011-01-03 90410 7520 67142 15748 10 11 ## 18 11C3C3C0 2017-12-24 180003 84295 31591 64117 2 4 ## 19 C2FC91E1 2004-05-21 105722 25398 24075 56249 16 17 ## 20 FB8F01C1 2001-09-06 217068 69738 86768 60562 19 20 ## 21 0128D2D0 2005-04-09 184421 82221 60149 42051 15 17 ## 22 BE6E4B3F 2009-10-20 150769 49607 55417 45745 11 12 ## 23 7C6E2ECC 2003-05-16 169814 82093 62756 24965 17 18 ## 24 02E63545 2015-10-25 125117 50287 23342 51488 5 6 ## 25 4399C98B 2001-05-19 130421 58177 43912 28332 19 20 ## 26 98F4CF0F 2014-05-27 143211 84645 7088 51478 6 7 ## 27 247222A6 2015-05-26 150372 69104 63369 17899 5 6 ## 28 420985EE 2008-12-27 123125 59390 27890 35845 12 13 ## 29 0E3903BA 2015-11-11 182668 47236 87437 47995 5 6 ## 30 64EF994F 2009-02-26 161141 89269 25939 45933 11 13 ## 31 CCF84EDB 2008-12-26 136128 33405 89016 13707 12 13 ## 32 51C21705 2016-04-22 155684 53542 38234 63908 4 6 ## 33 C868C6AD 2000-01-31 112818 17876 15057 79885 21 22 ## 34 92C237C6 2005-12-13 85362 72556 21739 19537 15 16 ## 35 9ECEADB2 2018-05-17 146153 40675 46482 58996 2 3 ## 36 DF0AFE50 2004-12-03 146635 67373 63443 15819 16 17 ## 37 5CD605B3 2016-10-19 87921 8474 50284 29163 4 5 ## 38 402839E2 2019-09-14 163416 59213 23460 80743 1 2 ## 39 78286CE7 2009-10-05 144704 72495 38450 33759 11 12 ## 40 168E071B 2013-07-11 87826 21642 42937 23247 7 8 ## 41 466CCDAA 2002-03-24 144051 19756 80182 44113 18 20 ## 42 8DE1ECB9 2015-10-17 217975 67105 72907 77963 5 6 ## 43 E19FE6B5 2009-06-06 101936 39942 38580 23414 11 12 ## 44 1240D39C 2011-09-07 151556 18835 46135 86586 9 10 ## 45 A7BFAA72 2019-11-12 133790 56001 54885 22904 1 2 ## 46 C3D24436 2002-05-24 101584 58434 21069 22081 18 19 ## 47 FAD92F0F 2007-09-13 164241 70211 73984 20046 13 14 ## 48 236A1D51 2019-10-01 177759 20886 80883 75990 1 2 ## 49 A6DDDC4C 2000-08-17 67962 5970 20088 41904 20 21 ## 50 DDFD0B3D 2001-04-11 151696 30596 84390 36710 19 21 ## 51 D13375E9 2005-11-01 134083 28545 37537 68001 15 16 ## 52 AC50B796 2016-06-30 154916 54451 35906 64559 4 5 ## 53 290319FD 2005-05-27 170178 54341 32764 83073 15 16 ## 54 FC71925A 2006-11-02 186281 89127 43356 53798 14 15 ## 55 7B0F3685 2013-05-23 179102 81321 18106 79675 7 8 ## 56 BE411172 2017-02-24 170096 86735 56580 26781 3 5 ## 57 58066E39 2015-09-16 163708 59004 16987 87717 5 6 ## 58 EA7FF83A 2004-11-02 111526 86856 19406 5264 15 17 ## 59 14A2DDB7 2019-03-06 123163 49666 25407 48090 1 3 ## 60 305EEAA8 2018-09-01 138632 20307 35028 83297 2 3 ## 61 8F25E54C 2008-11-24 189126 72037 62513 54576 12 13 ## 62 19DD73C6 2002-12-31 141275 72872 51219 17184 18 19 ## 63 ACB8E6AF 2013-07-27 71359 10203 51163 9993 7 8 ## 64 91BFCC40 2014-01-10 132859 67405 7399 58055 7 8 ## 65 86ACAF81 2011-12-14 235901 79599 79291 77011 9 10 ## 66 77E85C14 2009-11-20 133348 20954 33018 79376 11 12 ## 67 C5C6B79D 2008-03-01 188424 61972 69266 57186 12 14 ## 68 0E5B69F5 2018-05-07 134488 88475 44383 46475 2 4 ## 69 5275B518 2017-11-23 71665 16114 35691 19860 3 4 ## 70 17217048 2001-05-25 193377 45365 58558 89454 19 20 ## 71 E7496A7F 2008-09-27 142669 8615 72841 61213 12 13 ## 72 41BBB7B4 2005-02-22 144229 26449 83938 33842 15 17 ## 73 F6C7ABA1 2008-01-07 183440 82468 73281 27691 13 14 ## 74 E699DF01 2008-02-17 199603 84788 47808 67007 12 14 ## 75 BACA7378 2005-05-11 204271 87254 57043 59974 15 17 ## 76 84A4302F 2003-08-12 186737 86632 33506 66599 17 18 ## 77 F8A78C27 2006-04-05 41164 7560 21040 12564 14 16 ## 78 8BADDF6A 2010-12-31 158203 25477 43902 88824 10 11 ## 79 9FB57E68 2017-09-01 216352 86665 77117 52570 3 4 ## 80 5C98E8F5 2014-11-25 103200 28990 24986 49224 6 7 ## 81 6BB53C2A 2016-12-03 146394 29561 29023 87810 4 5 ## 82 E23F2505 2017-10-15 121614 59013 39086 23515 3 4 ## 83 0C121914 2017-06-21 227729 86625 79950 61154 3 4 ## 84 3627E08A 2008-04-01 238104 60475 89011 88618 11 14 ## 85 A94493B3 2009-08-01 85975 48482 7054 30439 11 12 ## 86 0682E9DE 2002-10-01 72832 15809 15617 41406 18 19 ## 87 49931170 2011-03-25 139614 83035 22239 34340 9 11 ## 88 A154F63B 2000-07-11 133800 42648 16464 74688 20 21 ## 89 3690CCED 2014-10-19 226595 70260 84337 71998 6 7 ## 90 48F5E6D8 2020-02-16 135435 29123 23204 83108 0 2 ## 91 515FAD84 2013-06-20 98190 6452 60014 31724 7 8 ## 92 59794264 2008-01-16 157964 68869 32999 56096 13 14 ## 93 2038185B 2016-06-24 194662 20591 89990 84081 4 5 ## 94 65EAC615 2004-02-20 140191 20108 46764 73319 16 18 ## 95 6C7509C9 2000-09-16 212089 58861 76975 76253 20 21 ## 96 BD969A9D 2007-04-29 167238 10234 83183 73821 13 15 ## 97 B0CDCE3D 2014-05-28 145240 62549 48606 34085 6 7 ## 98 33A7F03E 2007-10-14 191839 80542 87909 23388 13 14 Visualizing missing data Dealing with missing data is one of the most common tasks in data science. There are a variety of types of missingness, as well as a variety of types of solutions to missing data. A new version of the accounts data frame containing data on the amount held and amount invested for new and existing customers. However, there are rows with missing inv_amount values. Visualize the missing values in accounts by column using vis_miss() from the visdat package. # Visualize the missing values by column vis_miss(accounts_inv) Most customers below 25 do not have investment accounts yet, and suspect it could be driving the missingness. accounts_inv %&gt;% # missing_inv: Is inv_amount missing? mutate(missing_inv = is.na(inv_amount)) %&gt;% # Group by missing_inv group_by(missing_inv) %&gt;% # Calculate mean age for each missing_inv group summarize(avg_age = mean(age)) ## # A tibble: 2 × 2 ## missing_inv avg_age ## &lt;lgl&gt; &lt;dbl&gt; ## 1 FALSE 43.6 ## 2 TRUE 21.8 Since the average age for TRUE missing_inv is 22 and the average age for FALSE missing_inv is 44, it is likely that the inv_amount variable is missing mostly in young customers. # Sort by age and visualize missing vals accounts_inv %&gt;% arrange(age) %&gt;% vis_miss() 9.4 Record Linkage Damerau-Levenshtein distance is used to identify how similar two strings are. As a reminder, Damerau-Levenshtein distance is the minimum number of steps needed to get from String A to String B, using these operations: Insertion of a new character. Deletion of an existing character. Substitution of an existing character. Transposition of two existing consecutive characters. Use the stringdist package to compute string distances using various methods. # Calculate Damerau-Levenshtein distance stringdist(&quot;las angelos&quot;, &quot;los angeles&quot;, method = &quot;dl&quot;) ## [1] 2 LCS (Longest Common Subsequence) only considers Insertion and Deletion. # Calculate LCS distance stringdist(&quot;las angelos&quot;, &quot;los angeles&quot;, method = &quot;lcs&quot;) ## [1] 4 # Calculate Jaccard distance stringdist(&quot;las angelos&quot;, &quot;los angeles&quot;, method = &quot;jaccard&quot;) ## [1] 0 Fixing typos with string distance zagat, is a set of restaurants in New York, Los Angeles, Atlanta, San Francisco, and Las Vegas. The data is from Zagat, a company that collects restaurant reviews, and includes the restaurant names, addresses, phone numbers, as well as other restaurant information. The city column contains the name of the city that the restaurant is located in. However, there are a number of typos throughout the column. Map each city to one of the five correctly-spelled cities contained in the cities data frame. Left join zagat and cities based on string distance using the city and city_actual columns. stringdist_left_join function from the fuzzyjoin package that allows you to do a stringdist left join. # Count the number of each city variation zagat[1:10,] %&gt;% count(city) ## city n ## 1 llos angeles 1 ## 2 lo angeles 2 ## 3 los anegeles 1 ## 4 los angeles 6 # Join and look at results zagat[1:10,] %&gt;% # Left join based on stringdist using city and city_actual cols stringdist_left_join(cities, by = c(&quot;city&quot; = &quot;city_actual&quot;)) %&gt;% # Select the name, city, and city_actual cols select(name, city, city_actual) ## name city city_actual ## 1 apple pan the llos angeles los angeles ## 2 asahi ramen los angeles los angeles ## 3 baja fresh los angeles los angeles ## 4 belvedere the los angeles los angeles ## 5 benita&#39;s frites lo angeles los angeles ## 6 bernard&#39;s los angeles los angeles ## 7 bistro 45 lo angeles los angeles ## 8 brighton coffee shop los angeles los angeles ## 9 bristol farms market cafe los anegeles los angeles ## 10 cafe&#39;50s los angeles los angeles Record linkage record linkage is the act of linking data from different sources regarding the same entity. But unlike joins, record linkage does not require exact matches between different pairs of data, and instead can find close matches using string similarity. This is why record linkage is effective when there are no common unique keys between the data sources you can rely upon when linking data sources such as a unique identifier. Pair blocking Generate all possible pairs, and then use newly-cleaned city column as a blocking variable. A blocking variable is helpful when the dataset is too big and you don’t want to compare/match all the possible pairs with each every one of the observations. # Generate pairs with same city pair_blocking(zagat, fodors, blocking_var = &quot;city&quot;) ## Simple blocking ## Blocking variable(s): city ## First data set: 310 records ## Second data set: 533 records ## Total number of pairs: 27 694 pairs ## ## ldat with 27 694 rows and 2 columns ## x y ## 1 2 1 ## 2 2 2 ## 3 2 3 ## 4 2 4 ## 5 2 5 ## 6 2 6 ## 7 2 7 ## 8 2 8 ## 9 2 9 ## 10 2 10 ## : : : ## 27685 307 524 ## 27686 307 525 ## 27687 307 526 ## 27688 307 527 ## 27689 307 528 ## 27690 307 529 ## 27691 307 530 ## 27692 307 531 ## 27693 307 532 ## 27694 307 533 Comparing pairs Compare pairs by name, phone, and addr using jaro_winkler(). compare_pairs() can take in a character vector of column names as the by argument. # Generate pairs pair_blocking(zagat, fodors, blocking_var = &quot;city&quot;) %&gt;% # Compare pairs by name, phone, addr compare_pairs(by = c(&quot;name&quot;, &quot;phone&quot;, &quot;addr&quot;), default_comparator = jaro_winkler()) ## Compare ## By: name, phone, addr ## ## Simple blocking ## Blocking variable(s): city ## First data set: 310 records ## Second data set: 533 records ## Total number of pairs: 27 694 pairs ## ## ldat with 27 694 rows and 5 columns ## x y name phone addr ## 1 2 1 0.4959307 0.7152778 0.5948270 ## 2 2 2 0.6197391 0.6269841 0.6849415 ## 3 2 3 0.4737762 0.8222222 0.5754386 ## 4 2 4 0.4131313 0.6111111 0.6435407 ## 5 2 5 0.6026936 0.6527778 0.6132376 ## 6 2 6 0.5819625 0.7361111 0.6108862 ## 7 2 7 0.4242424 0.6111111 0.6207899 ## 8 2 8 0.4303030 0.5555556 0.5566188 ## 9 2 9 0.4559885 0.6666667 0.6283892 ## 10 2 10 0.5798461 0.7152778 0.4885965 ## : : : : : : ## 27685 307 524 0.6309524 0.7361111 0.6574074 ## 27686 307 525 0.3683473 0.6666667 0.6650327 ## 27687 307 526 0.5306878 0.7962963 0.4888889 ## 27688 307 527 0.4841270 0.7407407 0.6499183 ## 27689 307 528 0.4285714 0.6666667 0.5882173 ## 27690 307 529 0.5026455 0.6111111 0.6357143 ## 27691 307 530 0.4087302 0.6666667 0.5470085 ## 27692 307 531 0.5591479 0.7407407 0.8141026 ## 27693 307 532 0.4226190 0.7222222 0.5004274 ## 27694 307 533 0.4005602 0.6746032 0.6119048 "],["introduction-to-sql.html", "10 Introduction to SQL 10.1 Selecting columns 10.2 Filtering rows 10.3 Aggregate Functions 10.4 Sorting and grouping", " 10 Introduction to SQL https://learn.datacamp.com/courses/introduction-to-sql 10.1 Selecting columns SELECTing single columns A query is a request for data from a database table (or combination of tables). Querying is an essential skill for a data scientist, since the data you need for your analyses will often live in databases. In SQL, you can select data from a table using a SELECT statement. For example, the following query selects the name column from the people table: SELECT name FROM people; In this query, SELECT and FROM are called keywords. In SQL, keywords are not case-sensitive, which means you can write the same query as: select name from people; SELECTing multiple columns To select multiple columns from a table, simply separate the column names with commas! For example, this query selects two columns, name and birthdate, from the people table: SELECT name, birthdate FROM people; Sometimes, you may want to select all columns from a table. Typing out every column name would be a pain, so there’s a handy shortcut: SELECT (*) FROM people; If you only want to return a certain number of results, you can use the LIMIT keyword to limit the number of rows returned: SELECT (*) FROM people LIMIT 10; SELECT DISTINCT Often your results will include many duplicate values. If you want to select all the unique values from a column, you can use the DISTINCT keyword. This might be useful if, for example, you’re interested in knowing which languages are represented in the films table: SELECT DISTINCT language FROM films; Learning to COUNT The COUNT statement allow counting the number of observations in one or more columns. For example, this code gives the number of rows in the people table: SELECT COUNT(*) FROM people; 10.2 Filtering rows Filtering results In SQL, the WHERE keyword allows you to filter based on both text and numeric values in a table. There are a few different comparison operators you can use: = equal &lt;&gt; not equal &lt; less than &gt; greater than &lt;= less than or equal to &gt;= greater than or equal to For example, you can filter text records such as title. The following code returns all films with the title 'Metropolis': SELECT title FROM films WHERE title = &#39;Metropolis&#39;; # Notice that the `WHERE` clause always comes after the `FROM` statement! ## Note that in this course we will use `&lt;&gt;` and not `!=` for the not equal operator, as per the SQL standard. Simple filtering of numeric values The WHERE clause can also be used to filter numeric records, such as years or ages. For example, the following query selects all details for films with a budget over ten thousand dollars: SELECT * FROM films WHERE budget &gt; 10000; Simple filtering of text The WHERE clause can also be used to filter text results, such as names or countries. For example, this query gets the titles of all films which were filmed in China: SELECT title FROM films WHERE country = &#39;China&#39;; WHERE AND Often, you’ll want to select data based on multiple conditions. You can build up your WHERE queries by combining multiple conditions with the AND keyword: SELECT title FROM films WHERE release_year &gt; 1994 AND release_year &lt; 2000; gives you the titles of films released between 1994 and 2000. Note that you need to specify the column name separately for every AND condition, so the following would be invalid: SELECT title FROM films WHERE release_year &gt; 1994 AND &lt; 2000; WHERE AND OR To select rows based on multiple conditions where some but not all of the conditions need to be met, SQL has the OR operator. For example, the following returns all films released in either 1994 or 2000: SELECT title FROM films WHERE release_year = 1994 OR release_year = 2000; Note that you need to specify the column for every OR condition, so the following is invalid: SELECT title FROM films WHERE release_year = 1994 OR 2000; When combining AND and OR, be sure to enclose the individual clauses in parentheses, like so: SELECT title FROM films WHERE (release_year = 1994 OR release_year = 1995) AND (certification = &#39;PG&#39; OR certification = &#39;R&#39;); BETWEEN Use the following query to get titles of all films released in and between 1994 and 2000: SELECT title FROM films WHERE release_year &gt;= 1994 AND release_year &lt;= 2000; Checking for ranges like this is very common, so in SQL the BETWEEN keyword provides a useful shorthand for filtering values within a specified range. This query is equivalent to the one above: SELECT title FROM films WHERE release_year BETWEEN 1994 AND 2000; ## It&#39;s important to remember that `BETWEEN` is inclusive, meaning the beginning and end values are included in the results! WHERE IN As you’ve seen, WHERE is very useful for filtering results. However, if you want to filter based on many conditions, WHERE can get unwieldy. For example: SELECT name FROM kids WHERE age = 2 OR age = 4 OR age = 6 OR age = 8 OR age = 10; Enter the IN operator! The IN operator allows you to specify multiple values in a WHERE clause, making it easier and quicker to specify multiple OR conditions! So, the above example would become simply: SELECT name FROM kids WHERE age IN (2, 4, 6, 8, 10); NULL and IS NULL In SQL, NULL represents a missing or unknown value. You can check for NULL values using the expression IS NULL. For example, to count the number of missing birth dates in the people table: SELECT COUNT(*) FROM people WHERE birthdate IS NULL; IS NULL is useful when combined with WHERE to figure out what data you’re missing. Sometimes, you’ll want to filter out missing values so you only get results which are not NULL. To do this, you can use the IS NOT NULL operator. For example, this query gives the names of all people whose birth dates are not missing in the people table. SELECT name FROM people WHERE birthdate IS NOT NULL; LIKE and NOT LIKE The WHERE clause can be used to filter text data. However, so far you’ve only been able to filter by specifying the exact text you’re interested in. Often you’ll want to search for a pattern rather than a specific text string. In SQL, the LIKE operator can be used in a WHERE clause to search for a pattern in a column. To accomplish this, you use something called a wildcard as a placeholder for some other values. There are two wildcards you can use with LIKE: The % wildcard will match zero, one, or many characters in text. For example, the following query matches companies like 'Data', 'DataC' 'DataCamp', 'DataMind', and so on: SELECT name FROM companies WHERE name LIKE &#39;Data%&#39;; The _ wildcard will match a single character. For example, the following query matches companies like 'DataCamp', 'DataComp', and so on: SELECT name FROM companies WHERE name LIKE &#39;DataC_mp&#39;; You can also use the NOT LIKE operator to find records that don’t match the pattern you specify. 10.3 Aggregate Functions SQL provides a few functions, called aggregate functions: AVG(), MAX(), MIN(), SUM(). SELECT AVG(budget) FROM films; Gives the average value from the budget column of the films table. Similarly, the MAX function returns the highest budget: SELECT MAX(budget) FROM films; The SUM function returns the result of adding up the numeric values in a column: SELECT SUM(budget) FROM films; Combining aggregate functions with WHERE Aggregate functions can be combined with the WHERE clause to gain further insights from the data. For example, to get the total budget of movies made in the year 2010 or later: SELECT SUM(budget) FROM films WHERE release_year &gt;= 2010; Get the amount grossed by the worst performing film in 1994: SELECT MIN(gross) FROM films WHERE release_year = 1994; A note on arithmetic In addition to using aggregate functions, you can perform basic arithmetic with symbols like +, -, *, and /. So, for example, this gives a result of 12: SELECT (4 * 3); However, the following gives a result of 1: SELECT (4 / 3); SQL assumes that if you divide an integer by an integer, you want to get an integer back. Add decimal places to your numbers. For example, SELECT (4.0 / 3.0) AS result; gives the result expected: 1.333. It’s AS simple AS aliasing SQL allows aliasing. Aliasing simply means you assign a temporary name to something. To alias, use the AS keyword. For example, in the above example we could use aliases to make the result clearer: SELECT MAX(budget) AS max_budget, MAX(duration) AS max_duration FROM films; 10.4 Sorting and grouping ORDER BY In SQL, the ORDER BY keyword is used to sort results in ascending or descending order according to the values of one or more columns. By default ORDER BY will sort in ascending order. If you want to sort the results in descending order, you can use the DESC keyword. For example: SELECT title FROM films ORDER BY release_year DESC; Get the title and gross earnings for movies which begin with the letter ‘M’ and order the results alphabetically: SELECT title, gross FROM films WHERE title LIKE &#39;M%&#39; ORDER BY title; To order results in descending order, you can put the keyword DESC after ORDER BY. For example, to get all the names in the people table, in reverse alphabetical order: SELECT name FROM people ORDER BY name DESC; Sorting multiple columns ORDER BY can also be used to sort on multiple columns. It will sort by the first column specified, then sort by the next, then the next, and so on. For example: SELECT birthdate, name FROM people ORDER BY birthdate, name; Sorts on birth dates first (oldest to newest) and then sorts on the names in alphabetical order. The order of columns is important! GROUP BY In SQL, GROUP BY allows you to group a result by one or more columns, like so: SELECT sex, count(*) FROM employees GROUP BY sex; Note that you can combine GROUP BY with ORDER BY to group your results, calculate something about them, and then order your results. For example: SELECT sex, count(*) FROM employees GROUP BY sex ORDER BY count DESC; Get the release year, country, and highest budget spent making a film for each year, for each country. Sort your results by release year and country: SELECT release_year, country, MAX(budget) FROM films GROUP BY release_year, country ORDER BY release_year, country; HAVING a great time In SQL, aggregate functions can’t be used in WHERE clauses. For example, the following query is invalid: SELECT release_year FROM films GROUP BY release_year WHERE COUNT(title) &gt; 10; This means that if you want to filter based on the result of an aggregate function, you need another way! That’s where the HAVING clause comes in. For example: SELECT release_year FROM films GROUP BY release_year HAVING COUNT(title) &gt; 10; shows only those years in which more than 10 films were released. Get the country, average budget, and average gross take of countries that have made more than 10 films. Order the result by country name, and limit the number of results displayed to 5. You should alias the averages as avg_budget and avg_gross respectively: # select country, average budget, and average gross SELECT country, AVG(budget) AS avg_budget, AVG(gross) as avg_gross # from the films table FROM films # group by country GROUP BY country # where the country has more than 10 titles HAVING COUNT(title) &gt; 10 # order by country ORDER BY country # limit to only show 5 results LIMIT 5; "],["joining-data-in-sql.html", "11 Joining Data in SQL 11.1 Introduction to joins 11.2 Outer joins and cross joins 11.3 Set theory clauses 11.4 Subqueries", " 11 Joining Data in SQL https://learn.datacamp.com/courses/joining-data-in-postgresql 11.1 Introduction to joins Inner join SELECT * FROM left_table INNER JOIN right_table ON left_table.id = right_table.id; Instead of writing the full table name, you can use table aliasing as a shortcut. For tables you also use AS to add the alias immediately after the table name with a space. SELECT c1.name AS city, c2.name AS country FROM cities AS c1 INNER JOIN countries AS c2 ON c1.country_code = c2.code; Notice that to select a field in your query that appears in multiple tables, you’ll need to identify which table/table alias you’re referring to by using a . in your SELECT statement. The ability to combine multiple joins in a single query is a powerful feature of SQL: # Select fields SELECT c.code, name, region, e.year, fertility_rate, unemployment_rate # From countries (alias as c) FROM countries AS c # Join to populations (as p) INNER JOIN populations AS p # Match on country code ON c.code = p.country_code # Join to economies (as e) INNER JOIN economies AS e # Match on country code ON c.code = e.code; Inner join with USING When joining tables with a common field name, e.g. SELECT * FROM countries INNER JOIN economies ON countries.code = economies.code You can use USING as a shortcut: SELECT * FROM countries INNER JOIN economies USING(code) CASE WHEN and THEN Often it’s useful to look at a numerical field not as raw data, but instead as being in different categories or groups. You can use CASE with WHEN, THEN, ELSE, and END to define a new grouping field. SELECT name, continent, code, surface_area, # First case CASE WHEN surface_area &gt; 2000000 THEN &#39;large&#39; # Second case WHEN surface_area &gt; 350000 THEN &#39;medium&#39; # Else clause + end ELSE &#39;small&#39; END # Alias name AS geosize_group # From table FROM countries; 11.2 Outer joins and cross joins Left join # Select name, region, and gdp_percapita SELECT name, region, gdp_percapita # From countries (alias as c) FROM countries AS c # Left join with economies (alias as e) LEFT JOIN economies AS e # Match on code fields ON c.code = e.code # Focus on 2010 WHERE year = 2010; Right join Right joins aren’t as common as left joins. One reason why is that you can always write a right join as a left join. SELECT cities.name AS city, urbanarea_pop, countries.name AS country, indep_year, languages.name AS language, percent FROM languages RIGHT JOIN countries ON languages.code = countries.code RIGHT JOIN cities ON countries.code = cities.country_code ORDER BY city, language; Full join SELECT name AS country, code, region, basic_unit # From countries FROM countries # Join to currencies FULL JOIN currencies # Match on code USING (code) # Where region is North America or null WHERE region = &#39;North America&#39; OR region IS NULL # Order by region ORDER BY region; 11.3 Set theory clauses UNION UNION includes every record in both tables but doesn’t double count the matched/overlapped ones. # Select fields from 2010 table SELECT * # From 2010 table FROM economies2010 # Set theory clause UNION # Select fields from 2015 table SELECT * # From 2015 table FROM economies2015 # Order by code and year ORDER BY code, year; UNION ALL UNION ALL includes every record in both tables and does replicate the matched/overlapped ones. # Select fields SELECT code, year # From economies FROM economies # Set theory clause UNION ALL # Select fields SELECT country_code, year # From populations FROM populations # Order by code, year ORDER BY code, year; INTERSECT INTERSECT results in only those matched/overlapped. # Select fields SELECT code, year # From economies FROM economies # Set theory clause INTERSECT # Select fields SELECT country_code, year # From populations FROM populations # Order by code and year ORDER BY code, year; EXCEPT EXCEPT results in only those doesn’t matched/overlapped. # Select field SELECT name # From cities FROM cities #Set theory clause EXCEPT # Select field SELECT capital # From countries FROM countries # Order by result ORDER BY name; Diagnosing problems using anti-join Another powerful join in SQL is the anti-join. It is particularly useful in identifying which records are causing an incorrect number of records to appear in join queries. # 3. Select fields SELECT code, name # 4. From Countries FROM countries # 5. Where continent is Oceania WHERE continent = &#39;Oceania&#39; # 1. And code not in AND code NOT IN # 2. Subquery (SELECT code FROM currencies); 11.4 Subqueries Subquery inside WHERE # Select fields SELECT * # From populations FROM populations # Where life_expectancy is greater than WHERE life_expectancy &gt; # 1.15 * subquery 1.15 * (SELECT AVG(life_expectancy) FROM populations WHERE year = 2015) AND year = 2015; Subquery inside SELECT The code given in query.sql selects the top nine countries in terms of number of cities appearing in the cities table. Recall that this corresponds to the most populous cities in the world. SELECT countries.name AS country, COUNT(*) AS cities_num FROM cities INNER JOIN countries ON countries.code = cities.country_code GROUP BY country ORDER BY cities_num DESC, country LIMIT 9; convert the code to get the same result as the code shown: SELECT countries.name AS country, (SELECT COUNT(*) FROM cities WHERE countries.code = cities.country_code) AS cities_num FROM countries ORDER BY cities_num DESC, country LIMIT 9; Subquery inside FROM Begin by determining for each country code how many languages are listed in the languages table using SELECT, FROM, and GROUP BY. Alias the aggregated field as lang_num. -- Select fields (with aliases) SELECT code, COUNT(name) AS lang_num -- From languages FROM languages -- Group by code GROUP BY code; Include the previous query (aliased as subquery) as a subquery in the FROM clause of a new query. Select the local name of the country from countries. Also, select lang_num from subquery. Make sure to use WHERE appropriately to match code in countries and in subquery. Sort by lang_num in descending order. # Select fields SELECT local_name, subquery.lang_num # From countries FROM countries, # Subquery (alias as subquery) (SELECT code, COUNT(name) AS lang_num FROM languages GROUP BY code) AS subquery # Where codes match WHERE countries.code = subquery.code # Order by descending number of languages ORDER BY lang_num DESC; "],["web-scraping-in-r.html", "12 Web Scraping in R 12.1 Introduction to HTML and Web Scraping 12.2 Navigation and Selection with CSS 12.3 Advanced Selection with XPATH 12.4 Scraping Best Practices", " 12 Web Scraping in R https://learn.datacamp.com/courses/web-scraping-in-r 12.1 Introduction to HTML and Web Scraping Read in HTML A necessary package to read HTML is rvest: library(rvest) library(tidyverse) library(httr) library(xml2) Take the html_excerpt_raw variable and turn it into an HTML document that R understands using a function from the rvest package: html_excerpt_raw &lt;- &#39; &lt;html&gt; &lt;body&gt; &lt;h1&gt;Web scraping is cool&lt;/h1&gt; &lt;p&gt;It involves writing code – be it R or Python.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://datacamp.com&quot;&gt;DataCamp&lt;/a&gt; has courses on it.&lt;/p&gt; &lt;/body&gt; &lt;/html&gt;&#39; # Turn the raw excerpt into an HTML document R understands html_excerpt &lt;- read_html(html_excerpt_raw) html_excerpt ## {html_document} ## &lt;html&gt; ## [1] &lt;body&gt; \\n &lt;h1&gt;Web scraping is cool&lt;/h1&gt;\\n &lt;p&gt;It involves writing co ... Use the xml_structure() function to get a better overview of the tag hierarchy of the HTML excerpt: xml_structure(html_excerpt) ## &lt;html&gt; ## &lt;body&gt; ## {text} ## &lt;h1&gt; ## {text} ## {text} ## &lt;p&gt; ## {text} ## {text} ## &lt;p&gt; ## &lt;a [href]&gt; ## {text} ## {text} ## {text} read_html(url) : scrape HTML content from a given URL html_nodes(): identifies HTML wrappers. html_nodes(“.class”): calls node based on CSS class html_nodes(“#id”): calls node based on id html_nodes(xpath=”xpath”): calls node based on xpath (we’ll cover this later) html_table(): turns HTML tables into data frames html_text(): strips the HTML tags and extracts only the text 12.2 Navigation and Selection with CSS Select multiple HTML types CSS can be used to style a web page. In the most basic form, this happens via type selectors, where styles are defined for and applied to all HTML elements of a certain type. In turn, you can also use type selectors to scrape pages for specific HTML elements. CSS can also combine multiple type selectors via a comma, i.e. with html_nodes(\"type1, type2\"). This selects all elements that have type1 or type2. languages_raw_html &lt;- &#39;&lt;html&gt; &lt;body&gt; &lt;div&gt;Python is perfect for programming.&lt;/div&gt; &lt;p&gt;Still, R might be better suited for data analysis.&lt;/p&gt; &lt;small&gt;(And has prettier charts, too.)&lt;/small&gt; &lt;/body&gt; &lt;/html&gt;&#39; # Read in the HTML languages_html &lt;- read_html(languages_raw_html) # Select the div and p tags and print their text languages_html %&gt;% html_nodes(&#39;div, p&#39;) %&gt;% html_text() ## [1] &quot;Python is perfect for programming.&quot; ## [2] &quot;Still, R might be better suited for data analysis.&quot; CSS classes and IDs IDs should be unique across a web page. If you can make sure this is the case, it can reduce the complexity of your scraping selectors drastically. Here’s the structure of an HTML page you might encounter in the wild: structured_html &lt;- &quot;&lt;html&gt; &lt;body&gt; &lt;div id = &#39;first&#39;&gt; &lt;h1 class = &#39;big&#39;&gt;Joe Biden&lt;/h1&gt; &lt;p class = &#39;first blue&#39;&gt;Democrat&lt;/p&gt; &lt;p class = &#39;second blue&#39;&gt;Male&lt;/p&gt; &lt;/div&gt; &lt;div id = &#39;third&#39;&gt; &lt;h1 class = &#39;big&#39;&gt;Donald Trump&lt;/h1&gt; &lt;p class = &#39;first red&#39;&gt;Republican&lt;/p&gt; &lt;p class = &#39;second red&#39;&gt;Male&lt;/p&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt;&quot; structured_html &lt;- read_html(structured_html) Using html_nodes(), find the shortest possible selector to select the first div in structured_html: # Select the first div structured_html %&gt;% html_nodes(&#39;#first&#39;) ## {xml_nodeset (1)} ## [1] &lt;div id=&quot;first&quot;&gt;\\n &lt;h1 class=&quot;big&quot;&gt;Joe Biden&lt;/h1&gt;\\n &lt;p class=&quot;f ... Select the last child with a pseudo-class In the following HTML showing the author of a text in the last paragraph, there are two groups of p nodes: nested_html &lt;- &quot;&lt;html&gt; &lt;body&gt; &lt;div&gt; &lt;p class = &#39;text&#39;&gt;A sophisticated text [...]&lt;/p&gt; &lt;p class = &#39;text&#39;&gt;Another paragraph following [...]&lt;/p&gt; &lt;p class = &#39;text&#39;&gt;Author: T.G.&lt;/p&gt; &lt;/div&gt; &lt;p&gt;Copyright: DC&lt;/p&gt; &lt;/body&gt; &lt;/html&gt;&quot; nested_html &lt;- read_html(nested_html) use the pseudo-class that selects the last child to scrape the last p in each group. # Select the last child of each p group nested_html %&gt;% html_nodes(&#39;p:last-child&#39;) ## {xml_nodeset (2)} ## [1] &lt;p class=&quot;text&quot;&gt;Author: T.G.&lt;/p&gt; ## [2] &lt;p&gt;Copyright: DC&lt;/p&gt; As this selected the last p node from both groups, make use of the text class to get only the authorship information. # This time for real: Select only the last node of the p&#39;s wrapped by the div nested_html %&gt;% html_nodes(&#39;p.text:last-child&#39;) ## {xml_nodeset (1)} ## [1] &lt;p class=&quot;text&quot;&gt;Author: T.G.&lt;/p&gt; Select direct descendants with the child combinator There are cases where selectors like type, class, or ID won’t work, for example, if you only want to extract direct descendants of the top ul element. For that, you will use the child combinator (&gt;). Here, your goal is to scrape a list of all mentioned computer languages, but without the accompanying information in the sub-bullets: languages_html_2 &lt;- &quot;&lt;html&gt; &lt;ul id = &#39;languages&#39;&gt; &lt;li&gt;SQL&lt;/li&gt; &lt;ul&gt; &lt;li&gt;Databases&lt;/li&gt; &lt;li&gt;Query Language&lt;/li&gt; &lt;/ul&gt; &lt;li&gt;R&lt;/li&gt; &lt;ul&gt; &lt;li&gt;Collection&lt;/li&gt; &lt;li&gt;Analysis&lt;/li&gt; &lt;li&gt;Visualization&lt;/li&gt; &lt;/ul&gt; &lt;li&gt;Python&lt;/li&gt; &lt;/ul&gt; &lt;/html&gt;&quot; languages_html_2 &lt;- read_html(languages_html_2) First, gather all the li elements in the nested list shown above and print their text: # Extract the text of all list elements languages_html_2 %&gt;% html_nodes(&#39;li&#39;) %&gt;% html_text() ## [1] &quot;SQL&quot; &quot;Databases&quot; &quot;Query Language&quot; &quot;R&quot; ## [5] &quot;Collection&quot; &quot;Analysis&quot; &quot;Visualization&quot; &quot;Python&quot; Extract only direct descendants of the top-level ul element, using the child combinator: # Extract only the text of the computer languages (without the sub lists) languages_html_2 %&gt;% html_nodes(&#39;ul#languages &gt; li&#39;) %&gt;% html_text() ## [1] &quot;SQL&quot; &quot;R&quot; &quot;Python&quot; Not every sibling is the same The following HTML code contains two headings followed by some code and span tags: code_html &lt;- &quot;&lt;html&gt; &lt;body&gt; &lt;h2 class = &#39;first&#39;&gt;First example:&lt;/h2&gt; &lt;code&gt;some = code(2)&lt;/code&gt; &lt;span&gt;will compile to...&lt;/span&gt; &lt;code&gt;some = more_code()&lt;/code&gt; &lt;h2 class = &#39;second&#39;&gt;Second example:&lt;/h2&gt; &lt;code&gt;another = code(3)&lt;/code&gt; &lt;span&gt;will compile to...&lt;/span&gt; &lt;code&gt;another = more_code()&lt;/code&gt; &lt;/body&gt; &lt;/html&gt;&quot; code_html &lt;- read_html(code_html) Select the first code element in the second example using html_nodes() with the correct sibling combinator. # Select only the first code element in the second example code_html %&gt;% html_nodes(&#39;h2.second + code&#39;) ## {xml_nodeset (1)} ## [1] &lt;code&gt;another = code(3)&lt;/code&gt; Now select all code elements that are in the second example using another type of sibling combinator. # Select all code elements in the second example code_html %&gt;% html_nodes(&#39;h2.second ~ code&#39;) ## {xml_nodeset (2)} ## [1] &lt;code&gt;another = code(3)&lt;/code&gt; ## [2] &lt;code&gt;another = more_code()&lt;/code&gt; 12.3 Advanced Selection with XPATH Select by class and ID with XPATH weather_html &lt;- &quot; &lt;html&gt; &lt;body&gt; &lt;div id = &#39;first&#39;&gt; &lt;h1 class = &#39;big&#39;&gt;Berlin Weather Station&lt;/h1&gt; &lt;p class = &#39;first&#39;&gt;Temperature: 20°C&lt;/p&gt; &lt;p class = &#39;second&#39;&gt;Humidity: 45%&lt;/p&gt; &lt;/div&gt; &lt;div id = &#39;second&#39;&gt;...&lt;/div&gt; &lt;div id = &#39;third&#39;&gt; &lt;p class = &#39;first&#39;&gt;Sunshine: 5hrs&lt;/p&gt; &lt;p class = &#39;second&#39;&gt;Precipitation: 0mm&lt;/p&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt;&quot; weather_html &lt;- read_html(weather_html) Start by selecting all p tags in the above HTML using XPATH. # Select all p elements weather_html %&gt;% html_nodes(xpath = &#39;//p&#39;) ## {xml_nodeset (4)} ## [1] &lt;p class=&quot;first&quot;&gt;Temperature: 20°C&lt;/p&gt; ## [2] &lt;p class=&quot;second&quot;&gt;Humidity: 45%&lt;/p&gt; ## [3] &lt;p class=&quot;first&quot;&gt;Sunshine: 5hrs&lt;/p&gt; ## [4] &lt;p class=&quot;second&quot;&gt;Precipitation: 0mm&lt;/p&gt; Now select only the p elements with class second. The corresponding CSS selector would be .second, so here you need to use a [@class = ...] predicate applied to all p tags. # Select p elements with the second class weather_html %&gt;% html_nodes(xpath = &#39;//p[@class = &quot;second&quot;]&#39;) ## {xml_nodeset (2)} ## [1] &lt;p class=&quot;second&quot;&gt;Humidity: 45%&lt;/p&gt; ## [2] &lt;p class=&quot;second&quot;&gt;Precipitation: 0mm&lt;/p&gt; Now select all p elements that are children of the element with ID third. The corresponding CSS selector would be #third &gt; p – don’t forget the universal selector (*) before the @id = ... predicate and remember that children are selected with a /, not a //. # Select p elements that are children of &quot;#third&quot; weather_html %&gt;% html_nodes(xpath = &#39;//*[@id = &quot;third&quot;]/p&#39;) ## {xml_nodeset (2)} ## [1] &lt;p class=&quot;first&quot;&gt;Sunshine: 5hrs&lt;/p&gt; ## [2] &lt;p class=&quot;second&quot;&gt;Precipitation: 0mm&lt;/p&gt; Now select only the p element with class second that is a direct child of #third, again using XPATH. Here, you need to append to the XPATH from the previous step the @class predicate you used in the second step. # Select p elements with class &quot;second&quot; that are children of &quot;#third&quot; weather_html %&gt;% html_nodes(xpath = &#39;//*[@id = &quot;third&quot;]/p[@class = &quot;second&quot;]&#39;) ## {xml_nodeset (1)} ## [1] &lt;p class=&quot;second&quot;&gt;Precipitation: 0mm&lt;/p&gt; Use predicates to select nodes based on their children Here’s almost the same HTML as before. In addition, the third div has a p child with a third class. weather_html_2 &lt;- &quot;&lt;html&gt; &lt;body&gt; &lt;div id = &#39;first&#39;&gt; &lt;h1 class = &#39;big&#39;&gt;Berlin Weather Station&lt;/h1&gt; &lt;p class = &#39;first&#39;&gt;Temperature: 20°C&lt;/p&gt; &lt;p class = &#39;second&#39;&gt;Humidity: 45%&lt;/p&gt; &lt;/div&gt; &lt;div id = &#39;second&#39;&gt;...&lt;/div&gt; &lt;div id = &#39;third&#39;&gt; &lt;p class = &#39;first&#39;&gt;Sunshine: 5hrs&lt;/p&gt; &lt;p class = &#39;second&#39;&gt;Precipitation: 0mm&lt;/p&gt; &lt;p class = &#39;third&#39;&gt;Snowfall: 0mm&lt;/p&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt;&quot; weather_html_2 &lt;- read_html(weather_html_2) With XPATH, something that’s not possible with CSS can be done: selecting elements based on the properties of their descendants. For this, predicates may be used. Using XPATH, select all the div elements. # Select all divs weather_html_2 %&gt;% html_nodes(xpath = &#39;//div&#39;) ## {xml_nodeset (3)} ## [1] &lt;div id=&quot;first&quot;&gt;\\n &lt;h1 class=&quot;big&quot;&gt;Berlin Weather Station&lt;/h1&gt;\\n ... ## [2] &lt;div id=&quot;second&quot;&gt;...&lt;/div&gt; ## [3] &lt;div id=&quot;third&quot;&gt;\\n &lt;p class=&quot;first&quot;&gt;Sunshine: 5hrs&lt;/p&gt;\\n &lt;p cla ... Now select all divs with p descendants using the predicate notation. # Select all divs with p descendants weather_html_2 %&gt;% html_nodes(xpath = &#39;//div[p]&#39;) ## {xml_nodeset (2)} ## [1] &lt;div id=&quot;first&quot;&gt;\\n &lt;h1 class=&quot;big&quot;&gt;Berlin Weather Station&lt;/h1&gt;\\n ... ## [2] &lt;div id=&quot;third&quot;&gt;\\n &lt;p class=&quot;first&quot;&gt;Sunshine: 5hrs&lt;/p&gt;\\n &lt;p cla ... Now select divs with p descendants which have the third class. # Select all divs with p descendants having the &quot;third&quot; class weather_html_2 %&gt;% html_nodes(xpath = &#39;//div[p[@class = &quot;third&quot;]]&#39;) ## {xml_nodeset (1)} ## [1] &lt;div id=&quot;third&quot;&gt;\\n &lt;p class=&quot;first&quot;&gt;Sunshine: 5hrs&lt;/p&gt;\\n &lt;p cla ... Get to know the position() function position() function is very powerful when used within a predicate. Together with operators, you can basically select any node from those that match a certain path. You’ll try this out with the following HTML excerpt that is available to you via rules_html. Let’s assume this is a continuously updated website that displays certain Coronavirus rules for a given day and the day after. rules_html &lt;- &quot;&lt;html&gt; &lt;div&gt; &lt;h2&gt;Today&#39;s rules&lt;/h2&gt; &lt;p&gt;Wear a mask&lt;/p&gt; &lt;p&gt;Wash your hands&lt;/p&gt; &lt;/div&gt; &lt;div&gt; &lt;h2&gt;Tomorrow&#39;s rules&lt;/h2&gt; &lt;p&gt;Wear a mask&lt;/p&gt; &lt;p&gt;Wash your hands&lt;/p&gt; &lt;small&gt;Bring hand sanitizer with you&lt;/small&gt; &lt;/div&gt; &lt;/html&gt;&quot; rules_html &lt;- read_html(rules_html) Extract the text of the second p in every div using XPATH. # Select the text of the second p in every div rules_html %&gt;% html_nodes(xpath = &#39;//div/p[position() = 2]&#39;) %&gt;% html_text() ## [1] &quot;Wash your hands&quot; &quot;Wash your hands&quot; Now extract the text of every p (except the second) in every div. # Select every p except the second from every div rules_html %&gt;% html_nodes(xpath = &#39;//div/p[position() != 2]&#39;) %&gt;% html_text() ## [1] &quot;Wear a mask&quot; &quot;Wear a mask&quot; Extract the text of the last three children of the second div. Only use the &gt;= operator for selecting these nodes. # Select the text of the last three nodes of the second div rules_html %&gt;% html_nodes(xpath = &#39;//div[position() = 2]/*[position() &gt;= 2]&#39;) %&gt;% html_text() ## [1] &quot;Wear a mask&quot; &quot;Wash your hands&quot; ## [3] &quot;Bring hand sanitizer with you&quot; Extract nodes based on the number of their children XPATH count() function can be used within a predicate to narrow down a selection to these nodes that match a certain children count. This is especially helpful if your scraper depends on some nodes having a minimum amount of children. You’re only interested in divs that have exactly one h2 header and at least two paragraphs. Select the desired divs with the appropriate XPATH selector, making use of the count() function. # Select only divs with one header and at least two paragraphs rules_html %&gt;% html_nodes(xpath = &#39;//div[count(h2) = 1 and count(p) &gt; 1]&#39;) ## {xml_nodeset (2)} ## [1] &lt;div&gt;\\n &lt;h2&gt;Today&#39;s rules&lt;/h2&gt;\\n &lt;p&gt;Wear a mask&lt;/p&gt;\\n &lt;p&gt;Wash your han ... ## [2] &lt;div&gt;\\n &lt;h2&gt;Tomorrow&#39;s rules&lt;/h2&gt;\\n &lt;p&gt;Wear a mask&lt;/p&gt;\\n &lt;p&gt;Wash your ... Select directly from a parent element with XPATH’s text() extract the function information in parentheses into their own column, so you are required to extract a data frame with not two, but three columns: actors, roles, and functions. roles_html &lt;- &quot;&lt;html&gt; &lt;table&gt; &lt;tr&gt; &lt;th&gt;Actor&lt;/th&gt; &lt;th&gt;Role&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td class = &#39;actor&#39;&gt;Jayden Carpenter&lt;/td&gt; &lt;td class = &#39;role&#39;&gt;&lt;em&gt;Mickey Mouse&lt;/em&gt; (Voice)&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/html&gt;&quot; roles_html &lt;- read_html(roles_html) Extract the actors and roles from the table using XPATH. # Extract the actors in the cells having class &quot;actor&quot; actors &lt;- roles_html %&gt;% html_nodes(xpath = &#39;//table//td[@class = &quot;actor&quot;]&#39;) %&gt;% html_text() actors ## [1] &quot;Jayden Carpenter&quot; # Extract the roles in the cells having class &quot;role&quot; roles &lt;- roles_html %&gt;% html_nodes(xpath = &#39;//table//td[@class = &quot;role&quot;]/em&#39;) %&gt;% html_text() roles ## [1] &quot;Mickey Mouse&quot; Then, extract the function using the XPATH text() function. Extract only the text with the parentheses, which is contained within the same cell as the corresponding role, and trim leading spaces. # Extract the functions using the appropriate XPATH function functions &lt;- roles_html %&gt;% html_nodes(xpath = &#39;//table//td[@class = &quot;role&quot;]/text()&#39;) %&gt;% html_text(trim = TRUE) functions ## [1] &quot;(Voice)&quot; Combine extracted data into a data frame Combine the three vectors actors, roles, and functions into a data frame called cast (with columns Actor, Role and Function, respectively). # Create a new data frame from the extracted vectors cast &lt;- tibble( Actor = actors, Role = roles, Function = functions) cast ## # A tibble: 1 × 3 ## Actor Role Function ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Jayden Carpenter Mickey Mouse (Voice) 12.4 Scraping Best Practices *httr** read_html() actually issues an HTTP GET request if provided with a URL. The goal of this exercise is to replicate the same query without read_html(), but with httr methods instead. Use only httr functions to replicate the behavior of read_html(), including getting the response from Wikipedia and parsing the response object into an HTML document. Check the resulting HTTP status code with the appropriate httr function. # Get the HTML document from Wikipedia using httr wikipedia_response &lt;- GET(&#39;https://en.wikipedia.org/wiki/Varigotti&#39;) # Parse the response into an HTML doc wikipedia_page &lt;- content(wikipedia_response) # Check the status code of the response status_code(wikipedia_response) ## [1] 200 a fundamental part of the HTTP system are status codes: They tell you if everything is okay (200) or if there is a problem (404) with your request. It is good practice to always check the status code of a response before you start working with the downloaded page. For this, you can use the status_code() function from the httr() package. Add a custom user agent There are two ways of customizing your user agent when using httr for fetching web resources: Locally, i.e. as an argument to the current request method. Globally via set_config(). Send a GET request to https://httpbin.org/user-agent with a custom user agent that says \"A request from a DataCamp course on scraping\" and print the response. In this step, set the user agent locally. # Pass a custom user agent to a GET query to the mentioned URL response &lt;- GET(&#39;https://httpbin.org/user-agent&#39;, user_agent(&quot;A request from a DataCamp course on scraping&quot;)) # Print the response content content(response) ## $`user-agent` ## [1] &quot;A request from a DataCamp course on scraping&quot; Now, make that custom user agent (\"A request from a Alec at LU\") globally available across all future requests with set_config(). # Globally set the user agent to &quot;A request from a DataCamp course on scraping&quot; set_config(add_headers(`User-Agent` = &quot;A request from a Alec at LU&quot;)) # Pass a custom user agent to a GET query to the mentioned URL response &lt;- GET(&#39;https://httpbin.org/user-agent&#39;) # Print the response content content(response) ## $`user-agent` ## [1] &quot;A request from a Alec at LU&quot; Apply throttling to a multi-page crawler You’ll find the name of the peak within an element with the ID \"firstHeading\", while the coordinates are inside an element with class \"geo-dms\", which is a descendant of an element with ID \"coordinates\". Construct a read_html() function that executes with a delay of a half second when executed in a loop. mountain_wiki_pages &lt;- c(&quot;https://en.wikipedia.org/w/index.php?title=Mount_Everest&amp;oldid=958643874&quot;, &quot;https://en.wikipedia.org/w/index.php?title=K2&amp;oldid=956671989&quot;, &quot;https://en.wikipedia.org/w/index.php?title=Kangchenjunga&amp;oldid=957008408&quot;) # Define a throttled read_html() function with a delay of 0.5s read_html_delayed &lt;- slowly(read_html, rate = rate_delay(0.5)) Now write a for loop that goes over every page URL in the prepared variable mountain_wiki_pages and stores the HTML available at the corresponding Wikipedia URL into the html variable # Construct a loop that goes over all page urls for(page_url in mountain_wiki_pages){ # Read in the html of each URL with a delay of 0.5s html &lt;- read_html_delayed(page_url) } Finally, extract the name of the peak as well as its coordinates using the correct CSS selectors given above and store it in peak and coords. # Extract the name of the peak and its coordinates peak &lt;- html %&gt;% html_node(&quot;#firstHeading&quot;) %&gt;% html_text() coords &lt;- html %&gt;% html_node(&quot;#coordinates .geo-dms&quot;) %&gt;% html_text() print(paste(peak, coords, sep = &quot;: &quot;)) } Merge all the code chunks above to make it functional: # Define a throttled read_html() function with a delay of 0.5s read_html_delayed &lt;- slowly(read_html, rate = rate_delay(0.5)) # Construct a loop that goes over all page urls for(page_url in mountain_wiki_pages){ # Read in the html of each URL with a delay of 0.5s html &lt;- read_html_delayed(page_url) # Extract the name of the peak and its coordinates peak &lt;- html %&gt;% html_node(&quot;#firstHeading&quot;) %&gt;% html_text() coords &lt;- html %&gt;% html_node(&quot;#coordinates .geo-dms&quot;) %&gt;% html_text() print(paste(peak, coords, sep = &quot;: &quot;)) } ## [1] &quot;Mount Everest: 27°59′17″N 86°55′31″E&quot; ## [1] &quot;K2: 35°52′57″N 76°30′48″E&quot; ## [1] &quot;Kangchenjunga: 27°42′09″N 88°08′48″E&quot; "],["ch-2---slr.html", "13 Ch 2 - SLR 13.1 Notes 13.2 Example 2.3: CEO Salary and Return on Equity 13.3 Example 2.4: Wage and Education 13.4 Example 2.5: Voting Outcomes and Campaign Expenditures 13.5 Example of Fitted Values (\\(\\hat{y}\\))", " 13 Ch 2 - SLR For this chapter, you should do the simple linear regressions from the examples. For each, you should make sure you get the same results as in the example and understand the discussion of the example in the textbook. For this chapter, you should also use ggplot to create a scatter plot of the x and y variables used in the regression, and include a linear regression line. So I suggest doing this chapter after you complete the DataCamp course and your notes on Intro to Data Visualization with ggplot2. These are the packages you will need: library(wooldridge) library(ggplot2) library(pander) For displaying the results of a single regression, you should use the pander package as shown in example 2.3 below. (Later in 380 you will be comparing several regression models side-by-side and will be using the stargazer package instead, but for a single regression model this works well). I completed the first example (Example 2.3: CEO Salary and Return on Equity) for you to demonstrate what you need to do. For the subsequent examples you should fill in the code yourself. I’ve provided the template for you and included the code to load the data. I then added a comment that says “YOUR CODE GOES HERE” wherever you are supposed to add code. You should use the example I did for you as a guide. What you do later in 380 will not be as simple as copy/pasting code I give you and changing a few variable names, for for this chapter as you’re first learning what to do, it’s exactly that easy. Don’t overthink what you’re being asked to do. That said, you are of course welcome to add more if it will help you. You can add more regressions. You can explore the changes of units I talk about in LN2.7. You can experiment with log transformations shown in the later parts of chapter 2 (we’ll talk about these later with chapter 3). But the only things you’re required to do for the BP are the 2 examples I left for you below. 13.1 Notes Optionally, you can add notes here on chapter 2’s (and LN2’s) content. 13.2 Example 2.3: CEO Salary and Return on Equity Load the data from wooldridge package, estimate the regression, and display the results # Load data data(ceosal1) # Estimate regression model ex2.3 &lt;- lm(salary ~ roe, data=ceosal1) # Display model results pander(summary(ex2.3))   Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 963.2 213.2 4.517 1.053e-05 roe 18.5 11.12 1.663 0.09777 Fitting linear model: salary ~ roe Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 209 1367 0.01319 0.008421 Display a scatter plot with regression line corresponding to this model ggplot(data=ceosal1, aes(x=roe, y=salary)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se=FALSE) ## `geom_smooth()` using formula &#39;y ~ x&#39; 13.3 Example 2.4: Wage and Education Load the data from wooldridge package, estimate the regression, and display the results # Load data data(wage1) # Estimate regression model ex2.4 &lt;- lm(wage ~ educ, data=wage1) # Display model results pander(summary(ex2.4))   Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -0.9049 0.685 -1.321 0.1871 educ 0.5414 0.05325 10.17 2.783e-22 Fitting linear model: wage ~ educ Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 526 3.378 0.1648 0.1632 Display a scatter plot with regression line corresponding to this model ggplot(data=wage1, aes(x=educ, y=wage)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se=FALSE) ## `geom_smooth()` using formula &#39;y ~ x&#39; 13.4 Example 2.5: Voting Outcomes and Campaign Expenditures Load the data from wooldridge package, estimate the regression, and display the results # Load data data(vote1) # Estimate regression model ex2.5 &lt;- lm(voteA ~ shareA, data=vote1) # Display model results pander(summary(ex2.5))   Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 26.81 0.8872 30.22 1.729e-70 shareA 0.4638 0.01454 31.9 6.634e-74 Fitting linear model: voteA ~ shareA Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 173 6.385 0.8561 0.8553 Display a scatter plot with regression line corresponding to this model ggplot(data=vote1, aes(x=shareA, y=voteA)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se=FALSE) ## `geom_smooth()` using formula &#39;y ~ x&#39; 13.5 Example of Fitted Values (\\(\\hat{y}\\)) This example builds off of Example 2.3: CEO Salary and Return on Equity above. You don’t need to do anything for these, but you should look at them to make sure you understand them. First, you should understand how to calculate the OLS fitted values, the \\(\\hat{y}\\) values. Below you’ll see two ways to do so. The first is to manually use the OLS regression equation: \\[ \\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i \\] After estimating a model, you can use the coef() function to get the values of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\). We estimated the model above and stored it into the variable ex2.3. We can access \\(\\hat{\\beta}_0\\) with coef(ex2.3)[\"(Intercept)\"] or coef(ex2.3)[1]. We can access the first (and only) \\(x\\) variable is named roe, and \\(\\hat{\\beta}_1\\) is coef(ex2.3)[\"roe\"] or coef(ex2.3)[1]. ceosal1$salaryHat &lt;- coef(ex2.3)[&quot;(Intercept)&quot;] + coef(ex2.3)[&quot;roe&quot;] * ceosal1$roe The second way is by using the fitted() function. ceosal1$salaryHat_fitted &lt;- fitted(ex2.3) You can check that these two are the same by subtracting them and making sure they are all the same. ceosal1$dif &lt;- ceosal1$salaryHat - ceosal1$salaryHat_fitted summary(ceosal1$dif) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -2.046e-12 0.000e+00 0.000e+00 2.557e-14 2.274e-13 6.821e-13 Note that the min is -0.000000000002046363 and the max is 0.000000000000682121. Algebra with decimal numbers often results in small rounding errors, which is why these are 0 exactly, but they are effectively 0, demonstrating that the two methods of calculating the fitted \\((\\hat{y})\\) values are the same. The second reason I’m including this example is to demonstrate plotting the fitted \\((\\hat{y})\\) values. The black circles are the data. The red x’s are the predicted \\((\\hat{y})\\) values (in this setting, “predicted values” and “fitted” values and \\((\\hat{y})\\) all mean the same thing). The blue line is the OLS regression line. You should understand why all of the red x’s are on the blue line. ggplot(data=ceosal1, aes(x=roe, y=salary)) + geom_point(aes(color=&quot;data&quot;,shape=&quot;data&quot;)) + geom_point(aes(x=roe, y=salaryHat,color=&quot;predicted (yHat)&quot;,shape=&quot;predicted (yHat)&quot;)) + geom_smooth(method = &quot;lm&quot;,color=&quot;blue&quot;, se=FALSE,aes(linetype=&quot;OLS&quot;)) + scale_color_manual(values = c(&quot;black&quot;, &quot;red&quot;),name=&quot;Values&quot;) + scale_shape_manual(values = c(1,4),name=&quot;Values&quot;) + scale_linetype_manual(values = c(&quot;solid&quot;,&quot;solid&quot;), name=&quot;Regression Line&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; "],["ch-3---mlr.html", "14 Ch 3 - MLR 14.1 Data 14.2 Regression model comparisons 14.3 Adjusted R-Squared 14.4 Multicollinearity 14.5 Testing linear combinations of parameters 14.6 A note on presentation 14.7 Log transformations", " 14 Ch 3 - MLR This chapter walks through a few basics of working with MLR models. The last section at the bottom, Log transformations, asks you to create a few graphs, estimate a few models, and formally interpret a few coefficients. You’ll need to make sure you have the following packages installed: library(tidyverse) library(stargazer) library(multcomp) library(pander) library(car) library(ggplot2) 14.1 Data .variableTable th { text-align: left; color: orange; } The dataset ceo.csv has the following variables: Variable Description salary 1990 compensation in dollars ($) age age of CEO in years comten years CEO with company ceoten years as ceo with company sales 1990 firm sales in $millions profits 1990 profits in $millions mktval 1990 market value in $millions Note that the table above has custom formatting applied by adding an HTML class name “variableTable” (via a “div” element that surrounds the table), formatting the table header (th) to be left-aligned and have an orange font so you can easily see what was affected. .sumstats td { text-align: right; color: green; } 14.1.1 Summary Statistics The stargazer package provides an easy way to display summary statistics. You need the result='asis' code chunk option so that it displays the html table in the knit output file (and you need the type=\"html\" stargazer option to produce the html table). The warning=FALSE and message=FALSE keep it from displaying some of the messages it displays by default. mydata &lt;- read.csv(&quot;https://raw.githubusercontent.com/econ380w21/380data/main/ceosal.csv&quot;) stargazer(mydata, type = &quot;html&quot;,summary.stat = c(&quot;mean&quot;,&quot;sd&quot;, &quot;min&quot;, &quot;p25&quot;, &quot;median&quot;, &quot;p75&quot;, &quot;max&quot;)) Statistic Mean St. Dev. Min Pctl(25) Median Pctl(75) Max salary 865,864.400 587,589.300 100,000 471,000 707,000 1,119,000 5,299,000 age 56.429 8.422 33 52 57 62 86 comten 22.503 12.295 2 12 23 33 58 ceoten 7.955 7.151 0 3 6 11 37 sales 3,529.463 6,088.654 29 561 1,400 3,500 51,300 profits 207.831 404.454 -463 34 63 208 2,700 mktval 3,600.316 6,442.276 387 644 1,200 3,500 45,400 In addition to providing you with an example of how to display a summary statistics table, this section also provides an example of how you can control aspects of how the table displays using HTML/CSS. In bookdown, the horizontal space is already modified for you, but if you use this in a regular RMD file that you knit, there will be no spacing at all between cells (so the numbers bump right up next to each other). To fix this, you can add padding (e.g., padding-left, padding-right). Often you want to right-align summary statistics. To do this, I’ve added an HTML class name “sumstats” to this section by including .sumstats in curly brackets in the section header (the part started with ##). I included CSS styling to align the text right. I also make the font green so that you could easily see what was modified. Beyond simply formatting a table, this example, and the one above that uses a “div” and class name, are the two main ways you can control the formatting of specific parts of your book (without affecting the entire book). Now back to econometrics… 14.2 Regression model comparisons Consider four models: model1 &lt;- lm(salary~sales+mktval+profits,data=mydata) model2 &lt;- lm(salary~sales+mktval+profits+age,data=mydata) model3 &lt;- lm(salary~sales+mktval+profits+ceoten,data=mydata) model4 &lt;- lm(salary~sales+mktval+profits+ceoten+age,data=mydata) The package you should use for 380 to report the results of multiple models side-by-side is the stargazer package. Each column is a separate model. If a variable does not have a coefficient displayed in a column, that means it was not included as an explanatory variable in that model. stargazer(model1, model2, model3, model4, type = &quot;html&quot;, report=(&#39;vc*p&#39;), keep.stat = c(&quot;n&quot;,&quot;rsq&quot;,&quot;adj.rsq&quot;), notes = &quot;&lt;em&gt;&amp;#42;p&amp;lt;0.1;&amp;#42;&amp;#42;p&amp;lt;0.05;&amp;#42;&amp;#42;&amp;#42;p&amp;lt;0.01&lt;/em&gt;&quot;, notes.append = FALSE) Dependent variable: salary (1) (2) (3) (4) sales 15.984 15.369 18.233* 18.061 p = 0.152 p = 0.169 p = 0.100 p = 0.106 mktval 23.831 23.842 21.157 21.231 p = 0.137 p = 0.137 p = 0.183 p = 0.183 profits 31.703 28.100 48.636 47.528 p = 0.909 p = 0.920 p = 0.859 p = 0.863 age 4,528.026 823.966 p = 0.353 p = 0.873 ceoten 12,730.860** 12,390.430** p = 0.026 p = 0.042 Constant 717,062.400*** 464,424.700* 613,958.800*** 570,743.300** p = 0.000 p = 0.093 p = 0.000 p = 0.042 Observations 177 177 177 177 R2 0.178 0.182 0.201 0.202 Adjusted R2 0.163 0.163 0.183 0.178 Note: *p&lt;0.1;**p&lt;0.05;***p&lt;0.01 Just like for the summary statistics table, you need the result='asis' code chunk option so that it displays the html table in the knit output file (and you need the type=\"html\" stargazer option to produce the html table), and you want the warning=FALSE and message=FALSE code chunk options to keep it from displaying some of the messages it displays by default. For displaying regression results for 380, please use the report=('vc*p') stargazer option to display p-values and the keep.stat = c(\"n\",\"rsq\",\"adj.rsq\") stargazer option to display the sample size (it’s important to know the sample size for each model so you can see if it changes, which happens if some variables have missing values), r-squared, and adjusted r-squared. The final two options (notes and notes.append) fix a bug in stargazer. The table is supposed to display a note that explains the stars (asterisks) used to denote statistical significance (one star means statistically significant at the 10% level, two stars mean statistically significant at the 5% level, and 3 stars mean statistically significant at the 1% level). However, asterisks are also how you make words italic and bold in RMarkdown, and that’s what happens in the table footnote (try removing those options and building to see what I mean). The solution is to use the character code 42 (*) instead. So, just use the options above and it should work correctly.1 This option also highlights why you often want to have easily-accessible examples. You’re probably not going to remember all that to type it yourself, but if you have this chapter in your BP, you can easily search for stargazer, find this example, and copy/paste into your project or wherever you want to report results. 14.3 Adjusted R-Squared Recall the definition of \\(R^2\\) from “LN3.6 (R-Squared, Goodness of Fit).pptx” (posted in Moodle): \\[ R^2 = \\frac{SSM}{SST} = 1 - \\frac{SSR}{SST} \\] The denominator measures the total variation in the \\(y\\) variable: \\(SST = (n-1)\\text{VAR}(y)\\) (where \\(\\text{VAR}(y)\\) is the variance of \\(y\\)); thus, \\(SST\\) has nothing to do with the explanatory variables (it’s calculated only using \\(y\\)). Consequently, adding additional \\(x\\) variables to the model does not affect \\(SST\\). Adding an additional \\(x\\) variable cannot decrease how much of the variation in \\(y\\) is explained by the model, so \\(SSM\\) will not decrease. Usually it increases at least a little bit. Thus, adding an additional \\(x\\) variable cannot decrease \\(R^2\\), and it usually increases it at least a little bit. This means that \\(R^2\\) increasing is a not a good justification for adding an additional \\(x\\) variable to the model. Adjusted \\(R^2\\) penalizes you for adding an additional \\(x\\) variable. Adjusted \\(R^2\\) only increases if the new variable has a sufficiently significant effect on \\(y\\). Adjusted \\(R^2\\) is defined as \\[ \\text{Adjusted }\\bar{R}^2 = 1 - \\frac{\\left(\\frac{SSR}{n-k-1}\\right)}{\\left(\\frac{SST}{n-1}\\right)} \\] Look at the models above. All four models include measures of the company, including sales, market value, and profits. Models 2-4 add variables measuring characteristics of the CEO. Compare models 1 and 2. Adding the CEO’s age increases \\(R^2\\) (from 0.178 to 0.182) but adjusted \\(R^2\\) remains essentially the same (it goes down by 0.000625), indicating that adding age does not improve the model (you’ll also note that the p-value for the age coefficient is 0.352, meaning that the effect of the CEO’s age on salary is not statistically significant at any standard level of significance, providing further evidence that including age does not improve the model). Comparing models 1 and 3, both \\(R^2\\) and adjusted \\(R^2\\) increase when adding the CEO’s tenure (from 0.178 to 0.201 for \\(R^2\\) and from 0.163 to 0.183 for adjusted \\(R^2\\)). This increase in adjusted \\(R^2\\) corresponds with statistical significance of the coefficient; the p-value for the ceoten coefficient is 0.0251, meaning that the effect of CEO’s tenure on salary is statistically significant at the 5% level. The increase in adjusted \\(R^2\\) provides support for including this variable in the model (with the statistically significant coefficient providing additional support). Comparing models 3 and 4, we again see that adding the CEO’s age does not improve the model; \\(R^2\\) increases slightly (from 0.201 to 0.202) but adjusted \\(R^2\\) goes down (from 0.183 to 0.178). The coefficient on age is also not statistically significant at any standard level of significance (the p-value is 0.873,). These results indicate that age should be left out of the model. 14.4 Multicollinearity Sometimes two variables are both important in determining \\(y\\), but when both are included in the model, neither appears to have a significant effect. Consider the following four models. Note that the final column is model 3 from above (so I numbered the models 5, 6, 7, and 3). model5 &lt;- lm(salary~sales+ceoten,data=mydata) model6 &lt;- lm(salary~sales+ceoten+mktval,data=mydata) model7 &lt;- lm(salary~sales+ceoten+profits,data=mydata) model3 &lt;- lm(salary~sales+ceoten+mktval+profits,data=mydata) stargazer(model5, model6, model7, model3, type = &quot;html&quot;, report=(&#39;vc*p&#39;), keep.stat = c(&quot;n&quot;,&quot;rsq&quot;,&quot;adj.rsq&quot;), notes = &quot;&lt;em&gt;&amp;#42;p&amp;lt;0.1;&amp;#42;&amp;#42;p&amp;lt;0.05;&amp;#42;&amp;#42;&amp;#42;p&amp;lt;0.01&lt;/em&gt;&quot;, notes.append = FALSE, model.numbers = FALSE, column.labels = c(&quot;(5)&quot;,&quot;(6)&quot;, &quot;(7)&quot;, &quot;(3)&quot;)) Dependent variable: salary (5) (6) (7) (3) sales 37.801*** 19.019* 19.667* 18.233* p = 0.00000 p = 0.061 p = 0.076 p = 0.100 ceoten 13,925.610** 12,703.360** 13,296.830** 12,730.860** p = 0.016 p = 0.025 p = 0.020 p = 0.026 mktval 23.400** 21.157 p = 0.015 p = 0.183 profits 341.028** 48.636 p = 0.041 p = 0.859 Constant 621,670.900*** 613,436.100*** 619,800.500*** 613,958.800*** p = 0.000 p = 0.000 p = 0.000 p = 0.000 Observations 177 177 177 177 R2 0.173 0.201 0.193 0.201 Adjusted R2 0.164 0.187 0.179 0.183 Note: *p&lt;0.1;**p&lt;0.05;***p&lt;0.01 Following similar logic to our discussion above about using adjusted \\(R^2\\) and p-values to decide which variables to include in the model, we can make several conclusions about market value (mktval) and profits. Comparing models 5 and 6, adjusted \\(R^2\\) increases (from 0.164 to 0.187) and the coefficient on mktval is statistically significant at the 5% level (p-value of 0.0146). Thus, market value should be added to the model to join sales and CEO tenure in explaining CEO salary. Similarly, comparing models 5 and 7, adjusted \\(R^2\\) increases (from 0.164 to 0.179) and the coefficient on profits is statistically significant at the 5% level (p-value of 0.0402), indicating that profits should be added to the model to join sales and CEO tenure in explaining CEO salary. However, look at what happens when we include both market value (mktval) and profits. Comparing models 7 and 3, adjusted \\(R^2\\) goes up slightly (from 0.179 to 0.183), but comparing models 6 and 3, adjusted \\(R^2\\) goes down (from 0.187 to 0.183). And both mktval and profits are not statistically significant at any standard level of significance in model 3 (p-values are 0.1821 and 0.859, respectively). How can adding mktval improve model 5 and adding profits improve model 5, but adding both doesn’t? Often this happens when there is correlation between explanatory variables. Sometimes this happens when several explanatory variables essentially measure the same concept. If the variables are very similar measures of the same underlying concept, it often makes sense to include only one or the other (sometimes papers will include “robustness checks” that show results are similar if you use alternative measures). Sometimes, however, variables measure different concepts but are still highly correlated with each other. Correlation between explanatory variables is called multicollinearity. When we do a regression, we’re looking at variation in the explanatory variables and how that explains variation in the outcome. When two explanatory variables tend to move together (i.e., are correlated), it’s hard for OLS to tell which explanatory variable is driving the changes in the outcome. Pretend the index finger on your left hand represents \\(y\\) and the index finger on your right hand represents \\(x_1\\). Move your hands up and down somewhat together, as if demonstrating how \\(x_1\\) affects \\(y\\). Now also extend your pinky finger on your right hand to represent \\(x_4\\). Make the same hand movements. If we were watching your hand movements, we wouldn’t be able to tell if it was \\(x_1\\) or \\(x_4\\) affecting \\(y\\) because whenever \\(x_1\\) goes up, so does \\(x_4\\) (since both are on your right hand, you can tip your hand one way or the other so the correlation isn’t perfect, but they’re generally going to move pretty closely together). Formally, we can measure this relationship between explanatory variables with a regression. Here is the regression of profits on market value: mktvalAndProfits &lt;- lm(profits~mktval,data=mydata) pander(summary(mktvalAndProfits))   Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 0.3038 13.85 0.02194 0.9825 mktval 0.05764 0.001881 30.65 2.935e-72 Fitting linear model: profits ~ mktval Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 177 160.7 0.843 0.8421 The \\(R^2\\) is 0.843, meaning that 84.3% of the variation in profits can be explained by variation in market value (in a linear model). Because profits and market value tend to both be low or both be high at the same time, it is hard for OLS to tell to what extent it’s market value affecting salary, and to what extent it’s profits. 14.4.1 Variance Inflation Factor (VIF) The Variance Inflation Factor (VIF) is a common measure used to gauge how much multicollinearity might be affecting the precision with which we are able to measure a particular coefficient. The VIF is one part of the equation used to calculate the standard error of a coefficient.2 Consider the general regression equation: \\[ y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\cdots + \\beta_k x_{ki} + u_i \\] For variable \\(j\\), the VIF is \\[ \\text{VIF}_j = \\frac{1}{1-R^2_j} \\] where \\(R^2_j\\) is the \\(R^2\\) of the regression of \\(x_j\\) on the other explanatory variables (including an intercept). The variance of \\(\\hat{\\beta}_j\\) is \\[ \\text{VAR}(\\hat{\\beta}_j)=\\frac{\\sigma^2}{SST_j}\\text{VIF}_j \\] where \\(\\sigma^2\\) is a measure of the variance of the unobservables, given the explanatory variables, and \\(SST_j\\) is the total variation in \\(x_j\\). This equation makes a lot of intuitive sense. The more variable the unobservable factors that affect \\(y\\), the noisier (less precise) our estimates of how each explanatory variable affects \\(y\\); thus, \\(\\text{VAR}(\\hat{\\beta}_j)\\) is larger. The more variation there is in explanatory variable \\(x_j\\), the more information we have about how \\(x_j\\) relates to \\(y\\), and the more precise the estimate of \\(\\beta_j\\); thus, \\(\\text{VAR}(\\hat{\\beta}_j)\\) is smaller. However, if a lot of that variation in \\(x_j\\) can be explained by the other explanatory variables, the harder it is for OLS to tell which variable is affecting \\(y\\), and thus the less precise the estimate of \\(\\hat{\\beta}_j\\); thus, \\(\\text{VIF}_j\\) is higher, and so is \\(\\text{VAR}(\\hat{\\beta}_j)\\). In other words, to precisely estimate the effect of \\(x_j\\) on \\(y\\), we need independent variation in \\(x_j\\). Consider two extreme cases. First, if there is no variation in \\(x_j\\), then \\(SST_j=0\\) and \\(\\text{VAR}(\\hat{\\beta}_j)\\) is infinite. This situation is also a violation of MLR.3 that prohibits perfect collinearity (because if \\(x_j\\) is constant, it is perfectly collinear with the constant). Second, if all of the variation in \\(x_j\\) is perfectly explained by the other explanatory variables, then \\(R^2_j=1\\) and \\(\\text{VIF}_j\\) is infinite. This means \\(\\text{VAR}(\\hat{\\beta}_j)\\) is also infinite. This situation is also a violation of MLR.3; \\(R^2_j=1\\) means there is a perfect linear relationship between \\(x_j\\) and the other explanatory variables, exactly what is prohibited by MLR.3. At the other end if the spectrum is the best case for precisely estimating a coefficient \\(\\hat{\\beta}_j\\), which is when there is lots of variation in \\(x_j\\) (i.e., \\(SST_j\\) is large), and that variation is completely unrelated to variation in the other explanatory variables (i.e., \\(R^2_j=0\\) so \\(\\text{VIF}_j=1\\)). The only other term in \\(\\text{VAR}(\\hat{\\beta}_j)\\) we haven’t discussed is \\(\\sigma^2\\), the variation in the unobservables given the explanatory variables. The less unobservable factors are affecting \\(y\\), the more precisely we can estimate how our explanatory variables affect \\(y\\). In the extreme case, there are no unobservables affecting \\(y\\). Thus, \\(\\sigma^2=0\\), meaning \\(\\text{VAR}(\\hat{\\beta}_j)=0\\). In this situation, we can exactly estimate the effect of each explanatory variable on \\(y\\); hence, \\(\\text{VAR}(\\hat{\\beta}_j)=0\\). Let’s examine the VIF for our specific example. In the example above, we focused on just two variables, profits and market value, but the regression in question (model 3) also includes sales and CEO tenure. Thus, we want to know how much of the variation in market value can be explained by variation in the other explanatory variables. We find this via this regression: modelProfits &lt;- lm(profits~sales+ceoten+mktval,data=mydata) pander(summary(modelProfits))   Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -10.75 18.19 -0.5907 0.5555 sales 0.01615 0.002805 5.76 3.775e-08 ceoten -0.5654 1.567 -0.3608 0.7187 mktval 0.04612 0.002645 17.44 5.822e-40 Fitting linear model: profits ~ sales + ceoten + mktval Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 177 147.7 0.8689 0.8666 The \\(R^2\\) is 0.869, meaning that 86.9% of the variation in profits can be explained by variation in market value, sales, and CEO tenure (the other explanatory variables. This \\(R^2\\) value, denoted \\(R^2_{profits}\\), is the value used to calculate the VIF for \\(\\hat{\\beta}_{profits}\\): \\[ \\text{VIF}_{profits} = \\frac{1}{1-R^2_{profits}} = \\frac{1}{1-0.8689} = 7.63 \\] This is a somewhat high VIF, but not so high that we would necessarily expect a problem. Sometimes a VIF of 10 is used as a rule of thumb for a cutoff for how much multicollinearity is too much, but using 10 as a cutoff is arbitrary (so don’t rely on this too much). The following table gives you an idea of how \\(R^2_j\\) relates to \\(\\text{VIF}_j\\). .centerTable { margin-left: auto; margin-right: auto; } \\(R^2_j\\) \\(\\text{VIF}_j\\) 0 1 0.1 1.111 0.2 1.25 0.3 1.429 0.4 1.667 0.5 2 0.6 2.5 0.7 3.333 0.8 5 0.9 10 0.99 100 0.999 1000 The VIF value of 10 corresponds with \\(R^2_j=0.9\\). For \\(R^2_j\\) above 0.9, the VIF increases quite steeply. 14.4.2 Joint hypotheses test So should we only include one or the other? Not necessarily. Both market value and profits affect salary. If we leave one out, we’ll have omitted variable bias. Look at how the coefficient on profits changes between models 7 and 3: it falls by 292 (from 341 down to 48.63609). However, due to the multicollinearity in model 3, we get very imprecise estimates of mktval and profits. Multicollinearity often makes it difficult to estimate effects precisely, but if we don’t include both variables, we introduce omitted variable bias. A larger sample size might help,3 but that’s obviously not something we can change in most situations (since we’re usually using data other people have already collected). A common approach is to include several models for comparison, both with and without the variables (like the table above with models 5, 6, 7, and 3). Then, do a formal test of whether the two variables are jointly significant in determining \\(y\\). Formally, this is a hypothesis test of the joint null hypothesis that \\(\\beta_{mktval}=0\\) and \\(\\beta_{profits}=0\\). To perform this test, we can use the linearHypothesis function from the car package: jointTest &lt;- linearHypothesis(model3, c(&quot;mktval=0&quot;, &quot;profits=0&quot;)) jointTest ## Linear hypothesis test ## ## Hypothesis: ## mktval = 0 ## profits = 0 ## ## Model 1: restricted model ## Model 2: salary ~ sales + ceoten + mktval + profits ## ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 174 5.0244e+13 ## 2 172 4.8526e+13 2 1.7174e+12 3.0436 0.05024 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The p-value is 0.0502. The interpretation is the same as for other hypothesis tests. Here, we reject the null hypothesis at the 10% level. This provides evidence that market value and profits together are important in determining salary. You can see section 4-5a in Wooldridge 6e for more on joint hypothesis tests. This test can be beneficial when you have a situation where you have reason to believe two (or more) variables are important in determining \\(y\\), but each is individually insignificant. Take care not to abuse this test by testing a highly insignificant variable with a highly significant variable. Any joint hypothesis test that includes a variable that is individually highly significant (i.e., that has a p-value under 0.01) will usually be jointly significant as well; so trying to use this test as evidence that the insignificant variable is actually jointly significant is essentially cheating (i.e., mis-representing what the test is telling us). 14.5 Testing linear combinations of parameters It’s common for people to look at regression results and make comparisons between coefficients. For example, looking at models 6 and 3 above, the coefficient on market value is larger than the coefficient on sales. Does this mean market value is more important that sales in terms of its effect on salary? To see how we answer this question, let’s consider model 3 for the rest of this section, reprinted here for convenience: pander(summary(model3))   Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 613959 65486 9.375 3.95e-17 sales 18.23 11.01 1.656 0.09949 ceoten 12731 5636 2.259 0.02515 mktval 21.16 15.79 1.34 0.1821 profits 48.64 273.4 0.1779 0.859 Fitting linear model: salary ~ sales + ceoten + mktval + profits Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 177 531159 0.2014 0.1828 There’s two main things to keep in mind when attempting to compare coefficients in the same model. First, pay attention to the units. In this case, both market value and sales are measured in millions of dollars, so the coefficients are both in terms of a $1 million increase. Thus, the comparison is between a $1 million increase in market value versus a $1 million increase in sales, which increase CEO salary by $18.23 and $21.16, respectively (holding constant the other explanatory variables). This is an apples to apples comparison that is easy to understand. Be extra careful that the scale is the same. Here, the scale is the same because both are in millions, but watch out for when one variable is measured in millions and another is measured in thousands, etc. If the variables have different units, be extra clear about what you are comparing. For example, if you were trying to compare sales to CEO tenure, you’d be comparing the effects on salary of an additional $1 million in sales and 1 additional year of tenure as CEO. The other thing to keep in mind when comparing coefficients is statistical significance. A coefficient could be larger by chance, so we need to run a hypothesis test to see if we have evidence to believe the difference is not just occurring randomly. We need to do a hypothesis test on the null hypothesis that \\(\\left(\\hat{\\beta}_{profit}-\\hat{\\beta}_{sales}\\right)=0\\) versus the alternative that it does not. Formally, our test statistic is: \\[ t=\\frac{\\left(\\hat{\\beta}_{profit}-\\hat{\\beta}_{sales}\\right)-0}{\\text{SE}\\left(\\hat{\\beta}_{profit}-\\hat{\\beta}_{sales}\\right)} \\] Thus, we need to calculate not just the difference in coefficients, \\(\\left(\\hat{\\beta}_{profit}-\\hat{\\beta}_{sales}\\right)\\), but also the standard error of the difference, \\(\\text{SE}\\left(\\hat{\\beta}_{profit}-\\hat{\\beta}_{sales}\\right)\\). To calculate the standard error of the difference, we need to know not just the standard error of \\(\\hat{\\beta}_{profit}\\) and \\(\\hat{\\beta}_{sales}\\), but also the correlation between the two estimates (if both coefficients tend to be big together and small together, the coefficients tend to stay closer together, making the difference smaller, but if they are negatively correlated, one tends to be bigger when the other one is smaller and the difference tends to be larger). Thus, calculating \\(\\text{SE}\\left(\\hat{\\beta}_{profit}-\\hat{\\beta}_{sales}\\right)\\) requires the full variance-covariance matrix. Instead, we’ll use the glht function from the multcomp package to perform the test for us: testOfDif &lt;- glht(model3, linfct = c(&quot;profits - sales = 0&quot;)) summary(testOfDif) ## ## Simultaneous Tests for General Linear Hypotheses ## ## Fit: lm(formula = salary ~ sales + ceoten + mktval + profits, data = mydata) ## ## Linear Hypotheses: ## Estimate Std. Error t value Pr(&gt;|t|) ## profits - sales == 0 30.4 278.0 0.109 0.913 ## (Adjusted p values reported -- single-step method) We can interpret the p-value just like the p-value for a hypothesis test of a single coefficient. The p-value is 0.9130323, so we fail to reject the null hypothesis that the two coefficients are the same at all standard levels of significance. In other words, we can’t reject that the difference is due to chance. We could also form a confidence interval. Here’s the 90% confidence interval for the difference in coefficients: confint(testOfDif,level = 0.9) ## ## Simultaneous Confidence Intervals ## ## Fit: lm(formula = salary ~ sales + ceoten + mktval + profits, data = mydata) ## ## Quantile = 1.6538 ## 90% family-wise confidence level ## ## ## Linear Hypotheses: ## Estimate lwr upr ## profits - sales == 0 30.4026 -429.2872 490.0925 This 90% confidence interval contains 0; thus, we fail to reject the null hypothesis that the difference is 0 at all standard levels of significance. So you can see what it looks like when we reach the opposite conclusion, here’s the same test for the difference between the coefficient on ceoten and the coefficient on sales: testOfDif2 &lt;- glht(model3, linfct = c(&quot;ceoten - sales = 0&quot;)) summary(testOfDif2) ## ## Simultaneous Tests for General Linear Hypotheses ## ## Fit: lm(formula = salary ~ sales + ceoten + mktval + profits, data = mydata) ## ## Linear Hypotheses: ## Estimate Std. Error t value Pr(&gt;|t|) ## ceoten - sales == 0 12713 5635 2.256 0.0253 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## (Adjusted p values reported -- single-step method) confint(testOfDif2) ## ## Simultaneous Confidence Intervals ## ## Fit: lm(formula = salary ~ sales + ceoten + mktval + profits, data = mydata) ## ## Quantile = 1.9739 ## 95% family-wise confidence level ## ## ## Linear Hypotheses: ## Estimate lwr upr ## ceoten - sales == 0 12712.6292 1590.0204 23835.2380 The p-value is 0.0253289, so we reject the null hypothesis that the coefficients are the same at the 5% level. In other words, we have evidence (at the 5% level) supporting the claim that the effect on CEO salary of an additional year of CEO tenure is larger than the effect of an additional $1 million in sales, holding constant market value and profits. 14.6 A note on presentation If you include formal hypothesis tests like those just discussed in the Joint hypotheses test and Testing linear combinations of parameters sections in a paper, you will typically not include the complete test output. Instead, you’ll just say what the test is, report the p-value, and explain the conclusion of the test. The detailed output of the test is included here as part of the BP because it is useful to see all the output as a reference later. 14.7 Log transformations This section has several things you need to do (and after you complete everything you need to do in this section, you can submit this chapter of your BP). This data provides a good example of why visualizing your data can be helpful. By visualizing the data using histograms and scatter plots, it is easy to see how skewed the data is and how log transformations can help center it. You will focus on two variables, sales and salary. You will first make histograms and then make scatter plots showing the relationship between these two variables. 14.7.1 Histograms Make four histograms: salary log(salary) sales log(sales) You can format them however you’d like, but make sure they are clearly labeled. salary salary_histogram &lt;- ggplot(mydata, aes(salary)) + geom_histogram() salary_histogram ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. log(salary) log_salary_histogram &lt;- ggplot(mydata, aes(log(salary))) + geom_histogram() log_salary_histogram ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. sales sales_histogram &lt;- ggplot(mydata, aes(sales)) + geom_histogram() sales_histogram ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. log(sales) log_sales_histogram &lt;- ggplot(mydata, aes(log(sales))) + geom_histogram() log_sales_histogram ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 14.7.2 Scatter plots You can also see how log transformations spread out the data by looking at scatter plots of salary or log(salary) (y) versus sales or log(sales) corresponding with the four different combinations. Specifically, create four scatter plots: salary (y) vs. sales (x) log(salary) (y) vs. sales (x) salary (y) vs. log(sales) (x) log(salary) (y) vs. log(sales) (x) Also add the following to each of the four plots: A best fit (geom_smooth) “lm” line. Just to get a bit more experience with data visualization, color the points by ceoten using whatever scale_color_gradient (or scale_color_gradientn) you’d like. salary (y) vs. sales (x) salary_sales &lt;- ggplot(mydata, aes(sales, salary, color = ceoten)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se=FALSE) salary_sales ## `geom_smooth()` using formula &#39;y ~ x&#39; log(salary) (y) vs. sales (x) log_salary_sales &lt;- ggplot(mydata, aes(sales, log(salary), color = ceoten)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se=FALSE) + scale_color_gradient2(high = &quot;black&quot;) log_salary_sales ## `geom_smooth()` using formula &#39;y ~ x&#39; salary (y) vs. log(sales) (x) salary_log_sales &lt;- ggplot(mydata, aes(log(sales), salary, color = ceoten)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se=FALSE) + scale_color_gradient2(high = &quot;red&quot;) salary_log_sales ## `geom_smooth()` using formula &#39;y ~ x&#39; log(salary) (y) vs. log(sales) (x) salary_log_sales &lt;- ggplot(mydata, aes(log(sales), log(salary), color = ceoten)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se=FALSE) + scale_color_gradient2(high = &quot;blue&quot;) salary_log_sales ## `geom_smooth()` using formula &#39;y ~ x&#39; 14.7.3 Regression models with levels and logs To make sure you know how to estimate models and display the results using stargazer, you should estimate the following 4 models and display the results. model 3 (the same one discussed above…either version is fine) model 3, but with log(salary) instead of salary model 3, but with log(sales) instead of sales model 3, but with log(salary) instead of salary and log(sales) instead of sales Display all four models in the same stargazer table. And yes, this just requires you to copy/paste what you can find above in this file, but part of the point is to make sure you know how to use the above examples and modify them as needed. Finally, to demonstrate that you understand how to formally interpret regression coefficients, you will type formal interpretations of coefficients. To make sure you understand what is required of you when formally interpreting regression coefficients for Econ 380, you should use “LN3.3 (MLR Interpretations).pptx,” “Rules for Interpreting Regression Results.pdf,” “Interpretations Rubric (all 4 level log cases).pdf,” and the example PDFs posted in Moodle. The interpretations you type must include formal interpretation of coefficients that include the following combinations of level/log: One coefficient where \\(y\\) is a level and \\(x\\) is a level One coefficient where \\(y\\) is a log and \\(x\\) is a level One coefficient where \\(y\\) is a level and \\(x\\) is a log One coefficient where \\(y\\) is a log and \\(x\\) is a log Your interpretations must also include: One coefficient that is statistically significant at the 1% level One coefficient that is statistically significant at the 5% level One coefficient that is statistically significant at the 10% level One coefficient that is not statistically significant at any standard level of significance Note that you can combine these where possible (i.e., one interpretation can be both a log y / level x and a particular level of significance) so you don’t need to type out 8 separate interpretations. I don’t believe you can do it with only 4 interpretations, but I believe you can do it with 5. Please say which model you’re looking at for each interpretation. 14.7.3.1 Regression Models model 3 (the same one discussed above…either version is fine) model 3, but with log(salary) instead of salary model 3, but with log(sales) instead of sales model 3, but with log(salary) instead of salary and log(sales) instead of sales model3 &lt;- lm(salary~sales+mktval+profits+ceoten,data=mydata) model3_logsalary &lt;- lm(log(salary)~sales+mktval+profits+ceoten,data=mydata) model3_logsales &lt;- lm(salary~log(sales)+mktval+profits+ceoten,data=mydata) model3_logsalarylogsales &lt;- lm(log(salary)~log(sales)+mktval+profits+ceoten,data=mydata) stargazer(model3, model3_logsalary, model3_logsales, model3_logsalarylogsales, type = &quot;html&quot;, report=(&#39;vc*p&#39;), keep.stat = c(&quot;n&quot;,&quot;rsq&quot;,&quot;adj.rsq&quot;), notes = &quot;&lt;em&gt;&amp;#42;p&amp;lt;0.1;&amp;#42;&amp;#42;p&amp;lt;0.05;&amp;#42;&amp;#42;&amp;#42;p&amp;lt;0.01&lt;/em&gt;&quot;, notes.append = FALSE, model.numbers = FALSE, column.labels = c(&quot;level-level&quot;,&quot;log-level&quot;, &quot;level-log&quot;, &quot;log-log&quot;)) Dependent variable: salary log(salary) salary log(salary) level-level log-level level-log log-log sales 18.233* 0.00003** p = 0.100 p = 0.022 log(sales) 124,542.800*** 0.193*** p = 0.0004 p = 0.00000 mktval 21.157 0.00002 19.715 0.00002 p = 0.183 p = 0.276 p = 0.200 p = 0.314 profits 48.636 0.00002 21.408 -0.00004 p = 0.859 p = 0.935 p = 0.932 p = 0.873 ceoten 12,730.860** 0.011* 12,594.850** 0.011** p = 0.026 p = 0.056 p = 0.023 p = 0.042 Constant 613,958.800*** 13.240*** -210,327.500 11.957*** p = 0.000 p = 0.000 p = 0.385 p = 0.000 Observations 177 177 177 177 R2 0.201 0.206 0.247 0.312 Adjusted R2 0.183 0.187 0.229 0.296 Note: *p&lt;0.1;**p&lt;0.05;***p&lt;0.01 14.7.3.2 Interpretations One coefficient where \\(y\\) is a level and \\(x\\) is a level &amp; One coefficient that is statistically significant at the 5% level From model3: Holding the company’s sales, the company’s market value, and the company’s profits constant, if the CEO’s tenure as CEO at the company increases by 1 year, the CEO’s salary is predicted to increase by $12,730.86, on average. This is statistically significant at the 5% level. One coefficient where \\(y\\) is a log and \\(x\\) is a level &amp; One coefficient that is statistically significant at the 10% level From model3_logsalary: Holding the company’s sales, the company’s market value, and the company’s profits constant, if the CEO’s tenure as CEO at the company increases by 1 year, the CEO’s salary is predicted to increase by 1.1%, on average. This is statistically significant at the 10% level. One coefficient where \\(y\\) is a level and \\(x\\) is a log &amp; One coefficient that is statistically significant at the 1% level From model3_logsales: Holding the company’s market value, the company’s profits, and the CEO’s tenure constant, if the company’s sales increases by 1%, the CEO’s salary is predicted to increase by $1245.428, on average. This is statistically significant at the 1% level. One coefficient where \\(y\\) is a log and \\(x\\) is a log From model3_logsalarylogsales: Holding the company’s market value, the company’s profits, and the CEO’s tenure constant, if the company’s sales increases by 1%, the CEO’s salary is predicted to increase by 0.193%, on average. This is statistically significant at the 1% level. One coefficient that is not statistically significant at any standard level of significance From model3_logsalarylogsales: Holding the company’s sales, the company’s profits, and the CEO’s tenure constant, if the company’s market value increases by 1%, the CEO’s salary is predicted to increase by 0.00002%, on average. This is not statistically significant at any standard level. Credit to this Stack Exchange answer for this fix.↩︎ See Wooldridge section 3.4 for a full discussion of the variance of OLS coefficients, and section 3.4a for a discussion of multicollinearity, including the VIF. For these explanations I am assuming homoskedasticity, but the same concepts and intuition apply when there is heteroskedasticity.↩︎ In fact, the \\(SST_j\\) in the denominator of \\(\\text{VAR}(j)\\) is equivalent to \\((n-1)\\text{VAR}(x_j)\\), so you can re-write the equation for \\(\\text{VAR}(\\hat{\\beta}_j)\\) with \\((n-1)\\) in the denominator; hence, larger \\(n\\) leads to lower \\(\\text{VAR}(\\hat{\\beta}_j)\\).↩︎ "],["dummy-variables-part-1.html", "15 Dummy Variables Part 1 15.1 Obtain and prepare data 15.2 Define dummy variables 15.3 Compare the regressions side-by-side 15.4 Compare the predictions of each model", " 15 Dummy Variables Part 1 This chapter goes with LN4.1. In LN4.1 we examine two models of the relationship between education and wages. We examined two different ways we could define sets of dummy variables to represent people with a high school degree, a BA, a Master’s, or a PhD as their highest level of education. We saw how both models could be used to answer questions such as “what is the average wage for someone with a Master’s degree?” or “What is the expected difference in average wage for a person with a Master’s degree compared to a BA?” In this BP chapter you will explore these models empirically using micro-level data (i.e., data on individual people). Specifically, the data we’ll use comes from the 2019 ACS 1-year Public Use Microdata Sample (PUMS). This data has the responses of individuals that the Census Bureau uses to calculate the county- and state-level statistics they publish as part of the ACS (e.g., what you’re using for the CP). Formally to analyze this data properly we need to use survey weights to make calculates done using the individual responses representative of the population as a whole. We’re not going to worry about that. However, note that our calculations will be off somewhat from the true average wages for the groups we explore in this chapter. We’ll obtain our data using the tidycensus package, as described here. library(tidyverse) library(tidycensus) library(stargazer) library(pander) ## This turns off scientific notation when there are fewer then 4 digits. Otherwise pander displays very small p values with way too many decimal places options(scipen = 4) 15.1 Obtain and prepare data To work with the PUMS data via tidycensus, you can pull the table of variables to find what variables are included. I’ve set this code chunk to not be evaluated (eval = FALSE) when you build because while doing this is helpful while you’re working with this data, it doesn’t belong in your HTML output ## To work with the PUMS data via tidycensus, you can pull the table of variables to find what variables are included ## I&#39;ve set this code chunk to not be evaluated (eval = FALSE) when you build. You might want to do this on your own, but it doesn&#39;t belong in your HTML output pums_vars_2018 &lt;- pums_variables %&gt;% filter(year == 2019, survey == &quot;acs1&quot;) .variablesTable th { text-align: left; } Let’s pull the following variables from the 2019 ACS-1 year Public Use Microdata (i.e., data on individual people) for the state of Wisconsin (we’re limiting our sample to Wisconsin so that it doesn’t take too long to load or work with). Variable Description PUMA Public Use Microdata Areas WAGP WAGe of Person AGEP AGE of Person SCHL educational attainment of person sex SEX of person The results='hide' code chunk option keeps it from displaying status updates while it downloads (which is sometimes about 100 lines of output, so you definitely don’t want it in your HTML output). ## Download 2018 acs1 for Wisconsin. wiPUMS &lt;- get_pums( variables = c(&quot;PUMA&quot;,&quot;WAGP&quot;, &quot;AGEP&quot;, &quot;SCHL&quot;, &quot;SEX&quot;), state = &quot;wi&quot;, survey = &quot;acs1&quot;, year = 2018, recode = TRUE ) Let’s rename WAGP as wage so that we remember what it is. wiPUMS &lt;- rename(wiPUMS, wage = WAGP) Let’s see what levels of education exist in our data, and how many observations we have for each wiPUMS %&gt;% count(SCHL,SCHL_label) %&gt;% pander() SCHL SCHL_label n 01 No schooling completed 1552 02 Nursery school, preschool 766 03 Kindergarten 718 04 Grade 1 614 05 Grade 2 650 06 Grade 3 722 07 Grade 4 724 08 Grade 5 802 09 Grade 6 793 10 Grade 7 796 11 Grade 8 1306 12 Grade 9 1000 13 Grade 10 1321 14 Grade 11 1511 15 12th grade - no diploma 851 16 Regular high school diploma 14364 17 GED or alternative credential 1619 18 Some college, but less than 1 year 3724 19 1 or more years of college credit, no degree 6867 20 Associate’s degree 5169 21 Bachelor’s degree 8182 22 Master’s degree 2859 23 Professional degree beyond a bachelor’s degree 714 24 Doctorate degree 455 bb N/A (less than 3 years old) 1754 We’re interested in examining how wages differ by educational attainment. We’d like to limit our data to people who are old enough to have completed high school (or equivalent) and college, and then be working. So, let’s limit our sample to people age 25 or older. Let’s also only examine people who have a wage. wiPUMS &lt;- wiPUMS %&gt;% filter(AGEP &gt;= 25 &amp; wage &gt; 0) We should look at what education levels (SCHL) remain after filtering wiPUMS %&gt;% count(SCHL,SCHL_label) %&gt;% pander() SCHL SCHL_label n 01 No schooling completed 133 02 Nursery school, preschool 4 03 Kindergarten 5 04 Grade 1 1 05 Grade 2 3 06 Grade 3 5 07 Grade 4 4 08 Grade 5 8 09 Grade 6 39 10 Grade 7 14 11 Grade 8 92 12 Grade 9 95 13 Grade 10 168 14 Grade 11 282 15 12th grade - no diploma 349 16 Regular high school diploma 6826 17 GED or alternative credential 824 18 Some college, but less than 1 year 1948 19 1 or more years of college credit, no degree 3579 20 Associate’s degree 3664 21 Bachelor’s degree 5600 22 Master’s degree 2002 23 Professional degree beyond a bachelor’s degree 515 24 Doctorate degree 320 After filtering, the value “bb” that indicates a person who is under 3 years old and thus can’t have any schooling is now gone. Now all of the levels of SCHL are numeric, which allows us to convert them from a character to a numeric data type. This will then allow us to use inequalities to define dummy variables (e.g., SCHL &lt;16). wiPUMS$SCHL &lt;- as.numeric(wiPUMS$SCHL) We’re comparing wages of people who have a high school degree (or equivalent) or higher, so we also need to drop everyone who has less than a high school degree. Because SCHL is now numeric, we can do this using an inequality (rather than listing all of the options separately). wiPUMS &lt;- wiPUMS %&gt;% filter(SCHL &gt;= 16) In order to match with LN4.1, we also need to drop people with a professional degree. In practice we would probably want to examine them too, but for the sake of matching with what is in LN4.1 we’re going to drop them. wiPUMS &lt;- wiPUMS %&gt;% filter(SCHL != 23) Whenever you do something like filtering data, it’s a very good idea to look at the data and make sure it worked. In previous years of 380, students have wasted many hours trying to get models to work, only to finally go back and look at the data to find that they actually messed up an earlier step that seems easy. So even if it’s something easy you think you know how to do, look at the data. You can display summary measures as we’ll do here, but it’s also a good idea to click on it in the Environment tab and actually scroll through it quickly. Typically you don’t display the results in a paper, but for the purposes of the BP, I want to demonstrate what you might do (e.g., get a count by education levels). Here, we could just re-run this: wiPUMS %&gt;% count(SCHL,SCHL_label) %&gt;% pander() SCHL SCHL_label n 16 Regular high school diploma 6826 17 GED or alternative credential 824 18 Some college, but less than 1 year 1948 19 1 or more years of college credit, no degree 3579 20 Associate’s degree 3664 21 Bachelor’s degree 5600 22 Master’s degree 2002 24 Doctorate degree 320 15.2 Define dummy variables In LN4.1 we had an “alpha model” and “beta model” which defined education in different ways. Variable Alpha Model Definition Beta Model Definition HS N/A =1 if highest degree is high school, =0 otherwise BA =1 if have B.A. degree, =0 if don’t =1 if highest degree is B.A. degree, =0 otherwise Masters =1 if have Master’s degree, =0 if don’t =1 if highest degree is Master’s, =0 otherwise PhD =1 if have PhD, ==0 if don’t =1 if highest degree is PhD, =0 otherwise 15.2.1 Alpha Model In LN4.1, we defined the “Alpha Model” as \\[ wage = \\alpha_0 + \\alpha_1 BA + \\alpha_2 Masters + \\alpha_3 PhD + u \\] We need to create the dummy variables used for the “Alpha Model.” We’ll prefix the variables with “a” for “Alpha.” Later you’ll define “Beta Model” variables and prefix them with “b.” In our data, some people have some college or an Associates Degree. While we might be interested in differences between people with some college and a only a high school degree, for the purposes of what we’re doing here (learning about dummy variables), let’s classify anyone without a BA as having high school as their highest degree (even if they have some college or an Associates Degree). # Create dummy variables for &quot;alpha model&quot; (starting with a) wiPUMS &lt;- wiPUMS %&gt;% mutate(aBA = ifelse(SCHL &gt;= 21, 1, 0), aMasters = ifelse(SCHL &gt;= 22, 1, 0), aPhD = ifelse(SCHL == 24, 1, 0) ) Now let’s estimate the regression shown as “Alpha Model v1” in LN4 alphaModel &lt;- lm(wage ~ aBA + aMasters + aPhD, data = wiPUMS) pander(summary(alphaModel))   Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 40891 357.2 114.5 0 aBA 21960 715.1 30.71 2.893e-203 aMasters 9235 1207 7.65 2.08e-14 aPhD 34351 2791 12.31 1.041e-34 Fitting linear model: wage ~ aBA + aMasters + aPhD Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 24763 46360 0.07482 0.07471 15.2.2 Beta Model In LN4.1, we defined the “Beta Model” (version 1) as: \\[ wage = \\beta_0 + \\beta_1 BA + \\beta_2 Masters + \\beta_3 PhD + u \\] We need to create the dummy variables for the “Beta Model” from LN4.1. Use the same categories as you did for the Alpha Model (so we’re considering anyone with some college or an associates degree as having high school as their highest education). Prefix these variables with a “b” for “Beta.” YOUR CODE GOES HERE: add variables bBA, bMasters, and bPhD to wiPUMS # Create dummy variables for &quot;Beta Model&quot; (prefix with b) wiPUMS &lt;- wiPUMS %&gt;% mutate(bBA = ifelse(SCHL == 21, 1, 0), bMasters = ifelse(SCHL == 22, 1, 0), bPhD = ifelse(SCHL == 24, 1, 0) ) Now let’s estimate the regression shown as “Beta Model v1” in LN4.1 UN-COMMENT-OUT the following code after you estimate the Beta Model v1 betaModel &lt;- lm(wage ~ bBA + bMasters + bPhD, data = wiPUMS) pander(summary(betaModel))   Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 40891 357.2 114.5 0 bBA 21960 715.1 30.71 2.893e-203 bMasters 31195 1096 28.46 2.205e-175 bPhD 65546 2616 25.06 7.787e-137 Fitting linear model: wage ~ bBA + bMasters + bPhD Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 24763 46360 0.07482 0.07471 15.3 Compare the regressions side-by-side In LN4.1 we talk about how different models can lead to equivalent results theoretically. Here we want to examine that theoretical equivalence numerically. Specifically, we want to write out various conditional expectations that should be equivalent theoretically and show that the estimated values are indeed equivalent. Let’s start by displaying the two models on the same table. You have to be very careful doing this (so you don’t accidentally mis-label variables), but we can re-name the variable labels for each model so that they are the same (e.g., rename aBA and bBA to both be BA) and thus display on the same row of the stargazer table. We’ll make a copy of the model results that we use specifically for this purpose. Then we’ll rename the coefficients to have the same names. Then we’ll display them using stargazer. UN-COMMENT-OUT the following code after you estimate the Beta Model v1 alphaModelStargazer &lt;- alphaModel betaModelStargazer &lt;- betaModel names(alphaModelStargazer$coefficients) &lt;- c(&quot;(Intercept)&quot;, &quot;BA&quot;, &quot;Masters&quot;, &quot;PhD&quot;) names(betaModelStargazer$coefficients) &lt;- c(&quot;(Intercept)&quot;, &quot;BA&quot;, &quot;Masters&quot;, &quot;PhD&quot;) stargazer(alphaModelStargazer, betaModelStargazer, type = &quot;html&quot;, report=(&#39;vc*p&#39;), notes = &quot;&lt;em&gt;&amp;#42;p&amp;lt;0.1;&amp;#42;&amp;#42;p&amp;lt;0.05;&amp;#42;&amp;#42;&amp;#42;p&amp;lt;0.01&lt;/em&gt;&quot;, notes.append = FALSE, model.numbers = FALSE, column.labels = c(&quot;Alpha&quot;,&quot;Beta&quot;)) Dependent variable: wage Alpha Beta BA 21,960.030*** 21,960.030*** p = 0.000 p = 0.000 Masters 9,235.282*** 31,195.310*** p = 0.000 p = 0.000 PhD 34,351.040*** 65,546.350*** p = 0.000 p = 0.000 Constant 40,890.680*** 40,890.680*** p = 0.000 p = 0.000 Observations 24,763 24,763 R2 0.075 0.075 Adjusted R2 0.075 0.075 Residual Std. Error (df = 24759) 46,359.640 46,359.640 F Statistic (df = 3; 24759) 667.408*** 667.408*** Note: *p&lt;0.1;**p&lt;0.05;***p&lt;0.01 15.4 Compare the predictions of each model In LN4.1 we claimed that the Alpha Model and Beta Model were theoretically equivalent. Here we are demonstrating that they are also empirically equivalent. The first thing you notice is that the \\(R^2\\) and adjusted \\(R^2\\) are identical. We haven’t talked about the other statistics (Residual Std. Error and F STatistic), but they’re identical too. That’s what we would expect if the models are indeed equivalent. Recall the definition of \\(R^2\\). You’d expect equivalent models to explain the same fraction of the variation in wages. Now let’s check if real-world situations that are supposed to be equivalent in theory are indeed equivalent numerically. First, let’s store our coefficients in variables so they’re easier to work with: ## Alpha Model a0 &lt;- coef(alphaModel)[&quot;(Intercept)&quot;] a1 &lt;- coef(alphaModel)[&quot;aBA&quot;] a2 &lt;- coef(alphaModel)[&quot;aMasters&quot;] a3 &lt;- coef(alphaModel)[&quot;aPhD&quot;] ## Beta Model ## FILL THESE IN WITH COEFFICIENTS FROM YOUR BETA MODEL b0 &lt;- coef(betaModel)[&quot;(Intercept)&quot;] b1 &lt;- coef(betaModel)[&quot;bBA&quot;] b2 &lt;- coef(betaModel)[&quot;bMasters&quot;] b3 &lt;- coef(betaModel)[&quot;bPhD&quot;] ## I&#39;ll use this to let you know where you need to fill in answers PLACEHOLDER &lt;- &quot;PLACEHOLDER&quot; We’ll check a variety of situations explored by these models. For each situation, we’ll write out the following 6 steps: First, we’ll write out the situation in English (e.g., “Expected/average wage for a person whose highest degree is high school”) Next, we’ll write out the conditional conditional expectation corresponding to that situation (e.g., \\(E(wage|HS)\\)) Then we’ll write out the conditional expectation in terms of model parameters using parameters from the Alpha Model (e.g., \\(\\alpha_0\\))… …and in terms of model parameters using parameters from the Beta Model (e.g., \\(\\beta_0\\)). Last, we’ll calculate the value estimated using the Alpha Model (e.g., 40890.68)… …and the value estimated using the Beta Model (e.g., 40890.68). I’ve filled in a few parts for you below to show you what you’re supposed to do. Anyplace you see “PLACEHOLDER” you need to fill in whatever should go there. Use LaTex for parameters (e.g., \\(\\alpha_0\\), \\(\\beta_0\\)). Each value should be displayed to 2 decimal places using format(...,nsmall=2). Note that you can add together multiple coefficients inside the format function. This structure is already set up for you below. When you start filling it in, make sure to knit frequently so you don’t mess it up and then are unable to figure out what isn’t working. For the first few, I’d knit each time you make a change (remember once you’ve built your book once during your R session, you can simply knit the RMD file you’re working with and just re-build the entire book before committing/pushing to GitHub). Here are the scenarios: Expected/average wage for a person whose highest degree is high school (HS) \\[ \\begin{aligned} E(wage|HS) &amp;= \\alpha_0 \\\\&amp;= \\beta_0 \\\\&amp;= 40890.68 \\\\&amp;= 40890.68 \\end{aligned} \\] Expected/average wage for a person whose highest degree is BA \\[ \\begin{aligned} E(wage|BA) &amp;= \\alpha_0 + \\alpha_1 \\\\&amp;= \\beta_0 + \\beta_1 \\\\&amp;= 62850.71 \\\\&amp;= 62850.71 \\end{aligned} \\] Expected/average wage for a person whose highest degree is Master’s \\[ \\begin{aligned} E(wage|Masters) &amp;= \\alpha_0 + \\alpha_1 + \\alpha_2 \\\\&amp;= \\beta_0 + \\beta_2 \\\\&amp;= 72085.99 \\\\&amp;= 72085.99 \\end{aligned} \\] Expected/average wage for a person whose highest degree is PhD \\[ \\begin{aligned} E(wage|PhD) &amp;= \\alpha_0 + \\alpha_1 + \\alpha_2 + \\alpha_3 \\\\&amp;= \\beta_0 + \\beta_3 \\\\&amp;= 106437.03 \\\\&amp;= 106437.03 \\end{aligned} \\] How much higher do you expect the average wage to be for someone whose highest degree is a BA compared to someone whose highest degree is high school (HS) \\[ \\begin{aligned} E(wage|BA) - E(wage|HS) &amp;= \\alpha_0 + \\alpha_1 - (\\alpha_0) = \\alpha_1 \\\\&amp;= \\beta_0 + \\beta_1 - (\\beta_0) = \\beta_1 \\\\&amp;= 21960.03 \\\\&amp;= 21960.03 \\end{aligned} \\] How much higher do you expect the average wage to be for someone whose highest degree is a Master’s compared to someone whose highest degree is high school (HS) \\[ \\begin{aligned} E(wage|Masters) - E(wage|HS) &amp;= \\alpha_0 + \\alpha_1 + \\alpha_2 - (\\alpha_0) = \\alpha_1 + \\alpha_2 \\\\&amp;= \\beta_0 + \\beta_2 - (\\beta_0) = \\beta_2 \\\\&amp;= 31195.31 \\\\&amp;= 31195.31 \\end{aligned} \\] How much higher do you expect the average wage to be for someone whose highest degree is a PhD compared to someone whose highest degree is high school (HS) \\[ \\begin{aligned} E(wage|PhD) - E(wage|HS) &amp;= \\alpha_0 + \\alpha_1 + \\alpha_2 + \\alpha_3 - (\\alpha_0) = \\alpha_1 + \\alpha_2 + \\alpha_3 \\\\&amp;= \\beta_0 + \\beta_3 - (\\beta_0) = \\beta_3 \\\\&amp;= 65546.35 \\\\&amp;= 65546.35 \\end{aligned} \\] How much higher do you expect the average wage to be for someone whose highest degree is a Master’s compared to someone whose highest degree is a BA \\[ \\begin{aligned} E(wage|Masters) - E(wage|BA) &amp;= \\alpha_0 + \\alpha_1 + \\alpha_2 - (\\alpha_0 + \\alpha_1) = \\alpha_2 \\\\&amp;= \\beta_0 + \\beta_2 - (\\beta_0 + \\beta_1) = \\beta_2 - \\beta_1 \\\\&amp;= 9235.282 \\\\&amp;= 9235.282 \\end{aligned} \\] How much higher do you expect the average wage to be for someone whose highest degree is a PhD compared to someone whose highest degree is a BA \\[ \\begin{aligned} E(wage|PhD) - E(wage|BA) &amp;= \\alpha_0 + \\alpha_1 + \\alpha_2 + \\alpha_3 - (\\alpha_0 + \\alpha_1) = \\alpha_2 + \\alpha_3 \\\\&amp;= \\beta_0 + \\beta_3 - (\\beta_0 + \\beta_1) = \\beta_3 - \\beta_1 \\\\&amp;= 43586.32 \\\\&amp;= 43586.32 \\end{aligned} \\] How much higher do you expect the average wage to be for someone whose highest degree is a PhD compared to someone whose highest degree is a Master’s \\[ \\begin{aligned} E(wage|PhD) - E(wage|Masters) &amp;= \\alpha_0 + \\alpha_1 + \\alpha_2 + \\alpha_3 - (\\alpha_0 + \\alpha_1 + \\alpha_2) = \\alpha_3 \\\\&amp;= \\beta_0 + \\beta_3 - (\\beta_0 + \\beta_2) = \\beta_3 - \\beta_2 \\\\&amp;= 34351.04 \\\\&amp;= 34351.04 \\end{aligned} \\] 15.4.1 Group averages Linear regression when all explanatory variables are dummy variables produces the same estimates of average values for different groups as if you simply took the average of the \\(y\\) value for each group. You should be able to confirm this by going back in your BP to chapters where you calculated group averages. Create a single variable for education level (with levels HS, BA, Masters, PhD) and then calculate the average wage for each group. Why then do we use regression? One reason is because we easily get standard errors, confidence intervals, and p-values for hypothesis tests of the group averages and differences between groups. Another reason is because we can also control for other variables (such as age…see below). 15.4.2 Causal estimates? Make sure you can explain why we should not interpret our results as causal. For example, above we estimated that the average wage is 21960.03 higher for someone whose highest degree is a BA compared to someone whose highest degree is high school (HS). Why can we not claim that getting a BA causes this increase in wage? In other words, why is \\(\\hat{\\alpha}_1\\) (or \\(\\hat{\\beta}_1\\)) not an unbiased estimate of the true effect of getting a BA on wages? 15.4.3 What about age? You may find it interesting to estimate the models above, but controlling for age (i.e., add age as a variable). What changes 15.4.4 What about sex? How might you add sex to the model? Suppose you create a female dummy variable and then include it in the model? What would that allow you to say about differences in wages by sex? What would you have to do to estimate how education affects wages differently for males versus females? "],["dummy-variables-part-2.html", "16 Dummy Variables Part 2 16.1 Size only 16.2 Number of bathrooms and size 16.3 Slope dummy 16.4 Intercept and slope dummies 16.5 Models with the number of bedrooms", " 16 Dummy Variables Part 2 Solutions .textSoln { color: red; } \\[ \\newcommand{\\betahat}[1]{\\hat{\\beta}_{#1}} \\] This chapter uses the data in HousePriceDummies.csv with price of the house in dollars (price), size of the hose in square feet (size), number of bathrooms (baths), and number of bedrooms (beds). This is the second chapter covering dummy variables. From this chapter you should learn about intercept dummies that allow the average \\(y\\) value to differ by group by the same amount for all values of the other explanatory variables. Intercept dummies are the most common use of dummy variables. You’ll create an intercept dummy from a numerical variable (baths) that only has two possible values, so while you’re working with a dummy variable, you’re actually just working with regression models you should already understand. THis hopefully helps you better understand how to work with all dummy variables, include those based on qualitative information.You should also learn about slope dummies that allow the slope to vary by group. Wherever you see qCnt() in the RMD file you’ll see a number in the HTML output. qCnt() is a counter (defined in the code chunk above) that is putting what are essentially question numbers in the HTML output. This makes it easier to discuss with others because we can refer to the different questions by number. 16.1 Size only 1) In this first code chunk, first estimate the simple linear regression model of price on size and store it in modelS (that’s “model” and a capital letter “S” for “Size”). Then display modelS using pander. Next, add a variable yHatS to the mydatadata frame with the predicted price from modelS. Yes, R has a fitted() function to do this for you, but I want you to calculate it manually using: \\[ \\hat{y} = \\betahat{0} + \\betahat{1}size \\] Finally, create a scatter plot of the data using the number of bathrooms (baths) as a factor for the color (we need factor() so it treats baths as distinct integers, 1 and 2, instead of a continuous variable that could have values like 1.234). We also include yHatS as a scatterplot and as a line (also using yHatS). Make sure you understand why all of the yHatS points are on the yHatS line. Throughout this chapter we’ll use an “x” symbol (ggplot’s shape=4) to display the data and dots (i.e., filled-in circles, ggplot’s shape=19, which is also it’s default) to display predicted prices (i.e., yHatS). I filled this first one in for you. modelS &lt;- lm(price~size,data=mydata) pander(summary(modelS))   Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 49174 15291 3.216 0.001502 size 154.4 7.645 20.19 1.826e-51 Fitting linear model: price ~ size Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 216 55363 0.6558 0.6542 mydata$yHatS &lt;- coef(modelS)[&quot;(Intercept)&quot;] + coef(modelS)[&quot;size&quot;] * mydata$size ggplot(mydata) + geom_point(aes(y=price,x=size,col=factor(baths)),shape=4) + geom_point(aes(y=yHatS,x=size,col=factor(baths))) + geom_line(aes(y=yHatS,x=size),col=&quot;black&quot;) 2) A big part of our focus in this chapter is the regression lines, so let’s be more explicit about plotting the line. The line we plotted above used geom_line(aes(y=yHatS,x=size)). We could plot this same line using geom_smooth, but later we’re going to plot lines that don’t work easily with geom_smooth. Instead, we’re going to use use geom_abline() to plot a line using its intercept and slope. Recall that yHatS is: \\[ \\hat{y} = \\betahat{0} + \\betahat{1}size \\] so we need to use \\(\\betahat{0}=\\) 49173.68 for geom_abline’s intercept argument and \\(\\betahat{1}=\\) 154.37 for geom_abline’s slope argument. We’ll also expand the axes limits so we can see the y intercepts; to do this, we’ll include: scale_x_continuous(expand = c(0, 0),limits = c(0, max(mydata$size)*1.02),breaks = seq(0,max(mydata$size)*1.02,500)) and scale_y_continuous(expand = c(0, 0),limits = c(0, max(mydata$price)*1.02), breaks = seq(0,max(mydata$price)*1.02,50000)) ggplot(mydata) + scale_x_continuous(expand = c(0, 0),limits = c(0, max(mydata$size)*1.02), breaks = seq(0,max(mydata$size)*1.02,500)) + scale_y_continuous(expand = c(0, 0),limits = c(0, max(mydata$price)*1.02), breaks = seq(0,max(mydata$price)*1.02,50000)) + geom_point(aes(y=price,x=size,col=factor(baths)),shape=4) + geom_point(aes(y=yHatS,x=size,col=factor(baths))) + geom_abline(intercept = coef(modelS)[&quot;(Intercept)&quot;], slope = coef(modelS)[&quot;size&quot;],col=&quot;black&quot;) 3) The slope of yHatS is the effect of size (of an additional \\(ft^2\\)) on the predicted price from the model that only controls for size. What do you think will happen to the effect of size on the predicted price when we also control for baths? Controlling for the number of bathrooms (baths) should reduce the effect of size on the predicted price. Assuming that more bathrooms are associated with a higher price and that higher size is predicted when there are more bathrooms (i.e., houses with more bathrooms tend to be larger), leaving the number of bathrooms out of the model causes omitted variable bias (a violation of the ZCM assumption, MLR.4) that makes it look like size has a larger effect on price than it really does when baths is not controlled for. Note that omitted variable bias doesn’t always cause the coefficient to be larger (i.e., it could cause it to be smaller). In this case it’s larger because baths, size, and price are all positively correlated. 16.2 Number of bathrooms and size 4) Now add the number of bathrooms (baths) as a variable to the regression (in addition to size) and store the model as modelSB (“model” with “S” for size and “B” for baths). Display the output using pander. Add a variable yHatSB to mydata with the predicted prices from this model. Remember that for this model (the “SB” model that includes both size and baths), predicted prices are given by: \\[ \\hat{y} = \\betahat{0} + \\betahat{1}size + \\betahat{2}baths \\] ## Estimate regression modelSB &lt;- lm(price~size+baths,data=mydata) pander(summary(modelSB))   Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 20004 15384 1.3 0.1949 size 136.3 7.944 17.16 5.014e-42 baths 42005 7841 5.357 0.0000002187 Fitting linear model: price ~ size + baths Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 216 52094 0.6967 0.6938 ## Store yHat for this model with size and baths mydata$yHatSB &lt;- coef(modelSB)[&quot;(Intercept)&quot;] + coef(modelSB)[&quot;size&quot;] * mydata$size + coef(modelSB)[&quot;baths&quot;] * mydata$baths 5) Now let’s add yHatSB to the graph as a scatterplot. Copy the last graph you made above, remove the geom_point() of yHatS, and add a geom_point() of yHatSB. Make the color “orange” for all the yHatSB points (i.e., the new geom_point() should be this: geom_point(data=mydata,aes(y=yHatSB,x=size),col=\"orange\")). ggplot(mydata) + scale_x_continuous(expand = c(0, 0),limits = c(0, max(mydata$size)*1.02), breaks = seq(0,max(mydata$size)*1.02,500)) + scale_y_continuous(expand = c(0, 0),limits = c(0, max(mydata$price)*1.02), breaks = seq(0,max(mydata$price)*1.02,50000)) + geom_point(aes(y=price,x=size,col=factor(baths)),shape=4) + geom_point(aes(y=yHatS,x=size,col=factor(baths))) + geom_abline(intercept = coef(modelS)[&quot;(Intercept)&quot;], slope = coef(modelS)[&quot;size&quot;],col=&quot;black&quot;) + geom_point(data=mydata,aes(y=yHatSB,x=size),col=&quot;orange&quot;) 6) It looks like there are two upward-sloping parallel rows of yHatSB predicted prices. What accounts for the general upward slope of the yHatSB predicted prices? Why are there two rows (try looking at a count() of baths to help you answer this part of the question)? The general upward slope of the yHatSB predicted prices is from the relationship of price with size. Because yHatSB is the predicted price controlling for both size and baths but we are graphing price (and predicted price) against size only, the predicted points are not a straight line. Houses with more baths have a higher predicted price. The yHatSB predicted prices are in two rows because there are only two different values of baths, 1 and 2. mydata %&gt;% count(baths) ## baths n ## 1 1 102 ## 2 2 114 7) Look at the graph you just made. Notice that the yHatS line doesn’t go straight through the middle of the two rows of yHatSB predicted prices. What is the slope of the yHatS line (the answer is one of the coefficients estimated above)? What is the slope of the two rows of yHatSB predicted prices (the answer is a coefficient estimated above)? Which slope is steeper? What accounts for the difference between these different slopes? The yHatS line has a slope of about 154.37 ft2 (the coefficient on size from the regression of price on size, i.e., modelS). The rows of yHatSB predicted prices have a slope of about 136.32 ft2 (the coefficient on size from the regression of price on size and baths, i.e., modelSB). The yHatS line has a steeper slope than the rows of yHatSB predicted prices because the estimated effect of size on price is larger from the modelS that only controls for size than from the modelSB that also controls for baths. That slope is also less steep because size and the number of bathrooms positively affect price, and size and the number of bathrooms are themselves positively correlated (i.e., larger houses tend to have more bathrooms). 8) How far apart vertically in the y direction (the price direction) are the two rows of yHatSB predicted prices? Why? The higher row of yHatSB predicted prices are about $136.32 higher. This is the estimated effect of an additional bathroom on price (from the regression of price on size and baths). The lower and upper rows are for 1 and 2 bathroom houses consecutively. 9) Copy the code from the previous graph and then do the following: Using only the coefficients from modelSB and the size variable (and simple arithmetic), generate a variable name yHatSB1 that when you plot it, replaces the lower row of yHatSB predicted price points. Make sure that these points are only created for observations with 1 bathroom and are NA for other observation (I’d use ifelse() for baths==1). Add these to the graph as geom_point() and make these dots red. Also add a geom_abline() that goes through this row of dots and make this line red. Also using only the coefficients from modelSB and the size variable (and simple arithmetic), generate a variable name yHatSB2 that when you plot it, replaces the upper row of yHatSB predicted price points. Make sure that these points are only created for observations with 2 bathrooms and are NA for other observation (I’d use ifelse() for baths==2). Add these to the graph as geom_point() and make these dots blue. Also add a geom_abline() that goes through this row of dots and make this line blue. In addition, remove the orange yHatSB points you added before (because you’ve replaced them with red points and blue points). In mine, I also labeled the y-intercepts of the three lines (the black line that connects the yHatS points, the red line that connects the yHatSB1 points, and the blue line that connects the yHatSB2 points). Don’t waste much time trying to figure this out, but you should understand how the intercepts correspond with coefficients from the models. mydata$yHatSB1 &lt;- ifelse(mydata$baths == 1, coef(modelSB)[&quot;(Intercept)&quot;] + coef(modelSB)[&quot;baths&quot;]*1 + coef(modelSB)[&quot;size&quot;]*mydata$size, NA) mydata$yHatSB2 &lt;- ifelse(mydata$baths == 2, coef(modelSB)[&quot;(Intercept)&quot;] + coef(modelSB)[&quot;baths&quot;]*2 + coef(modelSB)[&quot;size&quot;]*mydata$size, NA) ## These labels the yAxis with the 3 y-intercept values, plus 0 and values from 200k to 600k in steps of 100k yLabels &lt;- c(0,unname(c(round(coef(modelS)[1],0),round(coef(modelSB)[&quot;(Intercept)&quot;] + coef(modelSB)[&quot;baths&quot;]*1,0),round(coef(modelSB)[&quot;(Intercept)&quot;] + coef(modelSB)[&quot;baths&quot;]*2,0))),seq(200000,600000,100000)) ggplot(mydata) + scale_x_continuous(expand = c(0, 0), limits = c(0, max(mydata$size)*1.02), breaks = seq(0,max(mydata$size)*1.02,500)) + scale_y_continuous(expand = c(0, 0), limits = c(0, max(mydata$price)*1.02), breaks = yLabels) + geom_point(aes(y=price,x=size,col=factor(baths)),shape=4) + geom_abline(intercept = coef(modelS)[&quot;(Intercept)&quot;], slope = coef(modelS)[&quot;size&quot;],col=&quot;black&quot;) + geom_point(data=mydata,aes(y=yHatS,x=size,col=factor(baths))) + geom_point(data=mydata,aes(y=yHatSB1,x=size),col=&quot;red&quot;) + geom_point(data=mydata,aes(y=yHatSB2,x=size),col=&quot;blue&quot;) + geom_abline(intercept = coef(modelSB)[&quot;(Intercept)&quot;] + coef(modelSB)[&quot;baths&quot;]*1, slope = coef(modelSB)[&quot;size&quot;],col=&quot;red&quot;) + geom_abline(intercept = coef(modelSB)[&quot;(Intercept)&quot;] + coef(modelSB)[&quot;baths&quot;]*2, slope = coef(modelSB)[&quot;size&quot;],col=&quot;blue&quot;) 10) Using ifelse(), create two dummy variables, baths1 and baths2, and add them to mydata. The variable baths1 equals 1 for houses with 1 bathroom and equals 0 otherwise. The variable baths2 equals 1 for houses with 2 bathroom and equals 0 otherwise. Make sure to look at the data after creating the variables to make sure you did it correctly (e.g., use head())! Calculate the mean of baths1 and baths2. What does the mean of baths1 tell us? What about the mean of baths2? mydata$baths1 &lt;- ifelse(mydata$baths == 1, 1, 0) mydata$baths2 &lt;- ifelse(mydata$baths == 2, 1, 0) head(mydata,10) ## price size beds baths yHatS yHatSB yHatSB1 yHatSB2 baths1 baths2 ## 1 427923 2211 3 2 390491.9 405408.3 NA 405408.3 0 1 ## 2 270778 1296 2 2 249240.8 280679.1 NA 280679.1 0 1 ## 3 329174 1525 2 2 284592.2 311895.4 NA 311895.4 0 1 ## 4 537281 2913 4 2 498861.6 501102.2 NA 501102.2 0 1 ## 5 275112 903 1 1 188572.3 185102.1 185102.1 NA 1 0 ## 6 390832 2302 4 2 404539.8 417813.1 NA 417813.1 0 1 ## 7 405488 2132 3 2 378296.5 394639.3 NA 394639.3 0 1 ## 8 402451 2052 2 1 365946.6 341729.3 341729.3 NA 1 0 ## 9 279597 1471 2 2 276256.1 304534.4 NA 304534.4 0 1 ## 10 337245 2252 3 1 396821.2 368992.5 368992.5 NA 1 0 mean(mydata$baths1) ## [1] 0.4722222 mean(mydata$baths2) ## [1] 0.5277778 47.2% of the houses from the data have 1 bathroom and 52.8% of the houses are 2 bathrooms. 11) Try estimating a regression (with price as the y variable) that includes size, baths1, and baths2. Call it model12. Display the output using pander, but also display coef(model12). What happens? Why? Hint: which of the 4 MLR assumptions is violated? model12 &lt;- lm(price~size+baths1+baths2, data = mydata) pander(summary(model12))   Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 104013 17658 5.89 0.00000001487 size 136.3 7.944 17.16 5.014e-42 baths1 -42005 7841 -5.357 0.0000002187 Fitting linear model: price ~ size + baths1 + baths2 Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 216 52094 0.6967 0.6938 The coefficient on baths2 is NA because it cannot include baths2 in the regression along with baths1. It cannot do OLS using both these variables because they are perfectly collinear, violating MLR.3. They are perfectly collinear because baths1 + baths2 = 1. 12) Since we cannot include both baths1 and baths2 in the regression, lets try again without baths1. Estimate a model (name it modeldummy) that includes size and baths2, but leave out baths1. What is the interpretation of \\(\\betahat{0}\\), \\(\\betahat{1}\\), and \\(\\betahat{2}\\)? modeldummy &lt;- lm(price~size+baths2, data = mydata) pander(summary(modeldummy))   Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 62009 14586 4.251 0.00003181 size 136.3 7.944 17.16 5.014e-42 baths2 42005 7841 5.357 0.0000002187 Fitting linear model: price ~ size + baths2 Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 216 52094 0.6967 0.6938 The intercept (62008.64) is the average house price for a house that is zero square feet in size and that is NOT a 2 bathroom house. The coefficient on size 136.32 is the expected increase in price for an additional square foot of size, holding constant whether or not the house has two bathrooms. Holding constant the size of the house, the average price of two bathroom houses is expected to be $42004.73 higher than houses that are not two bathroom houses. 13) Create the same graph you created above with the red and blue lines, except modify the geom_abline() layers that use coefficients from modelSB so that they use modeldummy instead. Leave everything else as it is in the previous graph (e.g., leave the black line geom_abline() that uses modelS, leave the geom_point() using yHatSB1 and yHatSB2). The graph itself should look identical (the two models are identical because the only possible values of baths are 1 and 2). Make sure that your red line (using geom_abline() based on modeldummy coefficients) actually goes through the red points (the geom_point() based on yHatSB1) and make sure that your blue line (using geom_abline() based on modeldummy coefficients) actually goes through the blue points (the geom_point() based on yHatSB2). ggplot(mydata) + scale_x_continuous(expand = c(0, 0), limits = c(0, max(mydata$size)*1.02), breaks = seq(0,max(mydata$size)*1.02,500)) + scale_y_continuous(expand = c(0, 0), limits = c(0, max(mydata$price)*1.02), breaks = yLabels) + geom_point(aes(y=price,x=size,col=factor(baths)),shape=4) + geom_abline(intercept = coef(modelS)[&quot;(Intercept)&quot;], slope = coef(modelS)[&quot;size&quot;],col=&quot;black&quot;) + geom_point(data=mydata,aes(y=yHatS,x=size,col=factor(baths))) + geom_point(data=mydata,aes(y=yHatSB1,x=size),col=&quot;red&quot;) + geom_point(data=mydata,aes(y=yHatSB2,x=size),col=&quot;blue&quot;) + geom_abline(intercept = coef(modeldummy)[&quot;(Intercept)&quot;] + coef(modeldummy)[&quot;baths2&quot;]*0, slope = coef(modeldummy)[&quot;size&quot;],col=&quot;red&quot;) + geom_abline(intercept = coef(modeldummy)[&quot;(Intercept)&quot;] + coef(modeldummy)[&quot;baths2&quot;]*1, slope = coef(modeldummy)[&quot;size&quot;],col=&quot;blue&quot;) 16.3 Slope dummy 14) Estimate a model that allows for the slope (with respect to size) to be different for 1 and 2 bathroom houses. Call it modelSlopeDummy. Display the results using pander. Create a graph similar to what you did above, except using this new model. Start with the previous graph and make the following changes: Remove the black line based on modelS. Remove the geom_point() based on modelS. Remove the red geom_point() based on yHatSB1 and replace it with red geom_point() based on modelSlopeDummy (I suggest creating a yHatlopeDummy1 similar to how you created yHatSB1). Remove the blue geom_point() based on yHatSB2 and replace it with blue geom_point() based on modelSlopeDummy (I suggest creating a yHatlopeDummy2 similar to how you created yHatSB2). Remove the red geom_abline() based on modeldummy and replace it with a red geom_abline() based on modelSlopeDummy. Remove the blue geom_abline() based on modeldummy and replace it with a red geom_abline() based on modelSlopeDummy. NOTE: you rarely want to estimate a model with a slope dummy unless you also have the corresponding intercept dummy…see the next question for that model # First, create a variable that multiplies size by baths2 mydata$sizeBaths2 &lt;- mydata$size * mydata$baths2 # Estimate the model using this new variable modelSlopeDummy &lt;- lm(price~size+sizeBaths2, data = mydata) pander(summary(modelSlopeDummy))   Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 82311 15618 5.27 0.0000003332 size 124.7 9.032 13.81 2.094e-31 sizeBaths2 21.57 3.984 5.415 0.0000001647 Fitting linear model: price ~ size + sizeBaths2 Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 216 52027 0.6975 0.6946 # predicted yHat points for not two bathroom houses mydata$yHatSlope1 &lt;- ifelse(mydata$baths2 == 0, coef(modelSlopeDummy)[&quot;(Intercept)&quot;] + coef(modelSlopeDummy)[&quot;size&quot;]*mydata$size ,NA) # predicted yHat points for two bathroom houses mydata$yHatSlope2 &lt;- ifelse(mydata$baths2 == 1, coef(modelSlopeDummy)[&quot;(Intercept)&quot;] + (coef(modelSlopeDummy)[&quot;size&quot;] + coef(modelSlopeDummy)[&quot;sizeBaths2&quot;]) *mydata$size ,NA) ## Add new intercepts to yLabels yLabelsSlope &lt;- sort(unname(c(yLabels, round(coef(modelSlopeDummy)[&quot;(Intercept)&quot;],0),round(coef(modelSB)[&quot;(Intercept)&quot;] + coef(modelSB)[&quot;baths&quot;]*1,0),round(coef(modelSB)[&quot;(Intercept)&quot;] + coef(modelSB)[&quot;baths&quot;]*2,0)))) ggplot(mydata) + scale_x_continuous(expand = c(0, 0), limits = c(0, max(mydata$size)*1.02), breaks = seq(0,max(mydata$size)*1.02,500)) + scale_y_continuous(expand = c(0, 0), limits = c(0, max(mydata$price)*1.02), breaks = yLabelsSlope) + geom_point(aes(y=price,x=size,col=factor(baths)),shape=4) + geom_point(data=mydata,aes(y=yHatSlope1,x=size),col=&quot;red&quot;) + geom_point(data=mydata,aes(y=yHatSlope2,x=size),col=&quot;blue&quot;) + geom_abline(intercept = coef(modelSlopeDummy)[&quot;(Intercept)&quot;], slope = coef(modelSlopeDummy)[&quot;size&quot;],col=&quot;red&quot;) + geom_abline(intercept = coef(modelSlopeDummy)[&quot;(Intercept)&quot;], slope = coef(modelSlopeDummy)[&quot;size&quot;] + coef(modelSlopeDummy)[&quot;sizeBaths2&quot;],col=&quot;blue&quot;) 16.4 Intercept and slope dummies 15) Estimate a model that allows for both the intercept and the slope (with respect to size) to be different for 1 and 2 bathroom houses. Call it modelSlopeAndInterceptDummies. Create a graph of this model by following the same steps you followed above to create the graph of the slope dummy model. # Estimate the model modelSlopeAndInterceptDummies &lt;- lm(price~baths2+size+sizeBaths2, data = mydata) pander(summary(modelSlopeAndInterceptDummies))   Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 75489 21120 3.574 0.0004349 baths2 15120 31443 0.4809 0.6311 size 128.5 11.92 10.77 7.231e-22 sizeBaths2 14.12 16 0.8829 0.3783 Fitting linear model: price ~ baths2 + size + sizeBaths2 Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 216 52121 0.6978 0.6935 # predicted yHat points for not two bathroom houses mydata$yHatSlopeAndIntercept1 &lt;- ifelse(mydata$baths2 == 0, coef(modelSlopeAndInterceptDummies)[&quot;(Intercept)&quot;] + coef(modelSlopeAndInterceptDummies)[&quot;size&quot;]*mydata$size ,NA) # predicted yHat points for two bathroom houses mydata$yHatSlopeAndIntercept2 &lt;- ifelse(mydata$baths2 == 1, coef(modelSlopeAndInterceptDummies)[&quot;(Intercept)&quot;] + coef(modelSlopeAndInterceptDummies)[&quot;baths2&quot;] + (coef(modelSlopeAndInterceptDummies)[&quot;size&quot;] + coef(modelSlopeAndInterceptDummies)[&quot;sizeBaths2&quot;]) *mydata$size ,NA) # Add new y intercepts to y axis labels yLabelsSlopeIntercept &lt;- sort(unname(c(yLabelsSlope,round(coef(modelSlopeAndInterceptDummies)[&quot;(Intercept)&quot;],0),round(coef(modelSlopeAndInterceptDummies)[&quot;(Intercept)&quot;]+coef(modelSlopeAndInterceptDummies)[&quot;baths2&quot;],0)))) ggplot(mydata) + scale_x_continuous(expand = c(0, 0), limits = c(0, max(mydata$size)*1.02), breaks = seq(0,max(mydata$size)*1.02,500)) + scale_y_continuous(expand = c(0, 0), limits = c(0, max(mydata$price)*1.02), breaks = yLabelsSlopeIntercept) + geom_point(aes(y=price,x=size,col=factor(baths)),shape=4) + # geom_abline(intercept = coef(modelS)[&quot;(Intercept)&quot;], # slope = coef(modelS)[&quot;size&quot;],col=&quot;black&quot;) + # geom_point(data=mydata,aes(y=yHatS,x=size,col=factor(baths))) + geom_point(data=mydata,aes(y=yHatSlopeAndIntercept1,x=size),col=&quot;red&quot;) + geom_point(data=mydata,aes(y=yHatSlopeAndIntercept2,x=size),col=&quot;blue&quot;) + geom_abline(intercept = coef(modelSlopeAndInterceptDummies)[&quot;(Intercept)&quot;], slope = coef(modelSlopeAndInterceptDummies)[&quot;size&quot;],col=&quot;red&quot;) + geom_abline(intercept = coef(modelSlopeAndInterceptDummies)[&quot;(Intercept)&quot;] + coef(modelSlopeAndInterceptDummies)[&quot;baths2&quot;], slope = coef(modelSlopeAndInterceptDummies)[&quot;size&quot;] + coef(modelSlopeAndInterceptDummies)[&quot;sizeBaths2&quot;],col=&quot;blue&quot;) 16) Compare the four models (modelS, modeldummy, modelSlopeDummy, and modelSlopeAndInterceptDummies). Display them all side-by-side using stargazer. Then write out the conditional expectations for all four models for one bathroom houses and two bathroom houses. stargazer(modelS,modeldummy,modelSlopeDummy,modelSlopeAndInterceptDummies, type = &quot;html&quot;, report=(&#39;vc*p&#39;), keep.stat = c(&quot;n&quot;,&quot;rsq&quot;,&quot;adj.rsq&quot;), notes = &quot;&lt;em&gt;&amp;#42;p&amp;lt;0.1;&amp;#42;&amp;#42;p&amp;lt;0.05;&amp;#42;&amp;#42;&amp;#42;p&amp;lt;0.01&lt;/em&gt;&quot;, notes.append = FALSE) Dependent variable: price (1) (2) (3) (4) size 154.373*** 136.316*** 124.733*** 128.467*** p = 0.000 p = 0.000 p = 0.000 p = 0.000 baths2 42,004.730*** 15,120.010 p = 0.00000 p = 0.632 sizeBaths2 21.572*** 14.123 p = 0.00000 p = 0.379 Constant 49,173.680*** 62,008.640*** 82,310.930*** 75,489.050*** p = 0.002 p = 0.00004 p = 0.00000 p = 0.0005 Observations 216 216 216 216 R2 0.656 0.697 0.697 0.698 Adjusted R2 0.654 0.694 0.695 0.694 Note: *p&lt;0.1;**p&lt;0.05;***p&lt;0.01 Here are the expected prices for 1 bathroom (not-two bathroom) houses \\[ \\begin{aligned} E(price|size,baths2=0) &amp;= 49174 + 154⋅size \\\\ E(price|size,baths2=0) &amp;= 62009+136⋅size \\\\ E(price|size,baths2=0) &amp;= 82311+125⋅size \\\\ E(price|size,baths2=0) &amp;= 75489+128⋅size \\end{aligned} \\] Here are the expected prices for two bathroom houses \\[ \\begin{aligned} E(price|size,baths2=1) &amp;= 49174+154⋅size \\\\ E(price|size,baths2=1) &amp;= 104013+136⋅size \\\\ E(price|size,baths2=1) &amp;= 82311+146⋅size \\\\ E(price|size,baths2=1) &amp;= 90609+143⋅size \\end{aligned} \\] 17) What do you notice about the intercepts and the slopes? Think about what variation each model allows and what restrictions it imposes. Why are the intercepts furthest out for the model with the intercept dummy (modeldummy), at one point in the middle for the model with the slope dummy, and in between for the model with both the intercept and slope dummies? How does that relate to the estimated slopes? How does that relate to the model that only includes size? I’d start by comparing the graphs of the models. yLabelsCombined &lt;- sort(c(0,unname(c(round(coef(modelS)[1],0),round(coef(modeldummy)[&quot;(Intercept)&quot;] + coef(modeldummy)[&quot;baths2&quot;]*0,0),round(coef(modeldummy)[&quot;(Intercept)&quot;] + coef(modeldummy)[&quot;baths2&quot;]*1,0),round(coef(modelSlopeDummy)[&quot;(Intercept)&quot;],0),round(coef(modelSlopeAndInterceptDummies)[&quot;(Intercept)&quot;] + coef(modelSlopeAndInterceptDummies)[&quot;baths2&quot;]*0,0),round(coef(modelSlopeAndInterceptDummies)[&quot;(Intercept)&quot;] + coef(modelSlopeAndInterceptDummies)[&quot;baths2&quot;]*1,0))),seq(200000,600000,100000))) ggplot(mydata) + scale_x_continuous(expand = c(0, 0), limits = c(0, max(mydata$size)*1.02), breaks = seq(0,max(mydata$size)*1.02,500)) + scale_y_continuous(expand = c(0, 0), limits = c(0, max(mydata$price)*1.02), breaks = yLabelsCombined) + geom_point(aes(y=price,x=size,col=factor(baths)),shape=4) + geom_abline(intercept = coef(modelS)[&quot;(Intercept)&quot;], slope = coef(modelS)[&quot;size&quot;],col=&quot;black&quot;) + geom_abline(intercept = coef(modeldummy)[&quot;(Intercept)&quot;] + coef(modeldummy)[&quot;baths2&quot;]*0, slope = coef(modeldummy)[&quot;size&quot;],col=&quot;red&quot;) + geom_abline(intercept = coef(modeldummy)[&quot;(Intercept)&quot;] + coef(modeldummy)[&quot;baths2&quot;]*1, slope = coef(modeldummy)[&quot;size&quot;],col=&quot;red&quot;) + geom_abline(intercept = coef(modelSlopeDummy)[&quot;(Intercept)&quot;], slope = coef(modelSlopeDummy)[&quot;size&quot;],col=&quot;blue&quot;) + geom_abline(intercept = coef(modelSlopeDummy)[&quot;(Intercept)&quot;], slope = coef(modelSlopeDummy)[&quot;size&quot;] + coef(modelSlopeDummy)[&quot;sizeBaths2&quot;],col=&quot;blue&quot;,show.legend = TRUE) + geom_abline(intercept = coef(modelSlopeAndInterceptDummies)[&quot;(Intercept)&quot;], slope = coef(modelSlopeAndInterceptDummies)[&quot;size&quot;],col=&quot;purple&quot;) + geom_abline(intercept = coef(modelSlopeAndInterceptDummies)[&quot;(Intercept)&quot;] + coef(modelSlopeAndInterceptDummies)[&quot;baths2&quot;], slope = coef(modelSlopeAndInterceptDummies)[&quot;size&quot;] + coef(modelSlopeAndInterceptDummies)[&quot;sizeBaths2&quot;],col=&quot;purple&quot;,show.legend = TRUE) + labs(title=&quot;Lines from All 4 Models&quot;, color = &quot;baths&quot;) The black line is from the first model that only includes size. The red lines are the model that includes baths2 as an intercept dummy only. These red lines are the most spread out because bathrooms is only accounted for by different intercepts. The blue lines are from the model that only allows the slope to differ. Forcing the intercept to be the same exaggerates the two slopes (think of taking the red lines and pulling the intercepts together…to still fit thought the data, the one bathroom line gets flatter and the two bathroom line gets steeper). Finally, the purple lines are for the model that allows both the intercept and slope to depend on if the house has 2 bathrooms. These intercepts lie between the other extremes, the model that only reflects bathrooms in the intercepts and the model that only reflects bathrooms in the slopes. 16.5 Models with the number of bedrooms 18) To help you gain additional intuition for what’s going on in linear regressions, try estimating a model that includes size and the number of bedrooms (beds). Try making a plot that includes yHat predicted values. Try coloring the yHat points based on the number of bedrooms. Why are the yHat points arranged in rows? How many rows are there? Why? Can you add geom_abline()s that connect the rows of dots? mydata %&gt;% count(beds) ## beds n ## 1 1 32 ## 2 2 66 ## 3 3 68 ## 4 4 50 modelBeds &lt;- lm(data=mydata,price~size+beds) pander(summary(modelBeds))   Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 69942 17390 4.022 0.00008013 size 113.8 18.4 6.184 0.000000003146 beds 22009 9103 2.418 0.01645 Fitting linear model: price ~ size + beds Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 216 54747 0.665 0.6619 ## Calculate &quot;yHat&quot; predicted points using fitted() function mydata$yHatBeds &lt;- fitted(modelBeds) yLabelsBeds &lt;- c(0,unname(c(round(coef(modelBeds)[&quot;(Intercept)&quot;] + coef(modelBeds)[&quot;beds&quot;]*1,0),round(coef(modelBeds)[&quot;(Intercept)&quot;] + coef(modelBeds)[&quot;beds&quot;]*2,0),round(coef(modelBeds)[&quot;(Intercept)&quot;] + coef(modelBeds)[&quot;beds&quot;]*3,0),round(coef(modelBeds)[&quot;(Intercept)&quot;] + coef(modelBeds)[&quot;beds&quot;]*4,0))),seq(200000,600000,100000)) ggplot(mydata) + scale_x_continuous(expand = c(0, 0), limits = c(0, max(mydata$size)*1.02), breaks = seq(0,max(mydata$size)*1.02,500)) + scale_y_continuous(expand = c(0, 0), limits = c(0, max(mydata$price)*1.02), breaks=yLabelsBeds) + geom_point(aes(y=price,x=size,col=factor(beds)),shape=4) + geom_abline(intercept = coef(modelBeds)[&quot;(Intercept)&quot;] + coef(modelBeds)[&quot;beds&quot;]*1, slope = coef(modelBeds)[&quot;size&quot;],col=&quot;red&quot;) + geom_abline(intercept = coef(modelBeds)[&quot;(Intercept)&quot;] + coef(modelBeds)[&quot;beds&quot;]*2, slope = coef(modelBeds)[&quot;size&quot;],col=&quot;orange&quot;) + geom_abline(intercept = coef(modelBeds)[&quot;(Intercept)&quot;] + coef(modelBeds)[&quot;beds&quot;]*3, slope = coef(modelBeds)[&quot;size&quot;],col=&quot;green&quot;) + geom_abline(intercept = coef(modelBeds)[&quot;(Intercept)&quot;] + coef(modelBeds)[&quot;beds&quot;]*4, slope = coef(modelBeds)[&quot;size&quot;],col=&quot;blue&quot;) + geom_point(aes(y=yHatBeds,x=size,col=factor(beds))) + scale_color_manual(values=c(&quot;red&quot;, &quot;orange&quot;, &quot;green&quot;,&quot;blue&quot;)) The houses have either 1, 2, 3, or 4 bedrooms so the yHat points should be arranged in 4 rows. 19) Now try changing the previous model to include factor(beds) instead of beds (alternatively, create a dummy variable for 2 bedroom houses, 3 bedroom houses, and 4 bedroom houses, and include these three dummy variables in the model). Try making a plot that includes yHat predicted values. Try coloring the yHat points based on the number of bedrooms. Why are the yHat points arranged in rows? How many rows are there? Why? Can you add geom_abline()s that connect the rows of dots? How doe the rows of dots (and the geom_abline()s that connect them) compare with the previous model? modelBedsDummies &lt;- lm(data=mydata,price~size+factor(beds)) pander(summary(modelBedsDummies))   Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 98518 25537 3.858 0.000152 size 111.7 18.75 5.956 0.00000001066 factor(beds)2 17174 13691 1.254 0.2111 factor(beds)3 39851 19369 2.057 0.04087 factor(beds)4 67649 27898 2.425 0.01616 Fitting linear model: price ~ size + factor(beds) Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 216 54947 0.6657 0.6594 ## Calculate &quot;yHat&quot; predicted points using fitted() function mydata$yHatBedsDummies &lt;- fitted(modelBedsDummies) yLabelsBedsDummies &lt;- c(0,unname(c(round(coef(modelBedsDummies)[&quot;(Intercept)&quot;],0),round(coef(modelBedsDummies)[&quot;(Intercept)&quot;] + coef(modelBedsDummies)[&quot;factor(beds)2&quot;],0),round(coef(modelBedsDummies)[&quot;(Intercept)&quot;] + coef(modelBedsDummies)[&quot;factor(beds)3&quot;],0),round(coef(modelBedsDummies)[&quot;(Intercept)&quot;] + coef(modelBedsDummies)[&quot;factor(beds)4&quot;],0))),seq(200000,600000,100000)) ggplot(mydata) + scale_x_continuous(expand = c(0, 0), limits = c(0, max(mydata$size)*1.02), breaks = seq(0,max(mydata$size)*1.02,500)) + scale_y_continuous(expand = c(0, 0), limits = c(0, max(mydata$price)*1.02), breaks = yLabelsBedsDummies) + geom_point(aes(y=price,x=size,col=factor(beds)),shape=4) + geom_abline(intercept = coef(modelBedsDummies)[&quot;(Intercept)&quot;], slope = coef(modelBedsDummies)[&quot;size&quot;],col=&quot;red&quot;) + geom_abline(intercept = coef(modelBedsDummies)[&quot;(Intercept)&quot;] + coef(modelBedsDummies)[&quot;factor(beds)2&quot;], slope = coef(modelBedsDummies)[&quot;size&quot;],col=&quot;orange&quot;) + geom_abline(intercept = coef(modelBedsDummies)[&quot;(Intercept)&quot;] + coef(modelBedsDummies)[&quot;factor(beds)3&quot;], slope = coef(modelBedsDummies)[&quot;size&quot;],col=&quot;green&quot;) + geom_abline(intercept = coef(modelBedsDummies)[&quot;(Intercept)&quot;] + coef(modelBedsDummies)[&quot;factor(beds)4&quot;], slope = coef(modelBedsDummies)[&quot;size&quot;],col=&quot;blue&quot;) + geom_point(aes(y=yHatBedsDummies,x=size,col=factor(beds))) + scale_color_manual(values=c(&quot;red&quot;, &quot;orange&quot;, &quot;green&quot;,&quot;blue&quot;)) The difference in predicted price for the different number of bedrooms in the last model is the same, meaning the gap between each colored line is the same. In the first model that includes beds, the coefficient on beds is 22009.1, so that is the vertical gap between each of the 4 lines. In this model, the gap can be different. Looking at the graph, it looks like the jump from 1 to 2 is smaller than for 2 to 3, and smaller still than for 3 to 4. In the model that includes 2-bedroom, 3-bedroom, and 4-bedroom as dummy variables, the gaps are different. Going from 1 to 2 bedrooms is an increase of 17174.45. Going from 2 to 3 bedrooms is an increase of 22676.43 (the coefficient on the 3 bedroom dummy variable minus the coefficient on the 2 bedroom dummy variable). Going from 3 to 4 bedrooms is an increase of 27798.32 (the coefficient on the 4-bedroom dummy variable minus the coefficient on the 3-bedroom dummy variable). 20) Now let’s look at models with the number of bathrooms in addition to size and the number of bedrooms. For this model, include bedrooms as beds. Try to answer the same questions as the first model with bedrooms. Try making a plot that includes yHat predicted values. Try coloring the yHat points based on the number of bedrooms. Why are the yHat points arranged in rows? How many rows are there? Why? Can you add geom_abline()s that connect the rows of dots? modelBedsAndBaths &lt;- lm(data=mydata,price~size+beds+baths) pander(summary(modelBedsAndBaths))   Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 38729 17445 2.22 0.02747 size 101.8 17.5 5.815 0.00000002211 beds 19000 8603 2.209 0.02828 baths 40858 7788 5.246 0.0000003751 Fitting linear model: price ~ size + beds + baths Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 216 51626 0.7035 0.6993 ## Calculate &quot;yHat&quot; predicted points using fitted() function mydata$yHatBedsAndBaths &lt;- fitted(modelBedsAndBaths) ggplot(mydata) + scale_x_continuous(expand = c(0, 0), limits = c(0, max(mydata$size)*1.02), breaks = seq(0,max(mydata$size)*1.02,500)) + scale_y_continuous(expand = c(0, 0), limits = c(0, max(mydata$price)*1.02)) + geom_point(aes(y=price,x=size,col=factor(beds)),shape=4) + geom_abline(intercept = coef(modelBedsAndBaths)[&quot;(Intercept)&quot;] + coef(modelBedsAndBaths)[&quot;beds&quot;]*1 + coef(modelBedsAndBaths)[&quot;baths&quot;]*1, slope = coef(modelBedsAndBaths)[&quot;size&quot;],col=&quot;red&quot;) + geom_abline(intercept = coef(modelBedsAndBaths)[&quot;(Intercept)&quot;] + coef(modelBedsAndBaths)[&quot;beds&quot;]*1 + coef(modelBedsAndBaths)[&quot;baths&quot;]*2, slope = coef(modelBedsAndBaths)[&quot;size&quot;],col=&quot;red&quot;) + geom_abline(intercept = coef(modelBedsAndBaths)[&quot;(Intercept)&quot;] + coef(modelBedsAndBaths)[&quot;beds&quot;]*2 + coef(modelBedsAndBaths)[&quot;baths&quot;]*1, slope = coef(modelBedsAndBaths)[&quot;size&quot;],col=&quot;orange&quot;) + geom_abline(intercept = coef(modelBedsAndBaths)[&quot;(Intercept)&quot;] + coef(modelBedsAndBaths)[&quot;beds&quot;]*2 + coef(modelBedsAndBaths)[&quot;baths&quot;]*2, slope = coef(modelBedsAndBaths)[&quot;size&quot;],col=&quot;orange&quot;) + geom_abline(intercept = coef(modelBedsAndBaths)[&quot;(Intercept)&quot;] + coef(modelBedsAndBaths)[&quot;beds&quot;]*3 + coef(modelBedsAndBaths)[&quot;baths&quot;]*1, slope = coef(modelBedsAndBaths)[&quot;size&quot;],col=&quot;green&quot;) + geom_abline(intercept = coef(modelBedsAndBaths)[&quot;(Intercept)&quot;] + coef(modelBedsAndBaths)[&quot;beds&quot;]*3 + coef(modelBedsAndBaths)[&quot;baths&quot;]*2, slope = coef(modelBedsAndBaths)[&quot;size&quot;],col=&quot;green&quot;) + geom_abline(intercept = coef(modelBedsAndBaths)[&quot;(Intercept)&quot;] + coef(modelBedsAndBaths)[&quot;beds&quot;]*4 + coef(modelBedsAndBaths)[&quot;baths&quot;]*1, slope = coef(modelBedsAndBaths)[&quot;size&quot;],col=&quot;blue&quot;) + geom_abline(intercept = coef(modelBedsAndBaths)[&quot;(Intercept)&quot;] + coef(modelBedsAndBaths)[&quot;beds&quot;]*4 + coef(modelBedsAndBaths)[&quot;baths&quot;]*2, slope = coef(modelBedsAndBaths)[&quot;size&quot;],col=&quot;blue&quot;) + geom_point(aes(y=yHatBedsAndBaths,x=size,col=factor(beds))) + scale_color_manual(values=c(&quot;red&quot;, &quot;orange&quot;, &quot;green&quot;,&quot;blue&quot;)) There would be 8 combinations corresponding to 4 types of bedrooms with either 1 or 2 bathrooms which resulting in 8 rows of yHat points. For each value of the number of bedrooms, the gap between the lower and upper yHat rows is the coefficient on baths (i.e., the vertical gap is the value of having 2 bathrooms instead of 1). 21) Finally, try adding baths to the model with factor(beds). If you understood the previous two questions, you should have no problem understanding this question too. If you didn’t, make sure you understand those models first before trying to wrap your head around this one. modelBedsDummiesAndBaths &lt;- lm(data=mydata,price~size+factor(beds)+baths) pander(summary(modelBedsDummiesAndBaths))   Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 67253 24763 2.716 0.00716 size 98.51 17.83 5.526 0.0000000964 factor(beds)2 10914 12946 0.8431 0.4001 factor(beds)3 32158 18296 1.758 0.08026 factor(beds)4 58536 26326 2.224 0.02725 baths 41370 7822 5.289 0.0000003082 Fitting linear model: price ~ size + factor(beds) + baths Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 216 51740 0.705 0.698 ## Calculate &quot;yHat&quot; predicted points using fitted() function mydata$yHatBedsDummiesAndBaths &lt;- fitted(modelBedsDummiesAndBaths) ggplot(mydata) + scale_x_continuous(expand = c(0, 0), limits = c(0, max(mydata$size)*1.02), breaks = seq(0,max(mydata$size)*1.02,500)) + scale_y_continuous(expand = c(0, 0), limits = c(0, max(mydata$price)*1.02)) + geom_point(aes(y=price,x=size,col=factor(beds)),shape=4) + geom_abline(intercept = coef(modelBedsDummiesAndBaths)[&quot;(Intercept)&quot;] + coef(modelBedsDummiesAndBaths)[&quot;baths&quot;]*1, slope = coef(modelBedsDummiesAndBaths)[&quot;size&quot;],col=&quot;red&quot;) + geom_abline(intercept = coef(modelBedsDummiesAndBaths)[&quot;(Intercept)&quot;] + coef(modelBedsDummiesAndBaths)[&quot;baths&quot;]*2, slope = coef(modelBedsDummiesAndBaths)[&quot;size&quot;],col=&quot;red&quot;) + geom_abline(intercept = coef(modelBedsDummiesAndBaths)[&quot;(Intercept)&quot;] + coef(modelBedsDummiesAndBaths)[&quot;factor(beds)2&quot;] + coef(modelBedsDummiesAndBaths)[&quot;baths&quot;]*1, slope = coef(modelBedsDummiesAndBaths)[&quot;size&quot;],col=&quot;orange&quot;) + geom_abline(intercept = coef(modelBedsDummiesAndBaths)[&quot;(Intercept)&quot;] + coef(modelBedsDummiesAndBaths)[&quot;factor(beds)2&quot;] + coef(modelBedsDummiesAndBaths)[&quot;baths&quot;]*2, slope = coef(modelBedsDummiesAndBaths)[&quot;size&quot;],col=&quot;orange&quot;) + geom_abline(intercept = coef(modelBedsDummiesAndBaths)[&quot;(Intercept)&quot;] + coef(modelBedsDummiesAndBaths)[&quot;factor(beds)3&quot;] + coef(modelBedsDummiesAndBaths)[&quot;baths&quot;]*1, slope = coef(modelBedsDummiesAndBaths)[&quot;size&quot;],col=&quot;green&quot;) + geom_abline(intercept = coef(modelBedsDummiesAndBaths)[&quot;(Intercept)&quot;] + coef(modelBedsDummiesAndBaths)[&quot;factor(beds)3&quot;] + coef(modelBedsDummiesAndBaths)[&quot;baths&quot;]*2, slope = coef(modelBedsDummiesAndBaths)[&quot;size&quot;],col=&quot;green&quot;) + geom_abline(intercept = coef(modelBedsDummiesAndBaths)[&quot;(Intercept)&quot;] + coef(modelBedsDummiesAndBaths)[&quot;factor(beds)4&quot;] + coef(modelBedsDummiesAndBaths)[&quot;baths&quot;]*1, slope = coef(modelBedsDummiesAndBaths)[&quot;size&quot;],col=&quot;blue&quot;) + geom_abline(intercept = coef(modelBedsDummiesAndBaths)[&quot;(Intercept)&quot;] + coef(modelBedsDummiesAndBaths)[&quot;factor(beds)4&quot;] + coef(modelBedsDummiesAndBaths)[&quot;baths&quot;]*2, slope = coef(modelBedsDummiesAndBaths)[&quot;size&quot;],col=&quot;blue&quot;) + geom_point(aes(y=yHatBedsDummiesAndBaths,x=size,col=factor(beds))) + scale_color_manual(values=c(&quot;red&quot;, &quot;orange&quot;, &quot;green&quot;,&quot;blue&quot;)) This model is just like the previous one, except the increase for each consecutive number of bedrooms isn’t the same because 2-bedroom, 3-bedroom, and 4-bedroom are included as dummy variables. "],["fixed-effects.html", "17 Fixed Effects 17.1 Variables 17.2 OLS 17.3 Country Fixed Effects 17.4 Year Fixed Effects 17.5 Country and Year Fixed Effects 17.6 Comparison of all models 17.7 Data Summary by Country 17.8 Bookdown Style Note", " 17 Fixed Effects Solutions This file demonstrates three approaches to estimating “fixed effects” models (remember this is what economists call “fixed effects,” but other disciplines use “fixed effects” to refer to something different). We’re going to use the wbstats package to download country-level data from the World Bank for 2015 - 2018 (the most recently available for the variables I chose). We’ll then estimate models with country fixed effects, with year fixed effects, and with both country and year fixed effects. We’ll estimate each model three ways: using the within transformation, using dummy variables, and using the plm package to apply the within transformation for us. If you don’t know what these are, go through LN5 first. Note that the model we’ll estimate isn’t a great model in terms of producing interesting, reliable result. This is by design. Part of this chapter is introducing you to another API you can use to get data. You’ve worked with the US Census Bureau’s API to get data on US counties. Now you have experience getting data on countries from the World Bank. You are free to use either source for data for your RP. If the model here was a great one, it might take away something you want to do for your RP. This way you get experience with fixed effects models and with getting data from the World Bank, but don’t waste any potential ideas you might have for your RP. So just don’t read too much into the results. They like suffer from reverse causation and omitted variable bias (both violations of ZCM). If you’re interested in cross-country variation in life expectancy you’ll need to dig much deeper than we’ll go in this chapter. library(wbstats) # To get data from the World Bank API library(plm) # To estimate fixed effects models library(formattable) # to make colorful tables similar to Excel&#39;s conditional formatting library(stargazer) library(tidyverse) ## Set how many decimals at which R starts to use scientific notation options(scipen=3) # This shows you a lot of what&#39;s available from the World Bank API ## listWBinfo &lt;- wb_cachelist # List of countries in the World Bank data countryList &lt;- wb_countries() # List of all available variables (what they call &quot;indicators&quot;) availableIndicators &lt;- wb_cachelist$indicators ## Sometimes it&#39;s easier to look through the indicator list if you write it to CSV and open it in Excel (you can do the same thing with the US Census data). The following does that: # write.csv(select(availableIndicators,indicator_id, indicator, indicator_desc),&quot;indicators.csv&quot;) ## NOTE: if you use any of these (the full list of variables, exporting it to CSV), make sure you do NOT leave that in your final code. It doesn&#39;t belong in your RMD file. I just put these thigns in here so you see how to get to this information. ## We&#39;ll use the following variables: # SP.DYN.LE00.IN Life expectancy at birth, total (years) # NY.GDP.PCAP.KD GDP per capita (constant 2016 US$) # SP.POP.TOTL Population, total # SP.POP.TOTL.FE.ZS Population, female (% of total population) # SP.RUR.TOTL.ZS Rural population (% of total population) ## Create named vector of indicators to download indicatorsToDownload &lt;- c( lifeExp = &quot;SP.DYN.LE00.IN&quot;, gdpPerCapita =&quot;NY.GDP.PCAP.KD&quot;, pop = &quot;SP.POP.TOTL&quot;, pctFemale = &quot;SP.POP.TOTL.FE.ZS&quot;, pctRural = &quot;SP.RUR.TOTL.ZS&quot; ) ## Download descriptions of on World Bank indicators (i.e., variables) indicatorInfo &lt;- availableIndicators %&gt;% filter(indicator_id %in% indicatorsToDownload) ## Build description of our variables that we&#39;ll output in the HTML body sDesc &lt;- &quot;&quot; for(i in 1:nrow(indicatorInfo)){ sDesc &lt;- paste0(sDesc ,&quot;&lt;b&gt;&quot;, indicatorInfo$indicator[i], &quot; (&quot;,indicatorInfo$indicator_id[i] , &quot;)&lt;/b&gt;: &quot; ,indicatorInfo$indicator_desc[i] ,&quot;&lt;br&gt;&quot;) } ## Download data mydataOrig &lt;- wb_data(indicatorsToDownload, start_date = 2015, end_date = 2018) ## get vector of TRUE and FALSE where FALSE indicates there&#39;s one or more NA noNAs &lt;- complete.cases(mydataOrig) ## When writing this code, I first checked how many rows do have NAs, and then out of how many rows # sum(noNAs) ## out of how many rows: # nrow(noNAs) ## keep rows without any NA mydata &lt;- mydataOrig[noNAs,] ## get count of rows for each country countOfYearsByCountry &lt;- mydata %&gt;% count(country) ## merge the count variable with the data mydata &lt;- inner_join(mydata,countOfYearsByCountry, by=&quot;country&quot;) ## keep only countries that have all 4 years complete mydata &lt;- mydata %&gt;% filter(n==4) ## drop count variable (since all are now 4) mydata &lt;- mydata %&gt;% select(-n) ## For the purposes of this chapter, lets only examine one group of countries ## so that we can output results without it taking up hundreds of lines ## If this weren&#39;t a BP chapter we wouldn&#39;t do this ## Merge in country info (e.g., region) mydata &lt;- inner_join(mydata,select(countryList,country,region),by=&quot;country&quot;) ## Keep only region &quot;Latin America &amp; Caribbean&quot; (so we end up with only 31 countries) mydata &lt;- mydata %&gt;% filter(region == &quot;Latin America &amp; Caribbean&quot;) %&gt;% select(-region) mydata &lt;- mydata %&gt;% rename(year=date) ## Change scale of variables. This re-scales regression coefficients (instead of getting 0.00000123) #### Measure population in millions of people instead of people #### Measure GDP per Capita in thousands of 2010 US $ (instead of 2010 US $) mydata &lt;- mydata %&gt;% mutate(pop=pop/1000000, gdpPerCapita=gdpPerCapita/1000) mydata %&gt;% select(lifeExp, gdpPerCapita, pop, pctFemale, pctRural) %&gt;% as.data.frame() %&gt;% stargazer(., type = &quot;html&quot;,summary.stat = c(&quot;n&quot;,&quot;mean&quot;,&quot;sd&quot;, &quot;min&quot;, &quot;p25&quot;, &quot;median&quot;, &quot;p75&quot;, &quot;max&quot;)) Statistic N Mean St. Dev. Min Pctl(25) Median Pctl(75) Max lifeExp 128 74.921 3.455 62.485 73.244 74.910 77.174 80.095 gdpPerCapita 128 10.659 8.200 1.387 5.321 8.248 13.817 35.074 pop 128 18.791 41.175 0.094 0.516 5.611 12.407 209.469 pctFemale 128 50.656 0.931 49.071 49.965 50.614 51.131 53.114 pctRural 128 35.487 21.253 4.279 19.814 33.622 47.630 81.485 17.1 Variables Variable descriptions from the World Bank API. These descriptions do not reflect two changes we made (that are reflected in the table of summary statistics above and in the regression results that follow): population is measured in millions of people and GDP per capita is measured in thousands of 2010 US dollars. GDP per capita (constant 2010 US$) (NY.GDP.PCAP.KD): GDP per capita is gross domestic product divided by midyear population. GDP is the sum of gross value added by all resident producers in the economy plus any product taxes and minus any subsidies not included in the value of the products. It is calculated without making deductions for depreciation of fabricated assets or for depletion and degradation of natural resources. Data are in constant 2010 U.S. dollars.Life expectancy at birth, total (years) (SP.DYN.LE00.IN): Life expectancy at birth indicates the number of years a newborn infant would live if prevailing patterns of mortality at the time of its birth were to stay the same throughout its life.Population, total (SP.POP.TOTL): Total population is based on the de facto definition of population, which counts all residents regardless of legal status or citizenship. The values shown are midyear estimates.Population, female (% of total population) (SP.POP.TOTL.FE.ZS): Female population is the percentage of the population that is female. Population is based on the de facto definition of population, which counts all residents regardless of legal status or citizenship.Rural population (% of total population) (SP.RUR.TOTL.ZS): Rural population refers to people living in rural areas as defined by national statistical offices. It is calculated as the difference between total population and urban population. Source of data definitions: wbstats package (Note that this is not how you would describe your variables in a paper, but for the purposes of this assignment, it’s an easy way to get an accurate description of each variable) 17.2 OLS Our focus is fixed effects models, but often when estimating fixed effects models we also estimate regular OLS without fixed effects for comparison. ols &lt;- lm(data=mydata,lifeExp~gdpPerCapita+pop+pctFemale+pctRural) 17.3 Country Fixed Effects Our basic OLS model is the following: \\[ lifeExp_{it} = \\beta_0+\\beta_1 gdpPerCapita_{it} + \\beta_2 pop_{it} + \\beta_3 pctFemale_{it} + \\beta_4 pctRural_{it} + v_{it} \\] To save on notation, we’ll use generic variables (and I wrote out the “composite error term”), i.e., \\[ y_{it} = \\beta_0+\\beta_1 x_{1it} + \\beta_2 x_{2it} + \\beta_3 x_{2it} + \\beta_4 x_{4it} + (c_i + u_{it}) \\] Below there are 3 equations. The first is for country \\(i\\) in year \\(t\\) (the same as the one above). The second equation is the average over the four years for country \\(i\\), where \\(\\bar{y}_{i}=\\sum_{t=2015}^{2018}y_{it}\\) is the average value of \\(y_{it}\\) over the 4 years for country \\(i\\), \\(\\bar{x}_{ji}=\\sum_{t=2015}^{2018}x_{jti}\\) is the average value of \\(x_{jti}\\) over the 4 years for country \\(i\\) for the four explanatory variables \\(j\\in\\{1,2,3,4\\}\\), \\(\\bar{c}_{i}=\\sum_{t=2015}^{2018}c_{i}=c_i\\) is the average value of \\(c_{i}\\) over the 4 years for country \\(i\\) (which just equals \\(c_i\\) because \\(c_i\\) is the same in all years for country \\(i\\)), and \\(\\bar{u}_{i}=\\sum_{t=2015}^{2018}u_{it}\\) is the average value of \\(u_{it}\\) over the 4 years for country \\(i\\). For the final equation, subtract country \\(i\\)’s average from the value in each year \\(t\\). \\[ \\begin{align} y_{it} &amp;= \\beta_0+\\beta_1 x_{1it} + \\beta_2 x_{2it} + \\beta_3 x_{2it} + \\beta_4 x_{4it} + (c_i + u_{it}) \\\\ \\bar{y}_{i} &amp;= \\beta_0+\\beta_1 \\bar{x}_{1i} + \\beta_2 \\bar{x}_{2i} + \\beta_3 \\bar{x}_{3i} + \\beta_4 \\bar{x}_{4i} + (\\bar{c}_i + \\bar{u}_{i}) \\\\ y_{it}-\\bar{y}_{i} &amp;= (\\beta_0-\\beta_0)+\\beta_1 (x_{1it}-\\bar{x}_{1i}) + \\beta_2 (x_{2it}-\\bar{x}_{2i}) + \\beta_3 (x_{3it}-\\bar{x}_{3i}) \\\\ &amp;\\hspace{6cm} + \\beta_4 (x_{4it}-\\bar{x}_{4i}) + (c_i-\\bar{c}_i + u_{it}-\\bar{u}_{i}) \\end{align} \\] This final equation simplifies to the “within transformation” for country \\(i\\), \\[ y_{it}-\\bar{y}_{i} = \\beta_1 (x_{1it}-\\bar{x}_{1i}) + \\beta_2 (x_{2it}-\\bar{x}_{2i}) + \\beta_3 (x_{3it}-\\bar{x}_{3i}) + \\beta_4 (x_{4it}-\\bar{x}_{4i}) + (u_{it}-\\bar{u}_{i}) \\] because \\(\\beta_0-\\beta_0=0\\) and \\(c_i-\\bar{c}_i=0\\), where \\(\\bar{c}_i=c_i\\) because \\(c_i\\) is the same in all years for country \\(i\\). Mathematically, this is why the fixed effects model allows us to control for unobservable factors that do not change of time (or whatever is measured by \\(t=1,,,,.T\\)). If \\(c_i\\) is not constant for all time periods, then \\(\\bar{c}_i=c_i\\) isn’t correct and it doesn’t drop out of the final equation. That means it remains in the equations we estimate, and our coefficients are biased. At the end of this file there are tables that demonstrate the within transformation for our dataset. There is a table for each variable. Look at the table for Life expectancy. Find the row for Argentina (iso3c code ARG). It’s average value of life expectancy is 76.3. In 2015, their value was 76.07, which is 0.23 below Argentina’s four-year average value of 76.3. In 2016, Argentina’s life expectancy was 76.22, which is 0.08 above Argentina’s four-year average. Below this table for life expectancy is a similar table for each explanatory variable. When economists say a model has country “fixed effects,” they mean estimating an OLS regression using data transformed by this “within” transformation. Alternatively, a model with country “fixed effects” can be estimated using the original OLS equation with the addition of a dummy variable for each country (omitting one). \\[ y_{it} = \\beta_0+\\beta_1 x_{1it} + \\beta_2 x_{2it} + \\beta_3 x_{2it} + \\beta_4 x_{4it} + \\sum_{i=2}^{50}\\sigma_idC_i + (c_i + u_{it}) \\] where \\(dC_i\\) is a dummy variable with a value of 1 if that observation is country \\(i\\) and equals 0 otherwise (and \\(\\sigma_i\\) is the coefficient on dummy variable \\(dC_i\\)). These two models, the “within transformation” and the model with a dummy variable for each country, are mathematically and empirically equivalent. To see that they are empirically equivalent, we’ll estimate both models and compare the results. Note that the standard errors and \\(R^2\\) values are not equivalent, as discussed below. ## Dummy variable for each country (it automatically omits one) countryDummies &lt;- lm(lifeExp~gdpPerCapita+pop+pctFemale+pctRural+factor(country),data=mydata) ## Within transformation (subtract country averages from each observation for each variable) ## Create Country Averages mydata &lt;- mydata %&gt;% group_by(country) %&gt;% mutate(cAvg_lifeExp=mean(lifeExp), cAvg_gdpPerCapita=mean(gdpPerCapita), cAvg_pop=mean(pop), cAvg_pctFemale=mean(pctFemale), cAvg_pctRural=mean(pctRural) ) %&gt;% ungroup() ## Within transformation mydataCountry &lt;- mydata %&gt;% mutate(lifeExp=lifeExp-cAvg_lifeExp, gdpPerCapita=gdpPerCapita-cAvg_gdpPerCapita, pop=pop-cAvg_pop, pctFemale=pctFemale-cAvg_pctFemale, pctRural=pctRural-cAvg_pctRural ) %&gt;% ungroup() ## Estimate within transformation using the transformed data countryWithin &lt;- lm(lifeExp~gdpPerCapita+pop+pctFemale+pctRural,data=mydataCountry) ## Using plm package countryPlm &lt;- plm(lifeExp~gdpPerCapita+pop+pctFemale+pctRural,data=mydata, index=c(&quot;country&quot;), model = &quot;within&quot;, effect=&quot;individual&quot;) stargazer(countryDummies,countryWithin,countryPlm, type = &quot;html&quot;, report=(&#39;vcs*&#39;), single.row = TRUE, digits = 4, keep.stat = c(&quot;n&quot;,&quot;rsq&quot;,&quot;adj.rsq&quot;), notes = &quot;Standard Errors reported in parentheses, &lt;em&gt;&amp;#42;p&amp;lt;0.1;&amp;#42;&amp;#42;p&amp;lt;0.05;&amp;#42;&amp;#42;&amp;#42;p&amp;lt;0.01&lt;/em&gt;&quot;, notes.append = FALSE) Dependent variable: lifeExp OLS panel linear (1) (2) (3) gdpPerCapita 0.0651 (0.0533) 0.0651 (0.0461) 0.0651 (0.0533) pop 0.0746 (0.0292)** 0.0746 (0.0253)*** 0.0746 (0.0292)** pctFemale -0.0821 (0.4343) -0.0821 (0.3756) -0.0821 (0.4343) pctRural -0.3415 (0.0389)*** -0.3415 (0.0336)*** -0.3415 (0.0389)*** factor(country)Argentina -26.4352 (2.5323)*** factor(country)Bahamas, The -24.2300 (2.1318)*** factor(country)Barbados -0.0618 (0.2654) factor(country)Belize -8.9778 (1.5499)*** factor(country)Bolivia -21.1922 (2.4176)*** factor(country)Brazil -37.4007 (5.6170)*** factor(country)Chile -19.5972 (2.5142)*** factor(country)Colombia -21.9154 (2.4081)*** factor(country)Costa Rica -15.3878 (2.4876)*** factor(country)Cuba -16.3491 (2.4175)*** factor(country)Dominican Republic -22.3355 (2.6281)*** factor(country)Ecuador -14.2608 (2.0636)*** factor(country)El Salvador -19.2981 (1.9355)*** factor(country)Grenada -7.9733 (1.3193)*** factor(country)Guatemala -12.3493 (1.4744)*** factor(country)Guyana -7.3506 (1.0755)*** factor(country)Haiti -23.5398 (1.7323)*** factor(country)Honduras -12.5973 (1.9355)*** factor(country)Jamaica -12.5303 (1.7901)*** factor(country)Mexico -29.4623 (3.5027)*** factor(country)Nicaragua -13.8416 (1.8645)*** factor(country)Panama -13.5153 (2.0809)*** factor(country)Paraguay -15.3079 (2.2995)*** factor(country)Peru -20.4454 (2.4008)*** factor(country)Puerto Rico -21.7780 (2.3829)*** factor(country)St. Lucia 1.4653 (0.5105)*** factor(country)St. Vincent and the Grenadines -13.2790 (2.0031)*** factor(country)Suriname -19.1671 (2.2540)*** factor(country)Trinidad and Tobago -13.5590 (1.3500)*** factor(country)Uruguay -23.4638 (2.7021)*** factor(country)Virgin Islands (U.S.) -22.7951 (2.4659)*** Constant 105.6350 (23.8548)*** 0.0000 (0.0114) Observations 128 128 128 R2 0.9987 0.6324 0.6324 Adjusted R2 0.9981 0.6205 0.4926 Note: Standard Errors reported in parentheses, *p&lt;0.1;**p&lt;0.05;***p&lt;0.01 We’ve changed a few of the stargazer options for this chapter. We’re displaying standard errors instead of p-values so that we can use only one row per variable (it lets us report coefficient and standard error on one line, but not if we use p-values instead). I modified the note to say that standard errors are reported in parentheses. Now look at the coefficient estimates. The 3 models all have the same coefficients on gdpPerCapita, pop, pctFemale, and pctRurla. In LN5 we discuss how the within transformation is equivalent to including dummy variables for each group (in this case, countries). That’s exactly what see in the table. The PLM package estimates the within transformation for us. You also may notice that the standard errors (and statistical significance stars) are different in the middle column. When we estimate the model with dummy variables (column 1), the regular OLS standard errors are correct. But when we apply the within transformation, we need to adjust the standard errors to account for the within transformation. This would be a bit difficult for us to do. Thankfully, the PLM package correctly adjusts the standard errors (and thus p-values) for us. Thus, in practice we won’t actually want to apply the within transformation ourselves. We’re doing it in this chapter so you can see exactly what it is in practice and see that the coefficient estimates for all 3 versions result in the same coefficients. If you compare the \\(R^2\\) values across models, you’ll notice that the \\(R^2\\) for the model with dummy variables is much higher. Including all the dummy variables makes it artificially high. We want to use the \\(R^2\\) from the within transformation. The PLM model does this for us. Another reason we want to use the PLM model when we estimate fixed effects models is that we often don’t want to see all of the coefficients on the dummy variables. For country fixed effects, the coefficient on each country dummy is estimated off of only 4 observations. Thus, it is not a reliable estimate of the effect of being Argentina (or Belize, etc). It still allows us to estimate the model with country fixed effects, even if we don’t care about the coefficient estimates themselves. However, if we had not dropped all countries except South America, we would have hundreds of dummy variables. If we were estimating a model using US county data, we would have over 3000. R probably wouldn’t even let us estimate a model with that many variables. This again makes the PLM package preferable. 17.4 Year Fixed Effects Above you saw how to estimate models with country fixed effects in three different ways. Here, you should estimate models with year fixed effects in the same three ways. Hint: you just have to switch “country” with “year” and everythign else is the same. ## Dummy variable for each year (it automatically omits one) yearDummies &lt;- lm(lifeExp~gdpPerCapita+pop+pctFemale+pctRural+factor(year),data=mydata) ## Within transformation (subtract year averages from each observation for each variable) ## Create Year Averages mydata &lt;- mydata %&gt;% group_by(year) %&gt;% mutate(yrAvg_lifeExp=mean(lifeExp), yrAvg_gdpPerCapita=mean(gdpPerCapita), yrAvg_pop=mean(pop), yrAvg_pctFemale=mean(pctFemale), yrAvg_pctRural=mean(pctRural) ) %&gt;% ungroup() ## Within transformation mydataYear &lt;- mydata %&gt;% mutate(lifeExp=lifeExp-yrAvg_lifeExp, gdpPerCapita=gdpPerCapita-yrAvg_gdpPerCapita, pop=pop-yrAvg_pop, pctFemale=pctFemale-yrAvg_pctFemale, pctRural=pctRural-yrAvg_pctRural ) %&gt;% ungroup() ## Estimate within transformation using the transformed data yearWithin &lt;- lm(lifeExp~gdpPerCapita+pop+pctFemale+pctRural,data=mydataYear) ## Using plm package yearPlm &lt;- plm(lifeExp~gdpPerCapita+pop+pctFemale+pctRural,data=mydata, index=c(&quot;year&quot;), model = &quot;within&quot;, effect=&quot;individual&quot;) stargazer(yearDummies,yearWithin,yearPlm, type = &quot;html&quot;, report=(&#39;vcs*&#39;), single.row = TRUE, digits = 4, keep.stat = c(&quot;n&quot;,&quot;rsq&quot;,&quot;adj.rsq&quot;), notes = &quot;Standard Errors reported in parentheses, &lt;em&gt;&amp;#42;p&amp;lt;0.1;&amp;#42;&amp;#42;p&amp;lt;0.05;&amp;#42;&amp;#42;&amp;#42;p&amp;lt;0.01&lt;/em&gt;&quot;, notes.append = FALSE) Dependent variable: lifeExp OLS panel linear (1) (2) (3) gdpPerCapita 0.1628 (0.0416)*** 0.1628 (0.0411)*** 0.1628 (0.0416)*** pop 0.0026 (0.0072) 0.0026 (0.0071) 0.0026 (0.0072) pctFemale 0.2108 (0.3435) 0.2108 (0.3393) 0.2108 (0.3435) pctRural -0.0325 (0.0147)** -0.0325 (0.0146)** -0.0325 (0.0147)** factor(year)2016 0.1532 (0.7523) factor(year)2017 0.2970 (0.7524) factor(year)2018 0.4251 (0.7524) Constant 63.3957 (17.2241)*** 0.0000 (0.2627) Observations 128 128 128 R2 0.2831 0.2809 0.2809 Adjusted R2 0.2413 0.2576 0.2390 Note: Standard Errors reported in parentheses, *p&lt;0.1;**p&lt;0.05;***p&lt;0.01 17.5 Country and Year Fixed Effects Now that you’ve estimated the models with year fixed effects, estimate models with both country and year fixed effects. It works the same way as above, just doing it for both country and year. ## Dummy variable for each country and each year (it automatically omits one of each) countryyearDummies &lt;- lm(lifeExp~gdpPerCapita+pop+pctFemale+pctRural+factor(year)+factor(country),data=mydata) ## Within transformation (subtract country AND year averages from each observation for each variable) ## We already created the country averages and year averages above so we don&#39;t need to create them again ## Within transformation mydataCountryYear &lt;- mydata %&gt;% mutate(lifeExp=lifeExp-cAvg_lifeExp-yrAvg_lifeExp, gdpPerCapita=gdpPerCapita-cAvg_gdpPerCapita-yrAvg_gdpPerCapita, pop=pop-cAvg_pop-yrAvg_pop, pctFemale=pctFemale-cAvg_pctFemale-yrAvg_pctFemale, pctRural=pctRural-cAvg_pctRural-yrAvg_pctRural ) %&gt;% ungroup() ##Estimate within transformation using the transformed data countryYearWithin &lt;- lm(lifeExp~gdpPerCapita+pop+pctFemale+pctRural,data=mydataCountryYear) ##Using plm package countryYearPlm &lt;- plm(lifeExp~gdpPerCapita+pop+pctFemale+pctRural,data=mydata, index=c(&quot;country&quot;,&quot;year&quot;), model = &quot;within&quot;, effect=&quot;twoways&quot;) stargazer(countryyearDummies,countryYearWithin, countryYearPlm, type = &quot;html&quot;, report=(&#39;vcs*&#39;), single.row = TRUE, digits = 4, keep.stat = c(&quot;n&quot;,&quot;rsq&quot;,&quot;adj.rsq&quot;), notes = &quot;Standard Errors reported in parentheses, &lt;em&gt;&amp;#42;p&amp;lt;0.1;&amp;#42;&amp;#42;p&amp;lt;0.05;&amp;#42;&amp;#42;&amp;#42;p&amp;lt;0.01&lt;/em&gt;&quot;, notes.append = FALSE) Dependent variable: lifeExp OLS panel linear (1) (2) (3) gdpPerCapita -0.0770 (0.0332)** -0.0770 (0.0282)*** -0.0770 (0.0332)** pop -0.0122 (0.0184) -0.0122 (0.0156) -0.0122 (0.0184) pctFemale -0.4519 (0.2566)* -0.4519 (0.2183)** -0.4519 (0.2566)* pctRural -0.1670 (0.0263)*** -0.1670 (0.0224)*** -0.1670 (0.0263)*** factor(year)2016 0.1409 (0.0231)*** factor(year)2017 0.2808 (0.0265)*** factor(year)2018 0.4187 (0.0322)*** factor(country)Argentina -11.4184 (1.8649)*** factor(country)Bahamas, The -11.8448 (1.5588)*** factor(country)Barbados 1.2703 (0.1851)*** factor(country)Belize -7.4386 (0.9190)*** factor(country)Bolivia -15.0054 (1.4948)*** factor(country)Brazil -10.0431 (3.8838)** factor(country)Chile -7.6845 (1.7254)*** factor(country)Colombia -9.6159 (1.6882)*** factor(country)Costa Rica -6.7688 (1.5976)*** factor(country)Cuba -7.8679 (1.5560)*** factor(country)Dominican Republic -13.6116 (1.6768)*** factor(country)Ecuador -8.0428 (1.2992)*** factor(country)El Salvador -11.8715 (1.2660)*** factor(country)Grenada -7.6154 (0.7762)*** factor(country)Guatemala -8.4458 (0.9150)*** factor(country)Guyana -9.0234 (0.6456)*** factor(country)Haiti -19.9197 (1.0544)*** factor(country)Honduras -8.7937 (1.1733)*** factor(country)Jamaica -8.9748 (1.0857)*** factor(country)Mexico -10.1515 (2.5150)*** factor(country)Nicaragua -9.7345 (1.1388)*** factor(country)Panama -6.6135 (1.3272)*** factor(country)Paraguay -10.7231 (1.3946)*** factor(country)Peru -10.3166 (1.6017)*** factor(country)Puerto Rico -7.2295 (1.7743)*** factor(country)St. Lucia -0.6604 (0.3402)* factor(country)St. Vincent and the Grenadines -10.7481 (1.1927)*** factor(country)Suriname -13.6043 (1.3885)*** factor(country)Trinidad and Tobago -8.6764 (0.8731)*** factor(country)Uruguay -10.8173 (1.8483)*** factor(country)Virgin Islands (U.S.) -7.4864 (1.8481)*** Constant 113.6250 (14.0285)*** -104.7890 (11.3948)*** Observations 128 128 128 R2 0.9996 0.3214 0.3214 Adjusted R2 0.9994 0.2994 0.0317 Note: Standard Errors reported in parentheses, *p&lt;0.1;**p&lt;0.05;***p&lt;0.01 17.6 Comparison of all models Below we have comparisons of the four models: ols, country fixed effects, year fixed effects, and country and year fixed effects. The comparisons are done three times, one for each method of estimating the models. 17.6.1 Within Transformation stargazer(ols,countryWithin,yearWithin,countryYearWithin, type = &quot;html&quot;, report=(&#39;vcs*&#39;), single.row = TRUE, digits = 3, keep.stat = c(&quot;n&quot;,&quot;rsq&quot;,&quot;adj.rsq&quot;), notes = &quot;Standard Errors reported in parentheses, &lt;em&gt;&amp;#42;p&amp;lt;0.1;&amp;#42;&amp;#42;p&amp;lt;0.05;&amp;#42;&amp;#42;&amp;#42;p&amp;lt;0.01&lt;/em&gt;&quot;, notes.append = FALSE) Dependent variable: lifeExp (1) (2) (3) (4) gdpPerCapita 0.163 (0.041)*** 0.065 (0.046) 0.163 (0.041)*** -0.077 (0.028)*** pop 0.003 (0.007) 0.075 (0.025)*** 0.003 (0.007) -0.012 (0.016) pctFemale 0.210 (0.340) -0.082 (0.376) 0.211 (0.339) -0.452 (0.218)** pctRural -0.033 (0.015)** -0.342 (0.034)*** -0.033 (0.015)** -0.167 (0.022)*** Constant 63.633 (17.031)*** 0.000 (0.011) 0.000 (0.263) -104.789 (11.395)*** Observations 128 128 128 128 R2 0.281 0.632 0.281 0.321 Adjusted R2 0.258 0.620 0.258 0.299 Note: Standard Errors reported in parentheses, *p&lt;0.1;**p&lt;0.05;***p&lt;0.01 17.6.2 PLM Package stargazer(ols,countryPlm,yearPlm,countryYearPlm, type = &quot;html&quot;, report=(&#39;vcs*&#39;), single.row = TRUE, digits = 3, keep.stat = c(&quot;n&quot;,&quot;rsq&quot;,&quot;adj.rsq&quot;), notes = &quot;Standard Errors reported in parentheses, &lt;em&gt;&amp;#42;p&amp;lt;0.1;&amp;#42;&amp;#42;p&amp;lt;0.05;&amp;#42;&amp;#42;&amp;#42;p&amp;lt;0.01&lt;/em&gt;&quot;, notes.append = FALSE) Dependent variable: lifeExp OLS panel linear (1) (2) (3) (4) gdpPerCapita 0.163 (0.041)*** 0.065 (0.053) 0.163 (0.042)*** -0.077 (0.033)** pop 0.003 (0.007) 0.075 (0.029)** 0.003 (0.007) -0.012 (0.018) pctFemale 0.210 (0.340) -0.082 (0.434) 0.211 (0.344) -0.452 (0.257)* pctRural -0.033 (0.015)** -0.342 (0.039)*** -0.033 (0.015)** -0.167 (0.026)*** Constant 63.633 (17.031)*** Observations 128 128 128 128 R2 0.281 0.632 0.281 0.321 Adjusted R2 0.258 0.493 0.239 0.032 Note: Standard Errors reported in parentheses, *p&lt;0.1;**p&lt;0.05;***p&lt;0.01 17.6.3 Dummy Variables stargazer(ols,countryDummies,yearDummies,countryyearDummies, type = &quot;html&quot;, report=(&#39;vcs*&#39;), single.row = TRUE, digits = 3, keep.stat = c(&quot;n&quot;,&quot;rsq&quot;,&quot;adj.rsq&quot;), notes = &quot;Standard Errors reported in parentheses, &lt;em&gt;&amp;#42;p&amp;lt;0.1;&amp;#42;&amp;#42;p&amp;lt;0.05;&amp;#42;&amp;#42;&amp;#42;p&amp;lt;0.01&lt;/em&gt;&quot;, notes.append = FALSE) Dependent variable: lifeExp (1) (2) (3) (4) gdpPerCapita 0.163 (0.041)*** 0.065 (0.053) 0.163 (0.042)*** -0.077 (0.033)** pop 0.003 (0.007) 0.075 (0.029)** 0.003 (0.007) -0.012 (0.018) pctFemale 0.210 (0.340) -0.082 (0.434) 0.211 (0.344) -0.452 (0.257)* pctRural -0.033 (0.015)** -0.342 (0.039)*** -0.033 (0.015)** -0.167 (0.026)*** factor(country)Argentina -26.435 (2.532)*** -11.418 (1.865)*** factor(country)Bahamas, The -24.230 (2.132)*** -11.845 (1.559)*** factor(country)Barbados -0.062 (0.265) 1.270 (0.185)*** factor(country)Belize -8.978 (1.550)*** -7.439 (0.919)*** factor(country)Bolivia -21.192 (2.418)*** -15.005 (1.495)*** factor(country)Brazil -37.401 (5.617)*** -10.043 (3.884)** factor(country)Chile -19.597 (2.514)*** -7.684 (1.725)*** factor(country)Colombia -21.915 (2.408)*** -9.616 (1.688)*** factor(country)Costa Rica -15.388 (2.488)*** -6.769 (1.598)*** factor(country)Cuba -16.349 (2.417)*** -7.868 (1.556)*** factor(country)Dominican Republic -22.336 (2.628)*** -13.612 (1.677)*** factor(country)Ecuador -14.261 (2.064)*** -8.043 (1.299)*** factor(country)El Salvador -19.298 (1.936)*** -11.871 (1.266)*** factor(country)Grenada -7.973 (1.319)*** -7.615 (0.776)*** factor(country)Guatemala -12.349 (1.474)*** -8.446 (0.915)*** factor(country)Guyana -7.351 (1.076)*** -9.023 (0.646)*** factor(country)Haiti -23.540 (1.732)*** -19.920 (1.054)*** factor(country)Honduras -12.597 (1.936)*** -8.794 (1.173)*** factor(country)Jamaica -12.530 (1.790)*** -8.975 (1.086)*** factor(country)Mexico -29.462 (3.503)*** -10.152 (2.515)*** factor(country)Nicaragua -13.842 (1.864)*** -9.735 (1.139)*** factor(country)Panama -13.515 (2.081)*** -6.613 (1.327)*** factor(country)Paraguay -15.308 (2.300)*** -10.723 (1.395)*** factor(country)Peru -20.445 (2.401)*** -10.317 (1.602)*** factor(country)Puerto Rico -21.778 (2.383)*** -7.229 (1.774)*** factor(country)St. Lucia 1.465 (0.511)*** -0.660 (0.340)* factor(country)St. Vincent and the Grenadines -13.279 (2.003)*** -10.748 (1.193)*** factor(country)Suriname -19.167 (2.254)*** -13.604 (1.389)*** factor(country)Trinidad and Tobago -13.559 (1.350)*** -8.676 (0.873)*** factor(country)Uruguay -23.464 (2.702)*** -10.817 (1.848)*** factor(country)Virgin Islands (U.S.) -22.795 (2.466)*** -7.486 (1.848)*** factor(year)2016 0.153 (0.752) 0.141 (0.023)*** factor(year)2017 0.297 (0.752) 0.281 (0.027)*** factor(year)2018 0.425 (0.752) 0.419 (0.032)*** Constant 63.633 (17.031)*** 105.635 (23.855)*** 63.396 (17.224)*** 113.625 (14.028)*** Observations 128 128 128 128 R2 0.281 0.999 0.283 1.000 Adjusted R2 0.258 0.998 0.241 0.999 Note: Standard Errors reported in parentheses, *p&lt;0.1;**p&lt;0.05;***p&lt;0.01 17.7 Data Summary by Country stargazer(as.data.frame(mydata) , type = &quot;html&quot;,digits = 2 ,summary.stat = c(&quot;n&quot;,&quot;mean&quot;,&quot;sd&quot;, &quot;min&quot;, &quot;median&quot;, &quot;max&quot;)) Statistic N Mean St. Dev. Min Median Max year 128 2,016.50 1.12 2,015 2,016.5 2,018 gdpPerCapita 128 10.66 8.20 1.39 8.25 35.07 lifeExp 128 74.92 3.45 62.48 74.91 80.10 pop 128 18.79 41.17 0.09 5.61 209.47 pctFemale 128 50.66 0.93 49.07 50.61 53.11 pctRural 128 35.49 21.25 4.28 33.62 81.48 cAvg_lifeExp 128 74.92 3.45 63.08 74.87 79.84 cAvg_gdpPerCapita 128 10.66 8.20 1.40 8.21 34.53 cAvg_pop 128 18.79 41.17 0.09 5.63 206.98 cAvg_pctFemale 128 50.66 0.93 49.13 50.61 53.05 cAvg_pctRural 128 35.49 21.25 4.46 33.38 81.41 yrAvg_lifeExp 128 74.92 0.19 74.67 74.92 75.17 yrAvg_gdpPerCapita 128 10.66 0.12 10.53 10.64 10.84 yrAvg_pop 128 18.79 0.23 18.49 18.79 19.09 yrAvg_pctFemale 128 50.66 0.01 50.65 50.66 50.67 yrAvg_pctRural 128 35.49 0.28 35.11 35.49 35.87 17.7.1 Average Values for Each Country country AvglifeExp AvggdpPerCapita Avgpop AvgpctFemale AvgpctRural Antigua and Barbuda 76.68 15.15 0.09 51.83 75.21 Argentina 76.30 13.46 43.82 51.26 8.31 Bahamas, The 73.43 31.80 0.38 51.45 17.12 Barbados 78.94 16.81 0.29 51.73 68.81 Belize 74.28 4.70 0.37 50.14 54.44 Bolivia 70.77 3.16 11.11 49.76 31.09 Brazil 75.34 8.59 206.98 50.80 13.83 Chile 79.84 13.67 18.34 50.75 12.54 Colombia 76.82 6.22 48.57 50.96 19.73 Costa Rica 79.83 12.15 4.92 49.99 21.88 Cuba 78.64 7.83 11.33 50.32 23.04 Dominican Republic 73.57 7.44 10.45 49.97 20.16 Ecuador 76.47 6.00 16.64 49.96 36.39 El Salvador 72.76 3.81 6.37 53.05 29.13 Grenada 72.41 9.58 0.11 49.58 63.87 Guatemala 73.67 4.07 15.96 50.78 49.49 Guyana 69.53 5.87 0.77 49.90 73.48 Haiti 63.08 1.40 10.91 50.64 46.14 Honduras 74.80 2.37 9.35 50.06 43.87 Jamaica 74.23 4.97 2.91 50.32 44.75 Mexico 74.94 9.79 124.04 51.09 20.28 Nicaragua 73.96 2.11 6.34 50.71 41.80 Panama 78.05 14.29 4.07 49.88 32.80 Paraguay 73.91 5.65 6.82 49.13 38.83 Peru 76.16 6.40 31.21 50.34 22.37 Puerto Rico 79.57 29.82 3.35 52.30 6.40 St. Lucia 75.83 10.55 0.18 50.75 81.41 St. Vincent and the Grenadines 72.25 7.06 0.11 49.14 48.42 Suriname 71.41 8.87 0.57 49.70 33.95 Trinidad and Tobago 73.17 17.07 1.38 50.57 46.76 Uruguay 77.57 15.87 3.43 51.75 4.81 Virgin Islands (U.S.) 79.27 34.53 0.11 52.39 4.46 17.7.2 Variable-Specific Values and Within Transformation for Each Country For each variable, display each country’s values in 2015, 2016, 2017, and 2018, followed by the country’s average. These 5 columns are shaded from red (lowest) to green (highest) for each country. Then, in the final 4 columns display the within transformation (i.e., subtract the country’s average from each year’s value). These last 4 columns are also shaded for each country. 17.7.3 Life expectancy iso3c 2015Life Exp 2016Life Exp 2017Life Exp 2018Life Exp AvgLife Exp 2015Within 2016Within 2017Within 2018Within ARG 76.07 76.22 76.37 76.52 76.30 -0.23 -0.07 0.08 0.22 ATG 76.48 76.62 76.75 76.89 76.68 -0.20 -0.07 0.07 0.20 BHS 73.09 73.33 73.55 73.75 73.43 -0.34 -0.10 0.12 0.32 BLZ 74.03 74.22 74.36 74.50 74.28 -0.24 -0.06 0.09 0.22 BOL 70.28 70.63 70.94 71.24 70.77 -0.49 -0.15 0.17 0.47 BRA 74.99 75.23 75.46 75.67 75.34 -0.34 -0.11 0.12 0.33 BRB 78.80 78.89 78.98 79.08 78.94 -0.14 -0.05 0.04 0.14 CHL 79.65 79.78 79.91 80.04 79.84 -0.20 -0.06 0.07 0.20 COL 76.53 76.73 76.92 77.11 76.82 -0.29 -0.09 0.10 0.28 CRI 79.56 79.74 79.91 80.10 79.83 -0.26 -0.09 0.09 0.27 CUB 78.56 78.61 78.66 78.73 78.64 -0.08 -0.03 0.02 0.09 DOM 73.24 73.47 73.69 73.89 73.57 -0.33 -0.10 0.12 0.32 ECU 76.14 76.36 76.58 76.80 76.47 -0.33 -0.11 0.11 0.33 GRD 72.44 72.41 72.39 72.38 72.41 0.04 0.00 -0.02 -0.02 GTM 73.25 73.54 73.81 74.06 73.67 -0.42 -0.12 0.14 0.40 GUY 69.26 69.45 69.62 69.77 69.53 -0.27 -0.07 0.10 0.25 HND 74.50 74.70 74.90 75.09 74.80 -0.30 -0.09 0.10 0.29 HTI 62.48 62.90 63.29 63.66 63.08 -0.60 -0.19 0.21 0.58 JAM 74.10 74.17 74.27 74.37 74.23 -0.13 -0.05 0.04 0.14 LCA 75.60 75.75 75.91 76.06 75.83 -0.23 -0.08 0.08 0.23 MEX 74.90 74.92 74.95 74.99 74.94 -0.04 -0.02 0.01 0.05 NIC 73.65 73.86 74.07 74.28 73.96 -0.31 -0.10 0.11 0.31 PAN 77.78 77.96 78.15 78.33 78.05 -0.28 -0.09 0.09 0.27 PER 75.79 76.04 76.29 76.52 76.16 -0.37 -0.12 0.13 0.36 PRI 79.35 79.49 79.63 79.78 79.57 -0.21 -0.07 0.07 0.21 PRY 73.66 73.84 73.99 74.13 73.91 -0.24 -0.07 0.09 0.23 SLV 72.41 72.64 72.87 73.10 72.76 -0.34 -0.11 0.12 0.34 SUR 71.25 71.36 71.46 71.57 71.41 -0.16 -0.05 0.05 0.16 TTO 72.94 73.10 73.25 73.38 73.17 -0.23 -0.07 0.08 0.21 URY 77.37 77.50 77.63 77.77 77.57 -0.20 -0.07 0.06 0.20 VCT 72.10 72.19 72.30 72.42 72.25 -0.16 -0.06 0.05 0.16 VIR 79.02 79.17 79.37 79.52 79.27 -0.25 -0.10 0.10 0.25 17.7.4 GDP per capita iso3c 2015GDP per Capita 2016GDP per Capita 2017GDP per Capita 2018GDP per Capita AvgGDP per Capita 2015Within 2016Within 2017Within 2018Within ARG 13789.06 13360.21 13595.04 13105.40 13462.43 0.33 -0.10 0.13 -0.36 ATG 14285.33 14919.21 15242.37 16146.59 15148.38 -0.86 -0.23 0.09 1.00 BHS 31776.06 31491.70 31682.70 32231.27 31795.43 -0.02 -0.30 -0.11 0.44 BLZ 4770.23 4671.91 4663.28 4707.40 4703.21 0.07 -0.03 -0.04 0.00 BOL 3035.97 3118.91 3203.00 3291.16 3162.26 -0.13 -0.04 0.04 0.13 BRA 8813.99 8455.31 8498.29 8582.34 8587.48 0.23 -0.13 -0.09 -0.01 BRB 16524.90 16906.59 16961.34 16838.73 16807.89 -0.28 0.10 0.15 0.03 CHL 13574.17 13624.68 13590.99 13901.02 13672.71 -0.10 -0.05 -0.08 0.23 COL 6175.88 6219.15 6208.99 6271.88 6218.97 -0.04 0.00 -0.01 0.05 CRI 11642.78 12004.67 12375.92 12573.96 12149.33 -0.51 -0.14 0.23 0.42 CUB 7694.01 7726.50 7863.39 8040.99 7831.22 -0.14 -0.10 0.03 0.21 DOM 6921.52 7300.03 7556.85 7997.76 7444.04 -0.52 -0.14 0.11 0.55 ECU 6124.49 5947.00 5981.13 5952.22 6001.21 0.12 -0.05 -0.02 -0.05 GRD 9096.54 9380.23 9742.60 10109.05 9582.11 -0.49 -0.20 0.16 0.53 GTM 3994.64 4034.16 4091.27 4160.07 4070.03 -0.08 -0.04 0.02 0.09 GUY 5576.83 5759.69 5945.07 6178.89 5865.12 -0.29 -0.11 0.08 0.31 HND 2286.20 2334.75 2406.75 2457.97 2371.42 -0.09 -0.04 0.04 0.09 HTI 1386.85 1393.18 1409.63 1415.00 1401.17 -0.01 -0.01 0.01 0.01 JAM 4907.93 4949.37 4973.72 5043.54 4968.64 -0.06 -0.02 0.01 0.07 LCA 10093.62 10409.54 10718.86 10975.83 10549.46 -0.46 -0.14 0.17 0.43 MEX 9616.65 9751.57 9842.40 9945.78 9789.10 -0.17 -0.04 0.05 0.16 NIC 2049.85 2115.93 2185.91 2086.03 2109.43 -0.06 0.01 0.08 -0.02 PAN 13630.30 14062.44 14596.74 14880.66 14292.54 -0.66 -0.23 0.30 0.59 PER 6229.10 6380.03 6432.92 6574.32 6404.09 -0.17 -0.02 0.03 0.17 PRI 29763.49 29961.75 29809.22 29753.36 29821.96 -0.06 0.14 -0.01 -0.07 PRY 5413.78 5570.61 5762.73 5871.28 5654.60 -0.24 -0.08 0.11 0.22 SLV 3705.58 3781.38 3846.97 3920.53 3813.61 -0.11 -0.03 0.03 0.11 SUR 9168.24 8628.86 8677.78 9020.44 8873.83 0.29 -0.24 -0.20 0.15 TTO 18214.46 17103.87 16515.03 16457.19 17072.64 1.14 0.03 -0.56 -0.62 URY 15613.75 15821.36 16020.38 16037.93 15873.35 -0.26 -0.05 0.15 0.16 VCT 6921.70 7031.73 7078.81 7206.50 7059.68 -0.14 -0.03 0.02 0.15 VIR 34007.35 34614.75 34435.49 35073.63 34532.81 -0.53 0.08 -0.10 0.54 17.7.5 Population iso3c 2015Population 2016Population 2017Population 2018Population AvgPopulation 2015Within 2016Within 2017Within 2018Within ARG 43131966 43590368 44044811 44494502 43815411.8 -0.68 -0.23 0.23 0.68 ATG 93571 94520 95425 96282 94949.5 0.00 0.00 0.00 0.00 BHS 374200 377923 381749 385635 379876.8 -0.01 0.00 0.00 0.01 BLZ 360926 368399 375775 383071 372042.8 -0.01 0.00 0.00 0.01 BOL 10869732 11031822 11192853 11353140 11111886.8 -0.24 -0.08 0.08 0.24 BRA 204471759 206163056 207833825 209469320 206984490.0 -2.51 -0.82 0.85 2.48 BRB 285327 285798 286229 286640 285998.5 0.00 0.00 0.00 0.00 CHL 17969356 18209072 18470435 18729166 18344507.2 -0.38 -0.14 0.13 0.38 COL 47520667 48175048 48909844 49661056 48566653.8 -1.05 -0.39 0.34 1.09 CRI 4847805 4899336 4949955 4999443 4924134.8 -0.08 -0.02 0.03 0.08 CUB 11324777 11335108 11339255 11338146 11334321.5 -0.01 0.00 0.00 0.00 DOM 10281675 10397738 10513111 10627147 10454917.8 -0.17 -0.06 0.06 0.17 ECU 16212022 16491116 16785356 17084359 16643213.2 -0.43 -0.15 0.14 0.44 GRD 109603 110263 110874 111449 110547.2 0.00 0.00 0.00 0.00 GTM 15567419 15827690 16087418 16346950 15957369.2 -0.39 -0.13 0.13 0.39 GUY 767433 771363 775218 779007 773255.2 -0.01 0.00 0.00 0.01 HND 9112904 9270794 9429016 9587523 9350059.2 -0.24 -0.08 0.08 0.24 HTI 10695540 10839976 10982367 11123183 10910266.5 -0.21 -0.07 0.07 0.21 JAM 2891024 2906242 2920848 2934853 2913241.8 -0.02 -0.01 0.01 0.02 LCA 179131 180028 180955 181890 180501.0 0.00 0.00 0.00 0.00 MEX 121858251 123333379 124777326 126190782 124039934.5 -2.18 -0.71 0.74 2.15 NIC 6223234 6303970 6384843 6465502 6344387.2 -0.12 -0.04 0.04 0.12 PAN 3968490 4037073 4106764 4176868 4072298.8 -0.10 -0.04 0.03 0.10 PER 30470739 30926036 31444299 31989265 31207584.8 -0.74 -0.28 0.24 0.78 PRI 3473232 3406672 3325286 3193354 3349636.0 0.12 0.06 -0.02 -0.16 PRY 6688746 6777878 6867058 6956069 6822437.8 -0.13 -0.04 0.04 0.13 SLV 6325121 6356137 6388124 6420740 6372530.5 -0.05 -0.02 0.02 0.05 SUR 559136 564883 570501 575987 567626.8 -0.01 0.00 0.00 0.01 TTO 1370332 1377563 1384060 1389841 1380449.0 -0.01 0.00 0.00 0.01 URY 3412013 3424139 3436645 3449290 3430521.8 -0.02 -0.01 0.01 0.02 VCT 109135 109467 109826 110210 109659.5 0.00 0.00 0.00 0.00 VIR 107712 107516 107281 107001 107377.5 0.00 0.00 0.00 0.00 17.7.6 Percent female iso3c 2015%Female 2016%Female 2017%Female 2018%Female Avg%Female 2015Within 2016Within 2017Within 2018Within ARG 51.27 51.26 51.25 51.24 51.26 0.02 0.01 -0.01 -0.02 ATG 51.88 51.84 51.81 51.79 51.83 0.05 0.01 -0.02 -0.04 BHS 51.47 51.45 51.44 51.43 51.45 0.02 0.01 -0.01 -0.02 BLZ 50.09 50.12 50.16 50.19 50.14 -0.05 -0.02 0.02 0.05 BOL 49.74 49.75 49.77 49.78 49.76 -0.02 -0.01 0.01 0.02 BRA 50.77 50.79 50.81 50.83 50.80 -0.03 -0.01 0.01 0.03 BRB 51.79 51.75 51.71 51.67 51.73 0.06 0.02 -0.02 -0.06 CHL 50.78 50.76 50.75 50.73 50.75 0.02 0.01 -0.01 -0.03 COL 50.99 50.97 50.95 50.93 50.96 0.03 0.01 -0.01 -0.03 CRI 49.97 49.98 49.99 50.01 49.99 -0.02 -0.01 0.01 0.02 CUB 50.31 50.31 50.32 50.33 50.32 -0.01 0.00 0.00 0.01 DOM 49.94 49.96 49.98 50.01 49.97 -0.04 -0.01 0.01 0.04 ECU 49.94 49.95 49.96 49.97 49.96 -0.01 0.00 0.00 0.01 GRD 49.56 49.58 49.59 49.61 49.58 -0.03 -0.01 0.01 0.02 GTM 50.81 50.79 50.78 50.76 50.78 0.02 0.01 -0.01 -0.02 GUY 50.00 49.92 49.86 49.81 49.90 0.10 0.03 -0.04 -0.09 HND 50.07 50.07 50.06 50.05 50.06 0.01 0.00 0.00 -0.01 HTI 50.64 50.64 50.65 50.65 50.64 -0.01 0.00 0.00 0.01 JAM 50.30 50.32 50.33 50.34 50.32 -0.02 -0.01 0.01 0.02 LCA 50.76 50.75 50.75 50.75 50.75 0.00 0.00 0.00 0.00 MEX 51.10 51.10 51.09 51.09 51.09 0.00 0.00 0.00 0.00 NIC 50.71 50.71 50.71 50.71 50.71 0.00 0.00 0.00 0.00 PAN 49.85 49.87 49.89 49.91 49.88 -0.03 -0.01 0.01 0.03 PER 50.33 50.35 50.35 50.34 50.34 -0.01 0.01 0.00 0.00 PRI 52.16 52.24 52.34 52.45 52.30 -0.14 -0.06 0.04 0.15 PRY 49.11 49.12 49.14 49.15 49.13 -0.02 -0.01 0.00 0.02 SLV 52.98 53.03 53.07 53.11 53.05 -0.07 -0.02 0.02 0.07 SUR 49.69 49.70 49.71 49.72 49.70 -0.01 0.00 0.01 0.01 TTO 50.54 50.56 50.57 50.59 50.57 -0.02 -0.01 0.01 0.02 URY 51.77 51.75 51.74 51.72 51.75 0.02 0.01 -0.01 -0.02 VCT 49.07 49.11 49.16 49.21 49.14 -0.06 -0.03 0.02 0.07 VIR 52.35 52.37 52.41 52.44 52.39 -0.04 -0.02 0.02 0.05 17.7.7 Percent rural iso3c 2015%Rural 2016%Rural 2017%Rural 2018%Rural Avg%Rural 2015Within 2016Within 2017Within 2018Within ARG 8.50 8.37 8.25 8.13 8.31 0.18 0.06 -0.06 -0.18 ATG 75.00 75.15 75.29 75.40 75.21 -0.21 -0.06 0.08 0.19 BHS 17.25 17.17 17.08 16.98 17.12 0.14 0.05 -0.04 -0.14 BLZ 54.59 54.51 54.40 54.28 54.44 0.15 0.06 -0.04 -0.17 BOL 31.61 31.26 30.92 30.58 31.09 0.52 0.17 -0.17 -0.52 BRA 14.23 13.96 13.69 13.43 13.83 0.40 0.13 -0.14 -0.40 BRB 68.75 68.81 68.84 68.85 68.81 -0.06 -0.01 0.03 0.04 CHL 12.64 12.58 12.51 12.44 12.54 0.10 0.04 -0.03 -0.11 COL 20.24 19.89 19.55 19.22 19.73 0.51 0.17 -0.17 -0.50 CRI 23.14 22.26 21.44 20.66 21.88 1.26 0.39 -0.44 -1.22 CUB 23.10 23.07 23.02 22.96 23.04 0.06 0.03 -0.02 -0.08 DOM 21.43 20.56 19.72 18.93 20.16 1.27 0.40 -0.44 -1.24 ECU 36.60 36.47 36.33 36.18 36.39 0.21 0.07 -0.06 -0.22 GRD 64.00 63.93 63.84 63.73 63.87 0.13 0.05 -0.04 -0.15 GTM 50.03 49.68 49.32 48.95 49.49 0.54 0.19 -0.17 -0.55 GUY 73.56 73.52 73.46 73.39 73.48 0.08 0.03 -0.02 -0.09 HND 44.84 44.19 43.54 42.90 43.87 0.97 0.32 -0.32 -0.96 HTI 47.57 46.60 45.65 44.72 46.14 1.43 0.47 -0.48 -1.42 JAM 45.17 44.90 44.62 44.33 44.75 0.41 0.15 -0.13 -0.43 LCA 81.48 81.44 81.39 81.32 81.41 0.08 0.03 -0.02 -0.09 MEX 20.72 20.42 20.13 19.84 20.28 0.44 0.14 -0.15 -0.43 NIC 42.10 41.91 41.70 41.48 41.80 0.31 0.11 -0.10 -0.32 PAN 33.30 32.97 32.63 32.29 32.80 0.50 0.17 -0.17 -0.51 PER 22.64 22.46 22.28 22.09 22.37 0.27 0.09 -0.09 -0.28 PRI 6.38 6.40 6.41 6.42 6.40 -0.03 0.00 0.01 0.02 PRY 39.25 38.97 38.70 38.42 38.83 0.42 0.14 -0.13 -0.42 SLV 30.30 29.50 28.73 27.98 29.13 1.17 0.37 -0.40 -1.15 SUR 33.94 33.96 33.96 33.94 33.95 -0.01 0.01 0.01 -0.01 TTO 46.68 46.75 46.80 46.82 46.76 -0.08 -0.01 0.03 0.06 URY 4.96 4.86 4.76 4.67 4.81 0.15 0.05 -0.05 -0.14 VCT 49.04 48.63 48.22 47.80 48.42 0.62 0.21 -0.20 -0.62 VIR 4.65 4.52 4.40 4.28 4.46 0.19 0.06 -0.06 -0.18 17.8 Bookdown Style Note To get the above tables to display in bookdown I added HTML code to allow the maximum width to be larger than the default setting. If you find this messes with the rest of your book, you can remove from here to the bottom of this file instide the HTML “style” tag. .book .book-body .page-wrapper .page-inner { max-width: 90%; } "],["difference-in-differences.html", "18 Difference-in-Differences 18.1 Data 18.2 Model 1 18.3 Model 2 18.4 Comparison of models 18.5 Additional questions 18.6 Polynomials", " 18 Difference-in-Differences Solutions .textSoln { color: red; } In this chapter you will estimate the Garbage Incinerator Difference-In-Differences model discussed in LN6. You can also gain experience working with models that include polynomials. Solutions are posted in Moodle. library(tidyverse) library(pander) library(stargazer) ## Set how many decimals at which R starts to use scientific notation options(scipen=4) mydata &lt;- read_csv(&quot;https://raw.githubusercontent.com/econ380w21/380data/main/garbageIncineratorDD.csv&quot;) 18.1 Data The data has 321 observations on house prices in North Andover, MA. The variables include: Variable Description year 1978 or 1981 dist miles from incinerator rprice price, 1978 dollars (“r” means real price, corrected for inflation) age age of house in years rooms # rooms in house baths # bathrooms area house size in square feet land land area in square feet The relevant background is covered in LN6, but I’ll summarize it again. We have house prices in 1978 and 1981 in North Andover, MA. In 1978 no one knows about any garbage incinerator, so prices in 1978 cannot be affected by the garbage incinerator. In 1979 it was announced publicly that a new garbage incinerator would be built and operational by 1981. The variable “dist” has the distance in miles from the future location of the incinerator. We want to estimate if knowledge of the future incinerator lowered house prices for houses near the incinerator. We’ll define “near” as within 3 miles of the incinerator location. stargazer(as.data.frame(mydata), type = &quot;html&quot;,summary.stat = c(&quot;n&quot;,&quot;mean&quot;,&quot;sd&quot;, &quot;min&quot;, &quot;median&quot;, &quot;max&quot;)) Statistic N Mean St. Dev. Min Median Max year 321 1,979.327 1.492 1,978 1,978 1,981 dist 321 3.923 1.611 0.947 3.769 7.576 rprice 321 83,721.350 33,118.790 26,000.000 82,000.000 300,000.000 age 321 18.009 32.566 0 4 189 rooms 321 6.586 0.901 4 7 10 baths 321 2.340 0.771 1 2 4 area 321 2,106.729 694.958 735 2,056 5,136 land 321 39,629.890 39,514.390 1,710 43,560 544,500 Note that I put mydata inside as.data.frame(). Sometimes stargazer doesn’t work otherwise (as some of you found out in the CP). 18.2 Model 1 Estimate the following model and store the results in model1 \\[rprice=\\beta_0+\\beta_1near+\\beta_2y81+\\beta_3near\\cdot y81+u\\] where Variable Description near =1 if house is 3 miles or less from incinerator location, = 0 o/w y81 =1 if house sold in 1981, = 0 o/w Fill in your code here. You need to generate the near and y81 variables a variable for the interaction term (i.e., y81near=y81*near). Tip about creating dummy variables: If you use something like dist&lt;=3 to create near, it will create a column of TRUE and FALSE. If you put the condition in parentheses and multiply by 1, it will turn TRUE into 1 and FALSE into 0. Thus, (dist&lt;=3)*1 is equivalent to ifelse(dist&lt;=3,1,0). Tip (or, more of a requirement for 380) about interaction terms: R will let you include y81*near in the regression directly, but do not do this in 380 (and I suggest not doing it after 380 either, at least unless you check to make sure it’s doing what you think it’s doing). Frequently people make mistakes when they rely on R to include the interactions for them. To be sure that you know what the interaction terms are, you need to create them yourself. So create the interaction term yourself. You can name it y81near. After you create the three variables, estimate the regression, store the regression results in model1, and display the results using pander. Then, use group_by() and summarize to calculate the mean price for the four groups (far 1978, far 1981, near 1978, near 1981) Finally, display the combinations of regression coefficients that equal the four group averages. These should match with the values shown and discussed in the LN6 slides. ## Create Variables mydata &lt;- mydata %&gt;% mutate(near = (dist &lt;=3) * 1, y81 = (year == 1981) * 1, y81near = y81 * near) ## Estimate model model1 &lt;- lm(rprice~near+y81+y81near,data=mydata) pander(summary(model1))   Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 82517 2727 30.26 1.709e-95 near -18824 4875 -3.861 0.0001368 y81 18790 4050 4.64 0.000005117 y81near -11864 7457 -1.591 0.1126 Fitting linear model: rprice ~ near + y81 + y81near Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 321 30243 0.1739 0.1661 ## 4 group averages using group_by() and summarize() mydata %&gt;% group_by(near,y81) %&gt;% summarize(priceAvg=mean(rprice)) ## # A tibble: 4 × 3 ## # Groups: near [2] ## near y81 priceAvg ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 82517. ## 2 0 1 101308. ## 3 1 0 63693. ## 4 1 1 70619. ## far 1978 coef(model1)[&quot;(Intercept)&quot;] ## (Intercept) ## 82517.23 ## far 1981 coef(model1)[&quot;(Intercept)&quot;]+coef(model1)[&quot;y81&quot;] ## (Intercept) ## 101307.5 ## near 1978 coef(model1)[&quot;(Intercept)&quot;]+coef(model1)[&quot;near&quot;] ## (Intercept) ## 63692.86 ## near 1981 coef(model1)[&quot;(Intercept)&quot;]+coef(model1)[&quot;near&quot;]+coef(model1)[&quot;y81&quot;]+coef(model1)[&quot;y81near&quot;] ## (Intercept) ## 70619.24 18.2.1 Equivalent model 1 In LN6 we discuss an equivalent model that defines dummy variables for each group. Create the set of group dummy variables and estimate the model. Stote it in model1equiv. Calculate the conditional expectation for the four groups and show that each is the same as in the version of model 1 estimated above. ## Write your code here ## Create Variables mydata &lt;- mydata %&gt;% mutate(y78near = (1-y81)*near, y81far = y81 * (1-near)) ## we already created y81near above ## Estimate model model1equiv &lt;- lm(rprice~y78near+y81near+y81far,data=mydata) pander(summary(model1equiv))   Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 82517 2727 30.26 1.709e-95 y78near -18824 4875 -3.861 0.0001368 y81near -11898 5505 -2.161 0.03141 y81far 18790 4050 4.64 0.000005117 Fitting linear model: rprice ~ y78near + y81near + y81far Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 321 30243 0.1739 0.1661 ## display conditional expectations for each model, and subtract model1equiv estimate from model 1 to show they are o (all are within 1e-10) ## far 1978 coef(model1)[&quot;(Intercept)&quot;] ## (Intercept) ## 82517.23 coef(model1equiv)[&quot;(Intercept)&quot;] ## (Intercept) ## 82517.23 coef(model1)[&quot;(Intercept)&quot;]-coef(model1equiv)[&quot;(Intercept)&quot;] ## (Intercept) ## -5.820766e-11 ## far 1981 coef(model1)[&quot;(Intercept)&quot;]+coef(model1)[&quot;y81&quot;] ## (Intercept) ## 101307.5 coef(model1equiv)[&quot;(Intercept)&quot;]+coef(model1equiv)[&quot;y81far&quot;] ## (Intercept) ## 101307.5 coef(model1)[&quot;(Intercept)&quot;]+coef(model1)[&quot;y81&quot;]-(coef(model1equiv)[&quot;(Intercept)&quot;]+coef(model1equiv)[&quot;y81far&quot;]) ## (Intercept) ## -5.820766e-11 ## near 1978 coef(model1)[&quot;(Intercept)&quot;]+coef(model1)[&quot;near&quot;] ## (Intercept) ## 63692.86 coef(model1equiv)[&quot;(Intercept)&quot;]+coef(model1equiv)[&quot;y78near&quot;] ## (Intercept) ## 63692.86 coef(model1)[&quot;(Intercept)&quot;]+coef(model1)[&quot;near&quot;]-(coef(model1equiv)[&quot;(Intercept)&quot;]+coef(model1equiv)[&quot;y78near&quot;]) ## (Intercept) ## 2.546585e-10 ## near 1981 coef(model1)[&quot;(Intercept)&quot;]+coef(model1)[&quot;near&quot;]+coef(model1)[&quot;y81&quot;]+coef(model1)[&quot;y81near&quot;] ## (Intercept) ## 70619.24 coef(model1equiv)[&quot;(Intercept)&quot;]+coef(model1equiv)[&quot;y81near&quot;] ## (Intercept) ## 70619.24 coef(model1)[&quot;(Intercept)&quot;]+coef(model1)[&quot;near&quot;]+coef(model1)[&quot;y81&quot;]+coef(model1)[&quot;y81near&quot;]-(coef(model1equiv)[&quot;(Intercept)&quot;]+coef(model1equiv)[&quot;y81near&quot;]) ## (Intercept) ## -5.820766e-11 18.3 Model 2 Now estimate the following model \\[ \\begin{align} rprice=\\beta_0+\\beta_1near+\\beta_2y81+\\beta_3near\\cdot y81 &amp;+\\beta_4age+\\beta_5age^2 + \\beta_6rooms \\\\ &amp;+ \\beta_7baths+\\beta_8area+\\beta_9land+u \\end{align} \\] where \\(age^2\\) is the variable age squared. Store your regression results in model2. mydata$agesq &lt;- mydata$age^2 model2 &lt;- lm(rprice~near+y81+y81near+age+agesq+rooms+baths+area+land,data=mydata) pander(summary(model2))   Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 2189 10440 0.2097 0.8341 near 9965 3881 2.568 0.0107 y81 15210 2788 5.455 0.00000009997 y81near -15021 5030 -2.986 0.003048 age -657.2 129 -5.095 0.0000006063 agesq 3.083 0.8099 3.806 0.0001698 rooms 3281 1679 1.954 0.05156 baths 7489 2601 2.879 0.004267 area 17.83 2.328 7.658 2.405e-13 land 0.1156 0.02993 3.863 0.0001364 Fitting linear model: rprice ~ near + y81 + y81near + age + agesq + rooms + baths + area + land Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 321 19824 0.6518 0.6417 Note that if you want to use \\(age^2\\) inside lm() instead of creating the agesq variable first, you need to put the age^2 inside I(). For example, you could do: # model2 &lt;- lm (rprice~near+y81+near*y81+age+I(age^2)+rooms+baths+area+land, data= mydata) # pander(summary (example)) 18.4 Comparison of models Here we display the two models in a stargazer table. First we’ll display them with the p-values. Then we’ll display them with standard errors, next with the t-statistic, and finally with the 95% confidence interval. It’s common for results to be presenting with one of these four statistics, so it’s important for you to be comfortable with each choice. I’ve included the stargazer tables for you. Just un-comment-them out (i.e., remove the hash tags…which you can do by selecting all of them and using ctrl/cmd-shift-C). stargazer(model1, model2, type = &quot;html&quot;, report=(&#39;vc*p&#39;), keep.stat = c(&quot;n&quot;,&quot;rsq&quot;,&quot;adj.rsq&quot;), notes = &quot;p-value reported in parentheses, &lt;em&gt;&amp;#42;p&amp;lt;0.1;&amp;#42;&amp;#42;p&amp;lt;0.05;&amp;#42;&amp;#42;&amp;#42;p&amp;lt;0.01&lt;/em&gt;&quot;, notes.append = FALSE) Dependent variable: rprice (1) (2) near -18,824.370*** 9,964.964** p = 0.0002 p = 0.011 y81 18,790.290*** 15,210.440*** p = 0.00001 p = 0.00000 y81near -11,863.900 -15,020.680*** p = 0.113 p = 0.004 age -657.159*** p = 0.00000 agesq 3.083*** p = 0.0002 rooms 3,280.510* p = 0.052 baths 7,489.360*** p = 0.005 area 17.830*** p = 0.000 land 0.116*** p = 0.0002 Constant 82,517.230*** 2,189.081 p = 0.000 p = 0.835 Observations 321 321 R2 0.174 0.652 Adjusted R2 0.166 0.642 Note: p-value reported in parentheses, *p&lt;0.1;**p&lt;0.05;***p&lt;0.01 stargazer(model1, model2, type = &quot;html&quot;, report=(&#39;vc*s&#39;), keep.stat = c(&quot;n&quot;,&quot;rsq&quot;,&quot;adj.rsq&quot;), notes = &quot;standard error reported in parentheses, &lt;em&gt;&amp;#42;p&amp;lt;0.1;&amp;#42;&amp;#42;p&amp;lt;0.05;&amp;#42;&amp;#42;&amp;#42;p&amp;lt;0.01&lt;/em&gt;&quot;, notes.append = FALSE) Dependent variable: rprice (1) (2) near -18,824.370*** 9,964.964** (4,875.322) (3,880.777) y81 18,790.290*** 15,210.440*** (4,050.065) (2,788.266) y81near -11,863.900 -15,020.680*** (7,456.646) (5,029.757) age -657.159*** (128.983) agesq 3.083*** (0.810) rooms 3,280.510* (1,678.566) baths 7,489.360*** (2,601.438) area 17.830*** (2.328) land 0.116*** (0.030) Constant 82,517.230*** 2,189.081 (2,726.910) (10,440.190) Observations 321 321 R2 0.174 0.652 Adjusted R2 0.166 0.642 Note: standard error reported in parentheses, *p&lt;0.1;**p&lt;0.05;***p&lt;0.01 stargazer(model1, model2, type = &quot;html&quot;, report=(&#39;vc*t&#39;), keep.stat = c(&quot;n&quot;,&quot;rsq&quot;,&quot;adj.rsq&quot;), notes = &quot;t-statistic reported for each coefficient, &lt;em&gt;&amp;#42;p&amp;lt;0.1;&amp;#42;&amp;#42;p&amp;lt;0.05;&amp;#42;&amp;#42;&amp;#42;p&amp;lt;0.01&lt;/em&gt;&quot;, notes.append = FALSE) Dependent variable: rprice (1) (2) near -18,824.370*** 9,964.964** t = -3.861 t = 2.568 y81 18,790.290*** 15,210.440*** t = 4.640 t = 5.455 y81near -11,863.900 -15,020.680*** t = -1.591 t = -2.986 age -657.159*** t = -5.095 agesq 3.083*** t = 3.806 rooms 3,280.510* t = 1.954 baths 7,489.360*** t = 2.879 area 17.830*** t = 7.658 land 0.116*** t = 3.863 Constant 82,517.230*** 2,189.081 t = 30.260 t = 0.210 Observations 321 321 R2 0.174 0.652 Adjusted R2 0.166 0.642 Note: t-statistic reported for each coefficient, *p&lt;0.1;**p&lt;0.05;***p&lt;0.01 stargazer(model1, model2, type = &quot;html&quot;, ci = TRUE, ci.level = 0.95, keep.stat = c(&quot;n&quot;,&quot;rsq&quot;,&quot;adj.rsq&quot;), notes = &quot;95% confidence interval reported in parentheses, &lt;em&gt;&amp;#42;p&amp;lt;0.1;&amp;#42;&amp;#42;p&amp;lt;0.05;&amp;#42;&amp;#42;&amp;#42;p&amp;lt;0.01&lt;/em&gt;&quot;, notes.append = FALSE) Dependent variable: rprice (1) (2) near -18,824.370*** 9,964.964** (-28,379.830, -9,268.915) (2,358.781, 17,571.150) y81 18,790.290*** 15,210.440*** (10,852.300, 26,728.270) (9,745.545, 20,675.350) y81near -11,863.900 -15,020.680*** (-26,478.660, 2,750.855) (-24,878.830, -5,162.541) age -657.159*** (-909.960, -404.357) agesq 3.083*** (1.496, 4.670) rooms 3,280.510* (-9.419, 6,570.438) baths 7,489.360*** (2,390.634, 12,588.080) area 17.830*** (13.267, 22.393) land 0.116*** (0.057, 0.174) Constant 82,517.230*** 2,189.081 (77,172.580, 87,861.870) (-18,273.310, 22,651.470) Observations 321 321 R2 0.174 0.652 Adjusted R2 0.166 0.642 Note: 95% confidence interval reported in parentheses, *p&lt;0.1;**p&lt;0.05;***p&lt;0.01 18.5 Additional questions 18.5.1 Question 1 Based on model 1, what effect did the garbage incinerator have on house prices for houses “near” (within 3 miles of) the incinerator? Is this effect statistically significant? Solution: Based on this regression, the incinerator decreased prices by $11863.90, on average. It is not statistically significant at any standard level of significance (based on the p-value of 0.113). 18.5.2 Question 2 Here is how you can easily report the confidence intervals for model1 for the 3 standard (i.e., commonly used/reported) levels of significance: pander(confint(model1,level = 0.99))   0.5 % 99.5 % (Intercept) 75451 89584 near -31458 -6190 y81 8295 29286 y81near -31187 7459 pander(confint(model1,level = 0.95))   2.5 % 97.5 % (Intercept) 77152 87882 near -28416 -9232 y81 10822 26759 y81near -26535 2807 pander(confint(model1,level = 0.90))   5 % 95 % (Intercept) 78019 87016 near -26867 -10782 y81 12109 25472 y81near -24165 437.1 To reach the same conclusion about statistical significance of the effect of the garbage incinerator as your answer to the previous question (for which you looked at the p-value), what level confidence interval do we need to look at? Why? Solution: In the previous question, we concluded that the effect of the garbage incinerator (-11864) is not statistically significant at any standard level. We reached this conclusion because the p-value (0.113) is larger than 0.10. When the p-value is larger than 0.10, that implies that the 90% confidence interval will contain 0. Any time the 90% confidence interval contains 0, we know for sure that the 95% and 99% confidence intervals must also contain 0 (just as we know that when a p-value is larger than 0.10, it obviously must also be larger than 0.05 and 0.01). So, to conclude that a coefficient is not statistically significant at any standard level, you need to look at the 90% confidence interval and see if it includes 0. If the 90% confidence interval includes 0, that means we fail to reject that the coefficient might be 0 at the 10% level (which means we also fail to reject at 5% and 1%, and thus, at any standard level). 18.5.3 Question 3 Now look at the second model. Based on model2, what effect did the garbage incinerator have on house prices for houses “near” (within 3 miles of) the incinerator? Is this effect statistically significant? Solution: Based on this regression, the incinerator decreased prices by $15,020.68, on average, holding constant characteristics of the house (number of rooms and bathrooms, size, land, and a quadratic in age). It is statistically significant at the 1% level (based on the p-value of 0.003). 18.5.4 Question 4 Calculate the confidence interval needed to reach the same conclusion about statistical significance of the effect of the garbage incinerator as your answer to the previous question (for which you looked at the p-value)? Explain why this is the level you need to look at to reach that conclusion. Solution: In the previous question, we concluded that the effect of the garbage incinerator (-15021) is statistically significant at the 1% level. We reached this conclusion because the p-value (0.003) is smaller than 0.01. If we calculate the 99% confidence interval, it will not contain zero. Thus, it’s less than 1% likely that the true coefficient is 0, so we can reject at the 1% level that the true coefficient is 0. The 95% and 90% confidence intervals will be narrower, and further away from 0, so they won’t contain 0 either. But to conclude that the coefficient is statistically significant at the 1% level, we need to look at the 99% confidence interval. As expected, it doesn’t include 0: pander(confint(model2), level=0.99)   2.5 % 97.5 % (Intercept) -18353 22731 near 2329 17601 y81 9724 20697 y81near -24917 -5124 age -910.9 -403.4 agesq 1.489 4.677 rooms -22.27 6583 baths 2371 12608 area 13.25 22.41 land 0.05672 0.1745 ## if we only want the confidence interval for y81near, we can do this: pander(confint(model2)[&quot;y81near&quot;,], level=0.99) 2.5 % 97.5 % -24917 -5124 18.6 Polynomials For this part I’ve left in all the code, so there’s nothing you have to do here other than comment out the code I give you. However, I suggest you go through the code and experiment a bit until you understand what’s going on with polynomial models. In model 2 you included a second order polynomial in age (i.e., you included age and age squared). What this allows for is the regression line to have a parabolic shape instead of being a straight line. To see an example of this, consider the following two models, the first that only includes age and the second that includes age and age squared. mydata$agesq &lt;- mydata$age^2 modelAge &lt;- lm(rprice~age,data=mydata) mydata$yHatAge &lt;- fitted(modelAge) modelAge2 &lt;- lm(rprice~age+agesq,data=mydata) mydata$yHatAge2 &lt;- fitted(modelAge2) ggplot(data=mydata,aes(y=rprice,x=age)) + geom_point(shape=4) + geom_point(aes(y=yHatAge,x=age),color=&quot;blue&quot;,size=2) + geom_line(aes(y=yHatAge,x=age),color=&quot;blue&quot;) + geom_point(aes(y=yHatAge2,x=age),color=&quot;green&quot;,size=2) + geom_line(aes(y=yHatAge2,x=age),color=&quot;green&quot;) To see a slightly more complete model, also including near, y81, and y81near. Let’s define a factor variable that will indicate the four groups by color. We’ll first display the graph for the model that only includes age (plus near, y81, and y81near). Then we’ll display the graph for the model that also includes age squared. mydata$group &lt;- ifelse(mydata$near==0 &amp; mydata$year==1978,&quot;y78far&quot;, ifelse(mydata$near==1 &amp; mydata$year==1978,&quot;y78near&quot;, ifelse(mydata$near==0 &amp; mydata$year==1981,&quot;y81far&quot;, &quot;y81near&quot; ))) modelAgeWithGroups &lt;- lm(rprice~age+near+y81+y81near,data=mydata) mydata$yHatAgeWithGroups &lt;- fitted(modelAgeWithGroups) ggplot(data=mydata,aes(y=rprice,x=age,color=group)) + geom_point(shape=4) + geom_point(aes(y=yHatAgeWithGroups,x=age,color=group),size=2) + geom_line(aes(y=yHatAgeWithGroups,x=age,color=group)) modelAge2WithGroups &lt;- lm(rprice~age+agesq+near+y81+y81near,data=mydata) mydata$yHatAge2WithGroups &lt;- fitted(modelAge2WithGroups) ggplot(data=mydata,aes(y=rprice,x=age,color=group)) + geom_point(shape=4) + geom_point(aes(y=yHatAge2WithGroups,x=age,color=group),size=2) + geom_line(aes(y=yHatAge2WithGroups,x=age,color=group)) What you see in the polynomial graphs is that include age squared allows for a nonlinear relationship. Here, initially house price goes down as the house gets older. However, after about 100 years, older houses start to sell for more. This makes sense. Initially getting older is bad because older houses are more likely to have problems, require repairs, etc. But a hundred+ year old house is probably a pretty nice house if it’s survived that long, it might be considered “historic,” etc. We can’t possibly capture that relationship unless we specify our model in a way that allows for a nonlinear relationship. Sometimes you will also see a cubic term. That allows for three turns in the relationship (down, up, down, or up, down, up). Note that in the final graph, the only reason the lines overlap is because ggplot simply connects the dots. If there were more points the bottom wouldn’t be flat and the lines wouldn’t cross. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
